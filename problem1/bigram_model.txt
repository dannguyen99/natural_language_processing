<s> Natural	0.005380
Natural	0.000363
Natural language	0.692308
language	0.004129
language processing	0.222973
processing	0.001507
processing -LRB-	0.074074
-LRB-	0.010295
-LRB- NLP	0.008130
NLP	0.001311
NLP -RRB-	0.085106
-RRB-	0.010295
-RRB- is	0.029810
is	0.013727
is a	0.109756
a	0.022739
a field	0.003681
field	0.000753
field of	0.444444
of	0.031304
of computer	0.003565
computer	0.001228
computer science	0.090909
science	0.000279
science ,	0.400000
,	0.049690
, artificial	0.001123
artificial	0.000307
artificial intelligence	0.636364
intelligence	0.000223
intelligence -LRB-	0.125000
-LRB- also	0.010840
also	0.001925
also called	0.043478
called	0.000502
called machine	0.055556
machine	0.002204
machine learning	0.240506
learning	0.001200
learning -RRB-	0.023256
-RRB- ,	0.211382
, and	0.106120
and	0.019307
and linguistics	0.002890
linguistics	0.000558
linguistics concerned	0.050000
concerned	0.000140
concerned with	0.800000
with	0.005106
with the	0.153005
the	0.040316
the interactions	0.000692
interactions	0.000028
interactions between	1.000000
between	0.001088
between computers	0.025641
computers	0.000251
computers and	0.111111
and human	0.001445
human	0.001283
human -LRB-	0.043478
-LRB- natural	0.002710
natural	0.002093
natural -RRB-	0.013333
-RRB- languages	0.002710
languages	0.001395
languages .	0.160000
.	0.035768
. </s>	0.987520
</s>	0.036298
<s> Specifically	0.000769
Specifically	0.000028
Specifically ,	1.000000
, it	0.013476
it	0.003264
it is	0.205128
is the	0.091463
the process	0.007612
process	0.001004
process of	0.333333
of a	0.081996
a computer	0.018405
computer extracting	0.022727
extracting	0.000140
extracting meaningful	0.200000
meaningful	0.000223
meaningful information	0.125000
information	0.001283
information from	0.065217
from	0.002902
from natural	0.009615
natural language	0.760000
language input	0.020270
input	0.001144
input and\/or	0.024390
and\/or	0.000084
and\/or producing	0.333333
producing	0.000084
producing natural	0.333333
language output	0.006757
output	0.000725
output .	0.153846
<s> In	0.074558
In	0.002930
In theory	0.019048
theory	0.000363
theory ,	0.307692
, natural	0.000561
processing is	0.037037
a very	0.013497
very	0.001144
very attractive	0.024390
attractive	0.000084
attractive method	0.333333
method	0.000446
method of	0.125000
of human	0.004456
human --	0.021739
--	0.000698
-- computer	0.040000
computer interaction	0.022727
interaction	0.000223
interaction .	0.250000
language understanding	0.094595
understanding	0.000921
understanding is	0.060606
is sometimes	0.006098
sometimes	0.000363
sometimes referred	0.230769
referred	0.000223
referred to	1.000000
to	0.021009
to as	0.005312
as	0.008007
as an	0.045296
an	0.003683
an AI-complete	0.007576
AI-complete	0.000084
AI-complete problem	0.333333
problem	0.001228
problem because	0.022727
because	0.000837
because it	0.100000
it seems	0.008547
seems	0.000056
seems to	1.000000
to require	0.001328
require	0.000614
require extensive	0.090909
extensive	0.000084
extensive knowledge	0.333333
knowledge	0.000753
knowledge about	0.111111
about	0.001116
about the	0.200000
the outside	0.000692
outside	0.000056
outside world	0.500000
world	0.000419
world and	0.066667
and the	0.059249
the ability	0.001384
ability	0.000112
ability to	0.750000
to manipulate	0.002656
manipulate	0.000084
manipulate it	0.333333
it .	0.042735
<s> Whether	0.001537
Whether	0.000056
Whether NLP	0.500000
NLP is	0.021277
is distinct	0.002033
distinct	0.000195
distinct from	0.428571
from ,	0.009615
, or	0.018529
or	0.006194
or identical	0.004505
identical	0.000056
identical to	1.000000
to ,	0.002656
, the	0.058394
the field	0.011765
of computational	0.002674
computational	0.000279
computational linguistics	0.600000
linguistics is	0.150000
a matter	0.001227
matter	0.000084
matter of	0.333333
of perspective	0.000891
perspective	0.000112
perspective .	0.250000
<s> The	0.112221
The	0.005357
The Association	0.005208
Association	0.000028
Association for	1.000000
for	0.007728
for Computational	0.003610
Computational	0.000084
Computational Linguistics	1.000000
Linguistics	0.000084
Linguistics defines	0.333333
defines	0.000056
defines the	0.500000
the latter	0.000692
latter	0.000028
latter as	1.000000
as focusing	0.003484
focusing	0.000028
focusing on	1.000000
on	0.005915
on the	0.306604
the theoretical	0.000692
theoretical	0.000084
theoretical aspects	0.333333
aspects	0.000195
aspects of	0.857143
of NLP	0.004456
NLP .	0.106383
<s> On	0.003843
On	0.000167
On the	0.333333
the other	0.005536
other	0.001953
other hand	0.071429
hand	0.000391
hand ,	0.500000
the open-access	0.000692
open-access	0.000028
open-access journal	1.000000
journal	0.000084
journal ``	0.333333
``	0.005273
`` Computational	0.005291
Linguistics ''	0.333333
''	0.005413
'' ,	0.154639
, styles	0.000561
styles	0.000028
styles itself	1.000000
itself	0.000140
itself as	0.200000
as ``	0.048780
`` the	0.026455
the longest	0.000692
longest	0.000028
longest running	1.000000
running	0.000084
running publication	0.333333
publication	0.000084
publication devoted	0.333333
devoted	0.000140
devoted exclusively	0.200000
exclusively	0.000028
exclusively to	1.000000
to the	0.102258
the design	0.001384
design	0.000112
design and	0.250000
and analysis	0.001445
analysis	0.001814
analysis of	0.184615
of natural	0.019608
processing systems	0.055556
systems	0.003125
systems ''	0.008929
'' -LRB-	0.046392
-LRB- Computational	0.002710
Linguistics -LRB-	0.333333
-LRB- Journal	0.002710
Journal	0.000084
Journal -RRB-	0.333333
-RRB- -RRB-	0.005420
-RRB- Modern	0.002710
Modern	0.000084
Modern NLP	0.333333
NLP algorithms	0.042553
algorithms	0.000977
algorithms are	0.057143
are	0.006724
are grounded	0.008299
grounded	0.000084
grounded in	1.000000
in	0.014899
in machine	0.009363
learning ,	0.093023
, especially	0.005053
especially	0.000419
especially statistical	0.066667
statistical	0.000921
statistical machine	0.090909
learning .	0.093023
<s> Research	0.001537
Research	0.000223
Research into	0.125000
into	0.002176
into modern	0.012821
modern	0.000140
modern statistical	0.200000
statistical NLP	0.060606
algorithms requires	0.028571
requires	0.000446
requires an	0.062500
an understanding	0.015152
understanding of	0.151515
a number	0.024540
number	0.001200
number of	0.837209
of disparate	0.000891
disparate	0.000028
disparate fields	1.000000
fields	0.000167
fields ,	0.333333
, including	0.004492
including	0.000391
including linguistics	0.142857
linguistics ,	0.400000
, computer	0.001123
and statistics	0.002890
statistics	0.000223
statistics .	0.375000
<s> For	0.043812
For	0.001702
For a	0.032787
a discussion	0.001227
discussion	0.000056
discussion of	0.500000
of the	0.173797
the types	0.001384
types	0.000391
types of	0.857143
of algorithms	0.000891
algorithms currently	0.028571
currently	0.000195
currently used	0.142857
used	0.003153
used in	0.203540
in NLP	0.014981
NLP ,	0.021277
, see	0.001123
see	0.000558
see the	0.100000
the article	0.000692
article	0.000809
article on	0.034483
on pattern	0.004717
pattern	0.000167
pattern recognition	0.666667
recognition	0.003376
recognition .	0.057851
<s> An	0.008455
An	0.000446
An automated	0.062500
automated	0.000195
automated online	0.142857
online	0.000223
online assistant	0.125000
assistant	0.000028
assistant providing	1.000000
providing	0.000056
providing customer	0.500000
customer	0.000028
customer service	1.000000
service	0.000140
service on	0.200000
on a	0.108491
a web	0.001227
web	0.000223
web page	0.125000
page	0.000195
page ,	0.428571
, an	0.005615
an example	0.037879
example	0.002260
example of	0.086420
of an	0.011586
an application	0.015152
application	0.000391
application where	0.071429
where	0.000977
where natural	0.028571
a major	0.006135
major	0.000335
major component	0.083333
component	0.000140
component .	0.200000
In 1950	0.019048
1950	0.000056
1950 ,	1.000000
, Alan	0.000561
Alan	0.000028
Alan Turing	1.000000
Turing	0.000056
Turing published	0.500000
published	0.000195
published his	0.142857
his	0.000335
his famous	0.083333
famous	0.000084
famous article	0.333333
article ``	0.034483
`` Computing	0.005291
Computing	0.000056
Computing Machinery	0.500000
Machinery	0.000028
Machinery and	1.000000
and Intelligence	0.001445
Intelligence	0.000084
Intelligence ''	0.333333
'' which	0.005155
which	0.003850
which proposed	0.007246
proposed	0.000251
proposed what	0.111111
what	0.000893
what is	0.093750
is now	0.006098
now	0.000363
now called	0.076923
called the	0.111111
the Turing	0.000692
Turing test	0.500000
test	0.000279
test as	0.100000
as a	0.118467
a criterion	0.001227
criterion	0.000056
criterion of	0.500000
of intelligence	0.000891
intelligence .	0.125000
<s> This	0.039969
This	0.001758
This criterion	0.015873
criterion depends	0.500000
depends	0.000223
depends on	0.875000
ability of	0.250000
computer program	0.113636
program	0.000614
program to	0.090909
to impersonate	0.001328
impersonate	0.000028
impersonate a	1.000000
a human	0.013497
human in	0.021739
in a	0.093633
a real-time	0.001227
real-time	0.000056
real-time written	0.500000
written	0.000725
written conversation	0.038462
conversation	0.000112
conversation with	0.500000
with a	0.109290
human judge	0.021739
judge	0.000112
judge ,	0.250000
, sufficiently	0.000561
sufficiently	0.000028
sufficiently well	1.000000
well	0.000781
well that	0.035714
that	0.007868
that the	0.081560
the judge	0.000692
judge is	0.250000
is unable	0.002033
unable	0.000056
unable to	1.000000
to distinguish	0.006640
distinguish	0.000140
distinguish reliably	0.200000
reliably	0.000028
reliably --	1.000000
-- on	0.040000
the basis	0.002768
basis	0.000167
basis of	0.666667
the conversational	0.000692
conversational	0.000028
conversational content	1.000000
content	0.000335
content alone	0.083333
alone	0.000112
alone --	0.250000
-- between	0.040000
between the	0.179487
the program	0.002076
program and	0.045455
and a	0.023121
a real	0.002454
real	0.000251
real human	0.111111
human .	0.065217
The Georgetown	0.015625
Georgetown	0.000084
Georgetown experiment	1.000000
experiment	0.000140
experiment in	0.200000
in 1954	0.003745
1954	0.000084
1954 involved	0.333333
involved	0.000167
involved fully	0.333333
fully	0.000167
fully automatic	0.500000
automatic	0.000642
automatic translation	0.086957
translation	0.002065
translation of	0.148649
of more	0.003565
more	0.002651
more than	0.042105
than	0.001256
than sixty	0.022222
sixty	0.000056
sixty Russian	1.000000
Russian	0.000056
Russian sentences	1.000000
sentences	0.002120
sentences into	0.039474
into English	0.025641
English	0.001032
English .	0.135135
The authors	0.015625
authors	0.000140
authors claimed	0.400000
claimed	0.000056
claimed that	1.000000
that within	0.007092
within	0.000502
within three	0.111111
three	0.000084
three or	0.333333
or five	0.004505
five	0.000140
five years	0.400000
years	0.000586
years ,	0.238095
, machine	0.001684
machine translation	0.493671
translation would	0.027027
would	0.001479
would be	0.169811
be	0.006612
be a	0.054852
a solved	0.002454
solved	0.000140
solved problem	0.400000
problem .	0.227273
<s> However	0.028440
However	0.001032
However ,	0.864865
, real	0.000561
real progress	0.111111
progress	0.000195
progress was	0.285714
was	0.002148
was much	0.025974
much	0.000614
much slower	0.090909
slower	0.000056
slower ,	1.000000
and after	0.004335
after	0.000335
after the	0.166667
the ALPAC	0.001384
ALPAC	0.000056
ALPAC report	1.000000
report	0.000112
report in	0.250000
in 1966	0.001873
1966	0.000084
1966 ,	0.333333
, which	0.031443
which found	0.014493
found	0.000391
found that	0.357143
that ten	0.003546
ten	0.000028
ten years	1.000000
years long	0.047619
long	0.000056
long research	0.500000
research	0.001172
research had	0.047619
had	0.000391
had failed	0.142857
failed	0.000056
failed to	1.000000
to fulfill	0.002656
fulfill	0.000056
fulfill the	0.500000
the expectations	0.000692
expectations	0.000056
expectations ,	1.000000
, funding	0.001684
funding	0.000223
funding for	0.250000
for machine	0.010830
translation was	0.027027
was dramatically	0.012987
dramatically	0.000028
dramatically reduced	1.000000
reduced	0.000112
reduced .	0.500000
<s> Little	0.000769
Little	0.000028
Little further	1.000000
further	0.000223
further research	0.125000
research in	0.142857
was conducted	0.025974
conducted	0.000140
conducted until	0.200000
until	0.000056
until the	0.500000
the late	0.005536
late	0.000251
late 1980s	0.444444
1980s	0.000251
1980s ,	0.555556
, when	0.003369
when	0.000977
when the	0.114286
the first	0.010381
first	0.000921
first statistical	0.060606
translation systems	0.027027
systems were	0.053571
were	0.001144
were developed	0.121951
developed	0.000725
developed .	0.076923
<s> Some	0.012298
Some	0.000586
Some notably	0.047619
notably	0.000084
notably successful	0.333333
successful	0.000251
successful NLP	0.111111
NLP systems	0.063830
systems developed	0.017857
developed in	0.230769
in the	0.260300
the 1960s	0.001384
1960s	0.000084
1960s were	0.333333
were SHRDLU	0.024390
SHRDLU	0.000167
SHRDLU ,	0.166667
, a	0.026951
a natural	0.006135
language system	0.006757
system	0.002595
system working	0.010753
working	0.000195
working in	0.285714
in restricted	0.001873
restricted	0.000112
restricted ``	0.250000
`` blocks	0.010582
blocks	0.000112
blocks worlds	0.250000
worlds	0.000028
worlds ''	1.000000
'' with	0.020619
with restricted	0.005464
restricted vocabularies	0.250000
vocabularies	0.000056
vocabularies ,	1.000000
and ELIZA	0.002890
ELIZA	0.000251
ELIZA ,	0.333333
a simulation	0.001227
simulation	0.000084
simulation of	0.333333
a Rogerian	0.001227
Rogerian	0.000028
Rogerian psychotherapist	1.000000
psychotherapist	0.000028
psychotherapist ,	1.000000
, written	0.000561
written by	0.230769
by	0.004883
by Joseph	0.005714
Joseph	0.000056
Joseph Weizenbaum	1.000000
Weizenbaum	0.000084
Weizenbaum between	0.333333
between 1964	0.025641
1964	0.000028
1964 to	1.000000
to 1966	0.001328
1966 .	0.333333
<s> Using	0.001537
Using	0.000056
Using almost	0.500000
almost	0.000028
almost no	1.000000
no	0.000363
no information	0.076923
information about	0.043478
about human	0.025000
human thought	0.021739
thought	0.000084
thought or	0.333333
or emotion	0.004505
emotion	0.000028
emotion ,	1.000000
, ELIZA	0.001123
ELIZA sometimes	0.111111
sometimes provided	0.076923
provided	0.000140
provided a	0.200000
a startlingly	0.001227
startlingly	0.000028
startlingly human-like	1.000000
human-like	0.000028
human-like interaction	1.000000
<s> When	0.004612
When	0.000195
When the	0.142857
the ``	0.003460
`` patient	0.005291
patient	0.000028
patient ''	1.000000
'' exceeded	0.005155
exceeded	0.000028
exceeded the	1.000000
the very	0.000692
very small	0.048780
small	0.000251
small knowledge	0.111111
knowledge base	0.148148
base	0.000112
base ,	0.500000
ELIZA might	0.111111
might	0.000725
might provide	0.038462
provide	0.000167
provide a	0.333333
a generic	0.001227
generic	0.000084
generic response	0.333333
response	0.000056
response ,	0.500000
, for	0.012353
for example	0.064982
example ,	0.666667
, responding	0.000561
responding	0.000028
responding to	1.000000
to ``	0.005312
`` My	0.005291
My	0.000028
My head	1.000000
head	0.000056
head hurts	1.000000
hurts	0.000056
hurts ''	0.500000
with ``	0.016393
`` Why	0.015873
Why	0.000195
Why do	0.142857
do	0.000725
do you	0.038462
you	0.000363
you say	0.076923
say	0.000195
say your	0.142857
your	0.000056
your head	0.500000
hurts ?	0.500000
?	0.000670
? ''	0.375000
'' .	0.067010
<s> During	0.003075
During	0.000112
During the	0.500000
the 70	0.000692
70	0.000112
70 's	0.250000
's	0.001423
's many	0.019608
many	0.001451
many programmers	0.019231
programmers	0.000028
programmers began	1.000000
began	0.000195
began to	0.571429
to write	0.001328
write	0.000028
write `	1.000000
`	0.000446
` conceptual	0.062500
conceptual	0.000056
conceptual ontologies	0.500000
ontologies	0.000167
ontologies '	0.166667
'	0.000530
' ,	0.315789
which structured	0.007246
structured	0.000167
structured real-world	0.166667
real-world	0.000167
real-world information	0.166667
information into	0.043478
into computer-understandable	0.012821
computer-understandable	0.000028
computer-understandable data	1.000000
data	0.002148
data .	0.220779
<s> Examples	0.002306
Examples	0.000084
Examples are	0.666667
are MARGIE	0.004149
MARGIE	0.000028
MARGIE -LRB-	1.000000
-LRB- Schank	0.002710
Schank	0.000140
Schank ,	0.200000
, 1975	0.000561
1975	0.000028
1975 -RRB-	1.000000
, SAM	0.000561
SAM	0.000028
SAM -LRB-	1.000000
-LRB- Cullingford	0.002710
Cullingford	0.000028
Cullingford ,	1.000000
, 1978	0.001123
1978	0.000084
1978 -RRB-	0.666667
, PAM	0.000561
PAM	0.000028
PAM -LRB-	1.000000
-LRB- Wilensky	0.002710
Wilensky	0.000056
Wilensky ,	1.000000
, TaleSpin	0.000561
TaleSpin	0.000028
TaleSpin -LRB-	1.000000
-LRB- Meehan	0.002710
Meehan	0.000028
Meehan ,	1.000000
, 1976	0.001123
1976	0.000056
1976 -RRB-	0.500000
, QUALM	0.000561
QUALM	0.000028
QUALM -LRB-	1.000000
-LRB- Lehnert	0.005420
Lehnert	0.000084
Lehnert ,	0.666667
, 1977	0.000561
1977	0.000028
1977 -RRB-	1.000000
, Politics	0.000561
Politics	0.000028
Politics -LRB-	1.000000
-LRB- Carbonell	0.002710
Carbonell	0.000028
Carbonell ,	1.000000
, 1979	0.000561
1979	0.000028
1979 -RRB-	1.000000
and Plot	0.001445
Plot	0.000028
Plot Units	1.000000
Units	0.000028
Units -LRB-	1.000000
Lehnert 1981	0.333333
1981	0.000028
1981 -RRB-	1.000000
-RRB- .	0.273713
During this	0.500000
this	0.002539
this time	0.032967
time	0.000921
time ,	0.333333
, many	0.003930
many chatterbots	0.019231
chatterbots	0.000056
chatterbots were	0.500000
were written	0.024390
written including	0.038462
including PARRY	0.071429
PARRY	0.000028
PARRY ,	1.000000
, Racter	0.000561
Racter	0.000028
Racter ,	1.000000
and Jabberwacky	0.001445
Jabberwacky	0.000028
Jabberwacky .	1.000000
<s> Up	0.000769
Up	0.000028
Up to	1.000000
the 1980s	0.001384
, most	0.004492
most	0.001618
most NLP	0.017241
were based	0.024390
based	0.001507
based on	0.833333
on complex	0.004717
complex	0.000670
complex sets	0.041667
sets	0.000307
sets of	0.363636
of hand-written	0.002674
hand-written	0.000195
hand-written rules	0.857143
rules	0.001200
rules .	0.139535
<s> Starting	0.000769
Starting	0.000028
Starting in	1.000000
, however	0.006176
however	0.000363
however ,	0.923077
, there	0.006176
there	0.001116
there was	0.075000
was a	0.038961
a revolution	0.001227
revolution	0.000028
revolution in	1.000000
NLP with	0.021277
the introduction	0.000692
introduction	0.000028
introduction of	1.000000
of machine	0.007130
learning algorithms	0.116279
algorithms for	0.114286
for language	0.003610
processing .	0.129630
This was	0.015873
was due	0.012987
due	0.000140
due both	0.400000
both	0.000865
both to	0.129032
the steady	0.001384
steady	0.000056
steady increase	0.500000
increase	0.000112
increase in	0.750000
in computational	0.003745
computational power	0.200000
power	0.000112
power resulting	0.250000
resulting	0.000112
resulting from	0.250000
from Moore	0.009615
Moore	0.000028
Moore 's	1.000000
's Law	0.019608
Law	0.000028
Law and	1.000000
the gradual	0.000692
gradual	0.000028
gradual lessening	1.000000
lessening	0.000028
lessening of	1.000000
the dominance	0.000692
dominance	0.000028
dominance of	1.000000
of Chomskyan	0.000891
Chomskyan	0.000028
Chomskyan theories	1.000000
theories	0.000140
theories of	0.600000
of linguistics	0.000891
linguistics -LRB-	0.050000
-LRB- e.g.	0.102981
e.g.	0.001562
e.g. transformational	0.017857
transformational	0.000056
transformational grammar	1.000000
grammar	0.001032
grammar -RRB-	0.081081
, whose	0.001123
whose	0.000084
whose theoretical	0.333333
theoretical underpinnings	0.333333
underpinnings	0.000028
underpinnings discouraged	1.000000
discouraged	0.000028
discouraged the	1.000000
the sort	0.000692
sort	0.000084
sort of	0.666667
of corpus	0.000891
corpus	0.000865
corpus linguistics	0.096774
linguistics that	0.100000
that underlies	0.003546
underlies	0.000028
underlies the	1.000000
the machine-learning	0.000692
machine-learning	0.000112
machine-learning approach	0.250000
approach	0.000977
approach to	0.171429
to language	0.001328
Some of	0.190476
the earliest-used	0.001384
earliest-used	0.000056
earliest-used machine	0.500000
algorithms ,	0.142857
, such	0.019652
such	0.003432
such as	0.731707
as decision	0.010453
decision	0.000112
decision trees	1.000000
trees	0.000167
trees ,	0.500000
, produced	0.001684
produced	0.000251
produced systems	0.222222
systems of	0.053571
of hard	0.001783
hard	0.000167
hard if-then	0.333333
if-then	0.000056
if-then rules	1.000000
rules similar	0.046512
similar	0.000753
similar to	0.555556
to existing	0.001328
existing	0.000140
existing hand-written	0.200000
<s> Increasingly	0.001537
Increasingly	0.000056
Increasingly ,	1.000000
, research	0.001123
research has	0.142857
has	0.002344
has focused	0.047619
focused	0.000307
focused on	0.909091
on statistical	0.009434
statistical models	0.212121
models	0.000725
models ,	0.076923
which make	0.014493
make	0.000558
make soft	0.200000
soft	0.000112
soft ,	0.500000
, probabilistic	0.001684
probabilistic	0.000195
probabilistic decisions	0.285714
decisions	0.000279
decisions based	0.200000
on attaching	0.009434
attaching	0.000056
attaching real-valued	1.000000
real-valued	0.000084
real-valued weights	0.666667
weights	0.000140
weights to	0.400000
the features	0.003460
features	0.000725
features making	0.038462
making	0.000195
making up	0.142857
up	0.000614
up the	0.045455
the input	0.005536
input data	0.146341
The cache	0.005208
cache	0.000028
cache language	1.000000
language models	0.013514
models upon	0.038462
upon	0.000028
upon which	1.000000
which many	0.007246
many speech	0.019231
speech	0.004241
speech recognition	0.421053
recognition systems	0.082645
systems now	0.008929
now rely	0.076923
rely	0.000195
rely are	0.142857
are examples	0.012448
examples	0.000670
examples of	0.208333
of such	0.004456
such statistical	0.008130
models .	0.115385
<s> Such	0.006149
Such	0.000223
Such models	0.250000
models are	0.038462
are generally	0.016598
generally	0.000307
generally more	0.181818
more robust	0.021053
robust	0.000112
robust when	0.500000
when given	0.057143
given	0.000670
given unfamiliar	0.083333
unfamiliar	0.000084
unfamiliar input	1.000000
input ,	0.073171
especially input	0.133333
input that	0.048780
that contains	0.010638
contains	0.000279
contains errors	0.200000
errors	0.000140
errors -LRB-	0.400000
-LRB- as	0.018970
as is	0.013937
is very	0.012195
very common	0.048780
common	0.000698
common for	0.080000
for real-world	0.007220
real-world data	0.333333
data -RRB-	0.038961
and produce	0.002890
produce	0.000614
produce more	0.090909
more reliable	0.031579
reliable	0.000112
reliable results	0.500000
results	0.000586
results when	0.095238
when integrated	0.028571
integrated	0.000084
integrated into	0.333333
into a	0.217949
a larger	0.004908
larger	0.000446
larger system	0.125000
system comprising	0.010753
comprising	0.000056
comprising multiple	0.500000
multiple	0.000363
multiple subtasks	0.076923
subtasks	0.000056
subtasks .	0.500000
<s> Many	0.008455
Many	0.000335
Many of	0.166667
the notable	0.000692
notable	0.000028
notable early	1.000000
early	0.000279
early successes	0.100000
successes	0.000028
successes occurred	1.000000
occurred	0.000028
occurred in	1.000000
translation ,	0.108108
, due	0.000561
due especially	0.200000
especially to	0.066667
to work	0.002656
work	0.000670
work at	0.041667
at	0.001897
at IBM	0.014706
IBM	0.000084
IBM Research	0.333333
Research ,	0.125000
, where	0.008422
where successively	0.028571
successively	0.000028
successively more	1.000000
more complicated	0.010526
complicated	0.000084
complicated statistical	0.333333
models were	0.038462
<s> These	0.012298
These	0.000474
These systems	0.235294
were able	0.024390
able	0.000446
able to	1.000000
to take	0.006640
take	0.000279
take advantage	0.400000
advantage	0.000140
advantage of	0.800000
of existing	0.001783
existing multilingual	0.200000
multilingual	0.000084
multilingual textual	0.333333
textual	0.000140
textual corpora	0.200000
corpora	0.000307
corpora that	0.090909
that had	0.003546
had been	0.071429
been	0.001897
been produced	0.014706
produced by	0.333333
by the	0.154286
the Parliament	0.000692
Parliament	0.000056
Parliament of	0.500000
of Canada	0.001783
Canada	0.000167
Canada and	0.166667
the European	0.001384
European	0.000084
European Union	0.333333
Union	0.000028
Union as	1.000000
a result	0.003681
result	0.000307
result of	0.272727
of laws	0.000891
laws	0.000028
laws calling	1.000000
calling	0.000028
calling for	1.000000
for the	0.111913
the translation	0.004152
of all	0.003565
all	0.001200
all governmental	0.023256
governmental	0.000028
governmental proceedings	1.000000
proceedings	0.000028
proceedings into	1.000000
into all	0.012821
all official	0.023256
official	0.000028
official languages	1.000000
languages of	0.020000
the corresponding	0.001384
corresponding	0.000167
corresponding systems	0.166667
of government	0.001783
government	0.000084
government .	0.333333
most other	0.017241
other systems	0.028571
systems depended	0.008929
depended	0.000028
depended on	1.000000
on corpora	0.004717
corpora specifically	0.090909
specifically	0.000056
specifically developed	0.500000
developed for	0.038462
the tasks	0.000692
tasks	0.000893
tasks implemented	0.031250
implemented	0.000140
implemented by	0.200000
by these	0.005714
these	0.001172
these systems	0.119048
systems ,	0.053571
which was	0.036232
was -LRB-	0.012987
-LRB- and	0.013550
and often	0.004335
often	0.001228
often continues	0.022727
continues	0.000028
continues to	1.000000
to be	0.057105
be -RRB-	0.004219
-RRB- a	0.005420
major limitation	0.083333
limitation	0.000028
limitation in	1.000000
the success	0.001384
success	0.000140
success of	0.600000
of these	0.009804
systems .	0.089286
<s> As	0.010761
As	0.000502
As a	0.111111
result ,	0.272727
a great	0.002454
great	0.000084
great deal	0.333333
deal	0.000112
deal of	0.250000
of research	0.007130
has gone	0.011905
gone	0.000028
gone into	1.000000
into methods	0.012821
methods	0.001228
methods of	0.045455
more effectively	0.010526
effectively	0.000084
effectively learning	0.333333
learning from	0.046512
from limited	0.009615
limited	0.000279
limited amounts	0.100000
amounts	0.000056
amounts of	1.000000
of data	0.006239
<s> Recent	0.002306
Recent	0.000084
Recent research	0.666667
has increasingly	0.011905
increasingly	0.000084
increasingly focused	0.333333
on unsupervised	0.004717
unsupervised	0.000223
unsupervised and	0.125000
and semi-supervised	0.001445
semi-supervised	0.000056
semi-supervised learning	0.500000
algorithms .	0.114286
Such algorithms	0.125000
are able	0.012448
to learn	0.007968
learn	0.000363
learn from	0.076923
from data	0.019231
data that	0.025974
that has	0.021277
has not	0.023810
not	0.003125
not been	0.017857
been hand-annotated	0.029412
hand-annotated	0.000056
hand-annotated with	1.000000
the desired	0.002768
desired	0.000140
desired answers	0.200000
answers	0.000335
answers ,	0.083333
or using	0.009009
using	0.001646
using a	0.169492
a combination	0.002454
combination	0.000140
combination of	0.200000
of annotated	0.000891
annotated	0.000056
annotated and	0.500000
and non-annotated	0.001445
non-annotated	0.000056
non-annotated data	1.000000
<s> Generally	0.003843
Generally	0.000140
Generally ,	0.600000
, this	0.003369
this task	0.043956
task	0.001172
task is	0.142857
is much	0.004065
much more	0.181818
more difficult	0.073684
difficult	0.000781
difficult than	0.107143
than supervised	0.022222
supervised	0.000446
supervised learning	0.312500
and typically	0.002890
typically	0.000502
typically produces	0.055556
produces	0.000112
produces less	0.250000
less	0.000335
less accurate	0.083333
accurate	0.000195
accurate results	0.142857
results for	0.047619
for a	0.104693
a given	0.014724
given amount	0.041667
amount	0.000140
amount of	1.000000
of input	0.002674
there is	0.350000
is an	0.020325
an enormous	0.007576
enormous	0.000028
enormous amount	1.000000
of non-annotated	0.000891
data available	0.038961
available	0.000474
available -LRB-	0.058824
-LRB- including	0.002710
including ,	0.071429
, among	0.000561
among	0.000223
among other	0.375000
other things	0.042857
things	0.000084
things ,	0.333333
the entire	0.000692
entire	0.000084
entire content	0.333333
content of	0.083333
the World	0.003460
World	0.000195
World Wide	0.571429
Wide	0.000112
Wide Web	1.000000
Web	0.000251
Web -RRB-	0.111111
which can	0.036232
can	0.005050
can often	0.005525
often make	0.022727
make up	0.100000
up for	0.090909
the inferior	0.000692
inferior	0.000028
inferior results	1.000000
results .	0.095238
<s> NLP	0.000769
NLP using	0.021277
using machine	0.016949
learning As	0.023256
As described	0.055556
described	0.000167
described above	0.500000
above	0.000363
above ,	0.307692
, modern	0.000561
modern approaches	0.200000
approaches	0.000781
approaches to	0.178571
to natural	0.001328
-RRB- are	0.008130
The paradigm	0.005208
paradigm	0.000084
paradigm of	0.333333
learning is	0.023256
is different	0.002033
different	0.001367
different from	0.122449
from that	0.009615
that of	0.028369
of most	0.000891
most prior	0.017241
prior	0.000084
prior attempts	0.333333
attempts	0.000167
attempts at	0.333333
at language	0.014706
<s> Prior	0.000769
Prior	0.000028
Prior implementations	1.000000
implementations	0.000056
implementations of	1.000000
of language-processing	0.000891
language-processing	0.000028
language-processing tasks	1.000000
tasks typically	0.031250
typically involved	0.055556
involved the	0.166667
the direct	0.000692
direct	0.000167
direct hand	0.166667
hand coding	0.071429
coding	0.000028
coding of	1.000000
of large	0.003565
large	0.000642
large sets	0.086957
of rules	0.002674
The machine-learning	0.005208
machine-learning paradigm	0.250000
paradigm calls	0.333333
calls	0.000028
calls instead	1.000000
instead	0.000195
instead for	0.142857
for using	0.003610
using general	0.016949
general	0.000614
general learning	0.045455
algorithms --	0.028571
-- often	0.040000
often ,	0.022727
, although	0.002246
although	0.000167
although not	0.166667
not always	0.008929
always	0.000084
always ,	0.333333
, grounded	0.000561
in statistical	0.003745
statistical inference	0.060606
inference	0.000112
inference --	0.250000
-- to	0.040000
to automatically	0.007968
automatically	0.000586
automatically learn	0.095238
learn such	0.076923
such rules	0.008130
rules through	0.023256
through	0.000223
through the	0.500000
the analysis	0.002768
large corpora	0.043478
corpora of	0.090909
of typical	0.000891
typical	0.000251
typical real-world	0.111111
real-world examples	0.166667
examples .	0.166667
<s> A	0.033820
A	0.001395
A corpus	0.020000
corpus -LRB-	0.032258
-LRB- plural	0.002710
plural	0.000140
plural ,	0.400000
, ``	0.014037
`` corpora	0.005291
corpora ''	0.090909
'' -RRB-	0.092784
a set	0.017178
set	0.001088
set of	0.717949
of documents	0.004456
documents	0.001060
documents -LRB-	0.131579
-LRB- or	0.027100
or sometimes	0.004505
sometimes ,	0.076923
, individual	0.000561
individual	0.000335
individual sentences	0.083333
sentences -RRB-	0.026316
-RRB- that	0.005420
that have	0.021277
have	0.002902
have been	0.250000
the correct	0.004152
correct	0.000419
correct values	0.066667
values	0.000223
values to	0.125000
be learned	0.004219
learned	0.000140
learned .	0.200000
<s> Consider	0.001537
Consider	0.000056
Consider the	1.000000
the task	0.004844
task of	0.214286
of part	0.000891
part	0.000753
part of	0.814815
of speech	0.040998
speech tagging	0.013158
tagging	0.000698
tagging ,	0.080000
, i.e.	0.003930
i.e.	0.000530
i.e. determining	0.052632
determining	0.000167
determining the	0.666667
correct part	0.200000
speech of	0.006579
of each	0.006239
each	0.001256
each word	0.111111
word	0.001674
word in	0.066667
given sentence	0.083333
sentence	0.001339
sentence ,	0.125000
, typically	0.001684
typically one	0.055556
one	0.001814
one that	0.015385
has never	0.023810
never	0.000140
never been	0.400000
been seen	0.044118
seen	0.000279
seen before	0.200000
before	0.000167
before .	0.166667
A typical	0.040000
typical machine-learning-based	0.111111
machine-learning-based	0.000028
machine-learning-based implementation	1.000000
implementation	0.000056
implementation of	1.000000
a part	0.002454
speech tagger	0.006579
tagger	0.000251
tagger proceeds	0.111111
proceeds	0.000028
proceeds in	1.000000
in two	0.001873
two	0.000809
two steps	0.034483
steps	0.000056
steps ,	0.500000
a training	0.001227
training	0.000781
training step	0.071429
step	0.000419
step and	0.066667
and an	0.004335
an evaluation	0.007576
evaluation	0.001507
evaluation step	0.037037
step .	0.133333
The first	0.036458
first step	0.060606
step --	0.133333
-- the	0.120000
the training	0.002768
-- makes	0.040000
makes	0.000223
makes use	0.125000
use	0.002009
use of	0.291667
a corpus	0.003681
corpus of	0.225806
of training	0.003565
training data	0.357143
data ,	0.129870
which consists	0.007246
consists	0.000056
consists of	1.000000
a large	0.009816
large number	0.086957
of sentences	0.006239
sentences ,	0.105263
, each	0.003369
each of	0.111111
of which	0.008913
which has	0.050725
has the	0.023810
speech attached	0.006579
attached	0.000056
attached to	0.500000
to each	0.006640
word .	0.133333
<s> -LRB-	0.014604
-LRB- An	0.005420
An example	0.187500
such a	0.048780
corpus in	0.064516
in common	0.003745
common use	0.080000
use is	0.013889
the Penn	0.005536
Penn	0.000251
Penn Treebank	0.666667
Treebank	0.000167
Treebank .	0.333333
This includes	0.015873
includes	0.000195
includes -LRB-	0.142857
-LRB- among	0.005420
things -RRB-	0.333333
of 500	0.001783
500	0.000056
500 texts	0.500000
texts	0.000474
texts from	0.058824
from the	0.211538
the Brown	0.005536
Brown	0.000391
Brown Corpus	0.857143
Corpus	0.000446
Corpus ,	0.062500
, containing	0.001123
containing	0.000223
containing examples	0.125000
of various	0.000891
various	0.000502
various genres	0.055556
genres	0.000028
genres of	1.000000
of text	0.021390
text	0.004436
text ,	0.188679
and 2500	0.001445
2500	0.000028
2500 articles	1.000000
articles	0.000223
articles from	0.125000
the Wall	0.001384
Wall	0.000056
Wall Street	1.000000
Street	0.000084
Street Journal	0.666667
Journal .	0.333333
. -RRB-	0.006240
-RRB- </s>	0.037940
This corpus	0.031746
corpus is	0.032258
is analyzed	0.006098
analyzed	0.000140
analyzed and	0.200000
a learning	0.002454
learning model	0.023256
model	0.000837
model is	0.100000
is generated	0.006098
generated	0.000419
generated from	0.133333
from it	0.009615
it ,	0.017094
, consisting	0.000561
consisting	0.000056
consisting of	1.000000
of automatically	0.001783
automatically created	0.047619
created	0.000195
created rules	0.142857
rules for	0.046512
for determining	0.007220
the part	0.001384
speech for	0.026316
a word	0.013497
a sentence	0.017178
typically based	0.055556
the nature	0.003460
nature	0.000167
nature of	1.000000
the word	0.005536
in question	0.003745
question	0.001172
question ,	0.261905
of surrounding	0.000891
surrounding	0.000140
surrounding words	0.400000
words	0.003041
words ,	0.137615
the most	0.013149
most likely	0.051724
likely	0.000446
likely part	0.062500
for those	0.007220
those	0.000614
those surrounding	0.045455
words .	0.146789
The model	0.005208
model that	0.133333
that is	0.056738
generated is	0.066667
is typically	0.006098
typically the	0.055556
the best	0.008997
best	0.000502
best model	0.055556
that can	0.046099
can be	0.502762
be found	0.012658
that simultaneously	0.003546
simultaneously	0.000056
simultaneously meets	0.500000
meets	0.000056
meets two	0.500000
two conflicting	0.034483
conflicting	0.000028
conflicting objectives	1.000000
objectives	0.000056
objectives :	0.500000
:	0.002846
: To	0.009804
To	0.000251
To perform	0.111111
perform	0.000307
perform as	0.090909
as well	0.048780
well as	0.464286
as possible	0.017422
possible	0.000670
possible on	0.041667
and to	0.015896
be as	0.012658
as simple	0.006969
simple	0.000725
simple as	0.076923
possible -LRB-	0.041667
-LRB- so	0.005420
so	0.000837
so that	0.200000
the model	0.002076
model avoids	0.033333
avoids	0.000028
avoids overfitting	1.000000
overfitting	0.000056
overfitting the	0.500000
i.e. so	0.052632
that it	0.010638
it generalizes	0.008547
generalizes	0.000028
generalizes as	1.000000
possible to	0.125000
to new	0.005312
new	0.000670
new data	0.041667
data rather	0.012987
rather	0.000446
rather than	0.875000
than only	0.044444
only	0.001060
only succeeding	0.026316
succeeding	0.000028
succeeding on	1.000000
on sentences	0.004717
sentences that	0.065789
have already	0.009615
already	0.000140
already been	0.400000
seen -RRB-	0.100000
In the	0.133333
the second	0.001384
second	0.000279
second step	0.200000
step -LRB-	0.066667
-LRB- the	0.021680
the evaluation	0.003460
step -RRB-	0.066667
has been	0.333333
been learned	0.029412
learned is	0.200000
is used	0.026423
used to	0.194690
to process	0.003984
process new	0.027778
new sentences	0.041667
sentences .	0.105263
An important	0.125000
important	0.000446
important part	0.062500
the development	0.003460
development	0.000335
development of	0.583333
of any	0.002674
any	0.000865
any learning	0.032258
learning algorithm	0.116279
algorithm	0.000781
algorithm is	0.178571
is testing	0.002033
testing	0.000140
testing the	0.200000
learned on	0.200000
on new	0.004717
new ,	0.041667
, previously	0.000561
previously	0.000056
previously unseen	0.500000
unseen	0.000028
unseen data	1.000000
<s> It	0.026134
It	0.001060
It is	0.605263
is critical	0.002033
critical	0.000112
critical that	0.250000
the data	0.002768
data used	0.025974
used for	0.132743
for testing	0.003610
testing is	0.200000
is not	0.038618
not the	0.044643
the same	0.015225
same	0.000698
same as	0.080000
as the	0.094077
for training	0.010830
training ;	0.035714
;	0.001311
; otherwise	0.021277
otherwise	0.000056
otherwise ,	0.500000
the testing	0.000692
testing accuracy	0.200000
accuracy	0.000865
accuracy will	0.032258
will	0.000977
will be	0.257143
be unrealistically	0.004219
unrealistically	0.000028
unrealistically high	1.000000
high	0.000502
high .	0.055556
Many different	0.166667
different classes	0.020408
classes	0.000140
classes of	0.400000
algorithms have	0.085714
been applied	0.088235
applied	0.000419
applied to	0.733333
to NLP	0.001328
NLP tasks	0.042553
tasks .	0.125000
In common	0.009524
common to	0.040000
to all	0.003984
all of	0.093023
these algorithms	0.023810
algorithms is	0.028571
is that	0.024390
that they	0.024823
they	0.001116
they take	0.025000
take as	0.100000
as input	0.006969
input a	0.024390
large set	0.043478
of ``	0.007130
`` features	0.005291
features ''	0.038462
'' that	0.020619
that are	0.053191
are generated	0.004149
As an	0.111111
a part-of-speech	0.001227
part-of-speech	0.000419
part-of-speech tagger	0.066667
tagger ,	0.444444
, typical	0.000561
typical features	0.111111
features might	0.038462
might be	0.230769
be the	0.012658
the identity	0.002768
identity	0.000112
identity of	1.000000
word being	0.033333
being	0.000502
being processed	0.055556
processed	0.000167
processed ,	0.166667
the words	0.004152
words immediately	0.009174
immediately	0.000028
immediately to	1.000000
the left	0.001384
left	0.000167
left and	0.333333
and right	0.002890
right	0.000279
right ,	0.100000
the part-of-speech	0.000692
part-of-speech tag	0.066667
tag	0.000446
tag of	0.062500
word to	0.016667
left ,	0.166667
and whether	0.001445
whether	0.000363
whether the	0.153846
being considered	0.111111
considered	0.000251
considered or	0.111111
or its	0.004505
its	0.000977
its immediate	0.028571
immediate	0.000028
immediate neighbors	1.000000
neighbors	0.000084
neighbors are	0.333333
are content	0.004149
content words	0.083333
words or	0.064220
or function	0.004505
function	0.000223
function words	0.125000
The algorithms	0.010417
algorithms differ	0.028571
differ	0.000084
differ ,	0.333333
, in	0.019090
the rules	0.003460
rules generated	0.023256
generated .	0.200000
earliest-used algorithms	0.500000
the systems	0.002076
rules that	0.093023
that were	0.014184
were then	0.024390
then	0.000977
then common	0.028571
common .	0.080000
each input	0.022222
input feature	0.024390
feature	0.000363
feature .	0.153846
models have	0.038462
have the	0.009615
the advantage	0.000692
advantage that	0.200000
they can	0.150000
can express	0.022099
express	0.000140
express the	0.400000
the relative	0.000692
relative	0.000084
relative certainty	0.333333
certainty	0.000028
certainty of	1.000000
of many	0.001783
many different	0.076923
different possible	0.020408
possible answers	0.041667
answers rather	0.083333
only one	0.026316
one ,	0.061538
, producing	0.000561
producing more	0.333333
when such	0.057143
a model	0.001227
is included	0.002033
included	0.000223
included as	0.125000
a component	0.002454
component of	0.600000
system .	0.118280
In addition	0.028571
addition	0.000167
addition ,	0.333333
, models	0.000561
models that	0.115385
that make	0.010638
soft decisions	0.500000
decisions are	0.100000
<s> Systems	0.006149
Systems	0.000335
Systems based	0.250000
on machine-learning	0.004717
machine-learning algorithms	0.250000
have many	0.048077
many advantages	0.019231
advantages	0.000028
advantages over	1.000000
over	0.000335
over hand-produced	0.083333
hand-produced	0.000028
hand-produced rules	1.000000
rules :	0.046512
: The	0.039216
The learning	0.005208
learning procedures	0.046512
procedures	0.000112
procedures used	0.250000
used during	0.008850
during	0.000279
during machine	0.100000
learning automatically	0.023256
automatically focus	0.047619
focus	0.000195
focus on	0.571429
most common	0.103448
common cases	0.040000
cases	0.000502
cases ,	0.388889
, whereas	0.001123
whereas	0.000084
whereas when	0.333333
when writing	0.057143
writing	0.000251
writing rules	0.111111
rules by	0.023256
by hand	0.034286
hand it	0.071429
is often	0.022358
often not	0.045455
not obvious	0.008929
obvious	0.000028
obvious at	1.000000
at all	0.073529
all where	0.023256
where the	0.371429
the effort	0.000692
effort	0.000112
effort should	0.250000
should	0.000530
should be	0.473684
be directed	0.004219
directed	0.000028
directed .	1.000000
<s> Automatic	0.005380
Automatic	0.000251
Automatic learning	0.111111
procedures can	0.500000
can make	0.016575
make use	0.050000
of statistical	0.001783
inference algorithms	0.250000
algorithms to	0.085714
to produce	0.013280
produce models	0.045455
are robust	0.004149
robust to	0.250000
to unfamiliar	0.001328
input -LRB-	0.048780
e.g. containing	0.017857
containing words	0.125000
or structures	0.004505
structures	0.000140
structures that	0.400000
have not	0.019231
before -RRB-	0.333333
-RRB- and	0.054201
to erroneous	0.001328
erroneous	0.000028
erroneous input	1.000000
e.g. with	0.017857
with misspelled	0.005464
misspelled	0.000028
misspelled words	1.000000
or words	0.004505
words accidentally	0.009174
accidentally	0.000028
accidentally omitted	1.000000
omitted	0.000028
omitted -RRB-	1.000000
, handling	0.001123
handling	0.000056
handling such	0.500000
such input	0.008130
input gracefully	0.024390
gracefully	0.000028
gracefully with	1.000000
with hand-written	0.005464
rules --	0.023256
-- or	0.040000
or more	0.018018
more generally	0.010526
generally ,	0.090909
, creating	0.001123
creating	0.000195
creating systems	0.142857
decisions --	0.100000
-- is	0.040000
is extremely	0.004065
extremely	0.000112
extremely difficult	0.750000
difficult ,	0.035714
, error-prone	0.000561
error-prone	0.000028
error-prone and	1.000000
and time-consuming	0.001445
time-consuming	0.000084
time-consuming .	0.333333
on automatically	0.004717
automatically learning	0.047619
learning the	0.023256
rules can	0.069767
be made	0.016878
made	0.000446
made more	0.125000
more accurate	0.031579
accurate simply	0.142857
simply	0.000335
simply by	0.083333
by supplying	0.005714
supplying	0.000028
supplying more	1.000000
more input	0.010526
, systems	0.001123
systems based	0.017857
on hand-written	0.004717
can only	0.011050
only be	0.026316
accurate by	0.142857
by increasing	0.005714
increasing	0.000084
increasing the	0.333333
the complexity	0.005536
complexity	0.000335
complexity of	0.666667
rules ,	0.116279
which is	0.094203
a much	0.003681
difficult task	0.035714
task .	0.238095
In particular	0.028571
particular	0.000363
particular ,	0.230769
a limit	0.001227
limit	0.000112
limit to	0.250000
of systems	0.001783
on hand-crafted	0.004717
hand-crafted	0.000056
hand-crafted rules	0.500000
, beyond	0.000561
beyond	0.000167
beyond which	0.166667
which the	0.057971
systems become	0.008929
become	0.000112
become more	0.250000
more and	0.021053
and more	0.007225
more unmanageable	0.010526
unmanageable	0.000028
unmanageable .	1.000000
creating more	0.142857
more data	0.021053
data to	0.012987
to input	0.002656
input to	0.073171
to machine-learning	0.001328
machine-learning systems	0.250000
systems simply	0.008929
simply requires	0.083333
requires a	0.062500
a corresponding	0.001227
corresponding increase	0.166667
the number	0.004844
of man-hours	0.000891
man-hours	0.000028
man-hours worked	1.000000
worked	0.000140
worked ,	0.200000
, generally	0.000561
generally without	0.090909
without	0.000363
without significant	0.076923
significant	0.000251
significant increases	0.111111
increases	0.000056
increases in	1.000000
the annotation	0.000692
annotation	0.000112
annotation process	0.250000
process .	0.138889
<s> Major	0.001537
Major	0.000056
Major tasks	0.500000
tasks in	0.093750
NLP The	0.042553
The following	0.020833
following	0.000419
following is	0.066667
a list	0.008589
list	0.000307
list of	0.727273
of some	0.004456
some	0.002316
some of	0.156627
most commonly	0.017241
commonly	0.000223
commonly researched	0.125000
researched	0.000028
researched tasks	1.000000
<s> Note	0.006918
Note	0.000251
Note that	0.777778
that some	0.014184
these tasks	0.047619
tasks have	0.031250
have direct	0.009615
direct real-world	0.166667
real-world applications	0.166667
applications	0.000698
applications ,	0.160000
, while	0.007861
while	0.000558
while others	0.200000
others	0.000335
others more	0.083333
more commonly	0.021053
commonly serve	0.125000
serve	0.000140
serve as	0.800000
as subtasks	0.003484
subtasks that	0.500000
are used	0.033195
to aid	0.001328
aid	0.000112
aid in	0.750000
in solving	0.001873
solving	0.000028
solving larger	1.000000
larger tasks	0.062500
<s> What	0.003075
What	0.000307
What distinguishes	0.090909
distinguishes	0.000056
distinguishes these	0.500000
tasks from	0.031250
from other	0.009615
other potential	0.014286
potential	0.000195
potential and	0.142857
and actual	0.001445
actual	0.000140
actual NLP	0.200000
tasks is	0.031250
not only	0.062500
only the	0.105263
the volume	0.000692
volume	0.000112
volume of	0.500000
research devoted	0.023810
devoted to	0.600000
to them	0.002656
them	0.000530
them but	0.052632
but	0.001897
but the	0.044118
the fact	0.002768
fact	0.000307
fact that	0.454545
that for	0.003546
for each	0.025271
each one	0.044444
one there	0.015385
typically a	0.055556
a well-defined	0.001227
well-defined	0.000028
well-defined problem	1.000000
problem setting	0.022727
setting	0.000140
setting ,	0.400000
a standard	0.002454
standard	0.000391
standard metric	0.071429
metric	0.000084
metric for	0.333333
for evaluating	0.010830
evaluating	0.000140
evaluating the	0.200000
task ,	0.095238
, standard	0.001123
standard corpora	0.071429
corpora on	0.090909
on which	0.014151
task can	0.023810
be evaluated	0.008439
evaluated	0.000195
evaluated ,	0.142857
and competitions	0.001445
competitions	0.000028
competitions devoted	1.000000
the specific	0.002076
specific	0.000586
specific task	0.047619
Automatic summarization	0.222222
summarization	0.001395
summarization :	0.020000
: Produce	0.009804
Produce	0.000028
Produce a	1.000000
a readable	0.001227
readable	0.000084
readable summary	0.333333
summary	0.001172
summary of	0.071429
a chunk	0.007362
chunk	0.000195
chunk of	1.000000
text .	0.169811
<s> Often	0.002306
Often	0.000084
Often used	0.333333
to provide	0.005312
provide summaries	0.166667
summaries	0.001200
summaries of	0.093023
text of	0.006289
a known	0.002454
known	0.000725
known type	0.038462
type	0.000391
type ,	0.071429
as articles	0.003484
articles in	0.250000
the financial	0.000692
financial	0.000112
financial section	0.250000
section	0.000167
section of	0.166667
a newspaper	0.001227
newspaper	0.000084
newspaper .	0.333333
<s> Coreference	0.000769
Coreference	0.000028
Coreference resolution	1.000000
resolution	0.000112
resolution :	0.250000
: Given	0.098039
Given	0.000391
Given a	0.714286
sentence or	0.041667
or larger	0.004505
larger chunk	0.062500
, determine	0.003369
determine	0.000642
determine which	0.086957
which words	0.014493
words -LRB-	0.027523
-LRB- ``	0.021680
`` mentions	0.005291
mentions	0.000084
mentions ''	0.333333
-RRB- refer	0.002710
refer	0.000167
refer to	1.000000
same objects	0.040000
objects	0.000140
objects -LRB-	0.200000
`` entities	0.005291
entities	0.000195
entities ''	0.142857
<s> Anaphora	0.000769
Anaphora	0.000028
Anaphora resolution	1.000000
resolution is	0.250000
a specific	0.006135
specific example	0.047619
of this	0.009804
and is	0.008671
is specifically	0.002033
specifically concerned	0.500000
with matching	0.005464
matching	0.000140
matching up	0.200000
up pronouns	0.045455
pronouns	0.000056
pronouns with	0.500000
the nouns	0.000692
nouns	0.000251
nouns or	0.111111
or names	0.004505
names	0.000195
names that	0.285714
they refer	0.050000
to .	0.003984
For example	0.622951
sentence such	0.020833
`` He	0.005291
He	0.000223
He entered	0.125000
entered	0.000056
entered John	0.500000
John	0.000223
John 's	0.250000
's house	0.039216
house	0.000056
house through	0.500000
the front	0.002076
front	0.000084
front door	1.000000
door	0.000112
door ''	0.500000
'' is	0.046392
a referring	0.001227
referring	0.000056
referring expression	0.500000
expression	0.000279
expression and	0.200000
the bridging	0.000692
bridging	0.000028
bridging relationship	1.000000
relationship	0.000167
relationship to	0.166667
be identified	0.008439
identified	0.000140
identified is	0.200000
the door	0.000692
door being	0.250000
being referred	0.055556
to is	0.001328
door of	0.250000
of John	0.000891
house -LRB-	0.500000
-LRB- rather	0.002710
than of	0.022222
some other	0.084337
other structure	0.014286
structure	0.000335
structure that	0.083333
that might	0.007092
might also	0.038462
also be	0.101449
be referred	0.004219
to -RRB-	0.001328
<s> Discourse	0.002306
Discourse	0.000084
Discourse analysis	1.000000
analysis :	0.061538
: This	0.039216
This rubric	0.015873
rubric	0.000028
rubric includes	1.000000
includes a	0.142857
of related	0.002674
related	0.000419
related tasks	0.200000
<s> One	0.009224
One	0.000363
One task	0.076923
is identifying	0.002033
identifying	0.000167
identifying the	0.666667
the discourse	0.002076
discourse	0.001004
discourse structure	0.027778
structure of	0.333333
of connected	0.000891
connected	0.000140
connected text	0.200000
i.e. the	0.263158
discourse relationships	0.027778
relationships	0.000167
relationships between	0.166667
between sentences	0.051282
sentences -LRB-	0.026316
e.g. elaboration	0.017857
elaboration	0.000028
elaboration ,	1.000000
, explanation	0.000561
explanation	0.000028
explanation ,	1.000000
, contrast	0.000561
contrast	0.000223
contrast -RRB-	0.125000
<s> Another	0.009992
Another	0.000363
Another possible	0.076923
possible task	0.041667
is recognizing	0.002033
recognizing	0.000140
recognizing and	0.200000
and classifying	0.001445
classifying	0.000140
classifying the	0.200000
the speech	0.006920
speech acts	0.019737
acts	0.000084
acts in	0.333333
text -LRB-	0.037736
e.g. yes-no	0.017857
yes-no	0.000028
yes-no question	1.000000
, content	0.000561
content question	0.083333
, statement	0.000561
statement	0.000028
statement ,	1.000000
, assertion	0.000561
assertion	0.000028
assertion ,	1.000000
, etc.	0.011230
etc.	0.000614
etc. -RRB-	0.409091
<s> Machine	0.002306
Machine	0.000251
Machine translation	0.555556
translation :	0.027027
: Automatically	0.009804
Automatically	0.000028
Automatically translate	1.000000
translate	0.000167
translate text	0.333333
text from	0.012579
from one	0.028846
one human	0.015385
human language	0.065217
language to	0.027027
to another	0.003984
another	0.000363
another .	0.230769
This is	0.269841
is one	0.012195
one of	0.215385
most difficult	0.017241
difficult problems	0.107143
problems	0.000474
problems ,	0.352941
a member	0.001227
member	0.000028
member of	1.000000
a class	0.001227
class	0.000112
class of	0.750000
of problems	0.001783
problems colloquially	0.117647
colloquially	0.000056
colloquially termed	1.000000
termed	0.000112
termed ``	0.500000
`` AI-complete	0.010582
AI-complete ''	0.666667
i.e. requiring	0.052632
requiring	0.000056
requiring all	0.500000
the different	0.000692
different types	0.040816
of knowledge	0.001783
knowledge that	0.037037
that humans	0.007092
humans	0.000335
humans possess	0.083333
possess	0.000028
possess -LRB-	1.000000
-LRB- grammar	0.002710
grammar ,	0.108108
, semantics	0.001684
semantics	0.000391
semantics ,	0.285714
, facts	0.000561
facts	0.000028
facts about	1.000000
the real	0.002076
real world	0.333333
world ,	0.066667
-RRB- in	0.010840
in order	0.013109
order	0.000391
order to	0.571429
to solve	0.005312
solve	0.000112
solve properly	0.250000
properly	0.000056
properly .	0.500000
<s> Morphological	0.000769
Morphological	0.000028
Morphological segmentation	1.000000
segmentation	0.000921
segmentation :	0.090909
: Separate	0.019608
Separate	0.000056
Separate words	0.500000
words into	0.036697
into individual	0.012821
individual morphemes	0.083333
morphemes	0.000084
morphemes and	0.333333
and identify	0.002890
identify	0.000335
identify the	0.500000
the class	0.000692
the morphemes	0.000692
morphemes .	0.333333
The difficulty	0.015625
difficulty	0.000195
difficulty of	0.428571
task depends	0.023810
depends greatly	0.125000
greatly	0.000195
greatly on	0.142857
the morphology	0.000692
morphology	0.000195
morphology -LRB-	0.142857
-LRB- i.e.	0.029810
the structure	0.002076
of words	0.013369
words -RRB-	0.027523
-RRB- of	0.018970
the language	0.005536
language being	0.013514
considered .	0.111111
<s> English	0.000769
English has	0.054054
has fairly	0.011905
fairly	0.000112
fairly simple	0.250000
simple morphology	0.038462
morphology ,	0.714286
especially inflectional	0.066667
inflectional	0.000056
inflectional morphology	1.000000
and thus	0.004335
thus	0.000279
thus it	0.100000
often possible	0.022727
to ignore	0.001328
ignore	0.000028
ignore this	1.000000
task entirely	0.023810
entirely	0.000056
entirely and	0.500000
and simply	0.001445
simply model	0.083333
model all	0.033333
all possible	0.069767
possible forms	0.041667
forms	0.000167
forms of	0.333333
word -LRB-	0.016667
e.g. ``	0.017857
`` open	0.005291
open	0.000112
open ,	0.250000
, opens	0.000561
opens	0.000028
opens ,	1.000000
, opened	0.000561
opened	0.000028
opened ,	1.000000
, opening	0.000561
opening	0.000028
opening ''	1.000000
-RRB- as	0.008130
as separate	0.003484
separate	0.000279
separate words	0.300000
In languages	0.009524
languages such	0.100000
as Turkish	0.003484
Turkish	0.000028
Turkish ,	1.000000
such an	0.016260
an approach	0.030303
approach is	0.142857
not possible	0.008929
possible ,	0.125000
, as	0.012914
as each	0.003484
each dictionary	0.022222
dictionary	0.000195
dictionary entry	0.142857
entry	0.000112
entry has	0.250000
has thousands	0.011905
thousands	0.000084
thousands of	0.666667
of possible	0.002674
possible word	0.041667
word forms	0.016667
forms .	0.166667
<s> Named	0.000769
Named	0.000028
Named entity	1.000000
entity	0.000140
entity recognition	0.400000
recognition -LRB-	0.049587
-LRB- NER	0.002710
NER	0.000028
NER -RRB-	1.000000
-RRB- :	0.024390
a stream	0.001227
stream	0.000056
stream of	0.500000
which items	0.007246
items	0.000056
items in	0.500000
the text	0.017993
text map	0.006289
map	0.000056
map to	0.500000
to proper	0.001328
proper	0.000195
proper names	0.142857
names ,	0.285714
as people	0.003484
people	0.000446
people or	0.062500
or places	0.004505
places	0.000056
places ,	0.500000
and what	0.001445
what the	0.125000
the type	0.002768
type of	0.571429
each such	0.022222
such name	0.008130
name	0.000140
name is	0.200000
is -LRB-	0.004065
e.g. person	0.017857
person	0.000530
person ,	0.210526
, location	0.000561
location	0.000028
location ,	1.000000
, organization	0.001123
organization	0.000140
organization -RRB-	0.200000
that ,	0.003546
although capitalization	0.166667
capitalization	0.000084
capitalization can	0.333333
can aid	0.005525
in recognizing	0.003745
recognizing named	0.200000
named	0.000195
named entities	0.428571
entities in	0.142857
in languages	0.001873
as English	0.010453
English ,	0.162162
this information	0.010989
information can	0.021739
can not	0.082873
not aid	0.008929
in determining	0.001873
of named	0.000891
named entity	0.285714
entity ,	0.400000
and in	0.010116
in any	0.001873
any case	0.096774
case	0.000474
case is	0.058824
often inaccurate	0.022727
inaccurate	0.000028
inaccurate or	1.000000
or insufficient	0.004505
insufficient	0.000056
insufficient .	1.000000
first word	0.030303
word of	0.016667
sentence is	0.041667
is also	0.020325
also capitalized	0.014493
capitalized	0.000084
capitalized ,	0.666667
and named	0.001445
entities often	0.142857
often span	0.022727
span	0.000028
span several	1.000000
several	0.000614
several words	0.045455
, only	0.001123
only some	0.052632
which are	0.086957
are capitalized	0.004149
capitalized .	0.333333
<s> Furthermore	0.004612
Furthermore	0.000167
Furthermore ,	1.000000
many other	0.096154
other languages	0.071429
languages in	0.020000
in non-Western	0.001873
non-Western	0.000028
non-Western scripts	1.000000
scripts	0.000084
scripts -LRB-	0.666667
e.g. Chinese	0.017857
Chinese	0.000195
Chinese or	0.142857
or Arabic	0.004505
Arabic	0.000112
Arabic -RRB-	0.250000
-RRB- do	0.002710
do not	0.500000
not have	0.017857
have any	0.009615
any capitalization	0.032258
capitalization at	0.333333
all ,	0.069767
and even	0.008671
even	0.000753
even languages	0.037037
languages with	0.020000
with capitalization	0.005464
capitalization may	0.333333
may	0.001451
may not	0.096154
not consistently	0.017857
consistently	0.000084
consistently use	0.333333
use it	0.027778
it to	0.042735
distinguish names	0.200000
names .	0.285714
, German	0.001123
German	0.000112
German capitalizes	0.250000
capitalizes	0.000028
capitalizes all	1.000000
all nouns	0.023256
nouns ,	0.666667
, regardless	0.001684
regardless	0.000084
regardless of	1.000000
of whether	0.000891
whether they	0.076923
to names	0.001328
and French	0.001445
French	0.000223
French and	0.250000
and Spanish	0.001445
Spanish	0.000056
Spanish do	0.500000
not capitalize	0.008929
capitalize	0.000028
capitalize names	1.000000
that serve	0.003546
as adjectives	0.003484
adjectives	0.000084
adjectives .	0.333333
language generation	0.033784
generation	0.000251
generation :	0.222222
: Convert	0.019608
Convert	0.000056
Convert information	0.500000
from computer	0.009615
computer databases	0.022727
databases	0.000223
databases into	0.125000
into readable	0.012821
readable human	0.333333
language .	0.074324
understanding :	0.030303
Convert chunks	0.500000
chunks	0.000028
chunks of	1.000000
text into	0.044025
into more	0.025641
more formal	0.010526
formal	0.000251
formal representations	0.222222
representations	0.000112
representations such	0.250000
as first-order	0.003484
first-order	0.000028
first-order logic	1.000000
logic	0.000112
logic structures	0.250000
are easier	0.004149
easier	0.000223
easier for	0.125000
for computer	0.010830
computer programs	0.045455
programs	0.000307
programs to	0.090909
manipulate .	0.333333
understanding involves	0.030303
involves	0.000279
involves the	0.200000
the identification	0.001384
identification	0.000140
identification of	0.400000
the intended	0.001384
intended	0.000140
intended semantic	0.200000
semantic	0.000586
semantic from	0.047619
the multiple	0.000692
multiple possible	0.153846
possible semantics	0.041667
semantics which	0.142857
be derived	0.008439
derived	0.000167
derived from	0.500000
from a	0.115385
language expression	0.006757
expression which	0.100000
which usually	0.007246
usually	0.000893
usually takes	0.031250
takes	0.000084
takes the	0.333333
the form	0.000692
form	0.000558
form of	0.350000
of organized	0.000891
organized	0.000028
organized notations	1.000000
notations	0.000056
notations of	0.500000
natural languages	0.120000
languages concepts	0.020000
concepts	0.000140
concepts .	0.400000
<s> Introduction	0.000769
Introduction	0.000028
Introduction and	1.000000
and creation	0.001445
creation	0.000056
creation of	1.000000
of language	0.004456
language metamodel	0.006757
metamodel	0.000028
metamodel and	1.000000
and ontology	0.001445
ontology	0.000056
ontology are	0.500000
are efficient	0.004149
efficient	0.000084
efficient however	0.333333
however empirical	0.076923
empirical	0.000028
empirical solutions	1.000000
solutions	0.000056
solutions .	0.500000
An explicit	0.062500
explicit	0.000140
explicit formalization	0.200000
formalization	0.000056
formalization of	0.500000
languages semantics	0.020000
semantics without	0.071429
without confusions	0.076923
confusions	0.000028
confusions with	1.000000
with implicit	0.005464
implicit	0.000028
implicit assumptions	1.000000
assumptions	0.000140
assumptions such	0.200000
as closed	0.003484
closed	0.000028
closed world	1.000000
world assumption	0.133333
assumption	0.000056
assumption -LRB-	0.500000
-LRB- CWA	0.002710
CWA	0.000028
CWA -RRB-	1.000000
-RRB- vs.	0.002710
vs.	0.000335
vs. open	0.083333
open world	0.250000
assumption ,	0.500000
or subjective	0.009009
subjective	0.000167
subjective Yes\/No	0.166667
Yes\/No	0.000028
Yes\/No vs.	1.000000
vs. objective	0.083333
objective	0.000140
objective True\/False	0.200000
True\/False	0.000028
True\/False is	1.000000
is expected	0.002033
expected	0.000195
expected for	0.142857
the construction	0.000692
construction	0.000084
construction of	0.666667
a basis	0.002454
of semantics	0.000891
semantics formalization	0.071429
formalization .	0.500000
<s> Optical	0.001537
Optical	0.000084
Optical character	0.666667
character	0.000614
character recognition	0.500000
-LRB- OCR	0.002710
OCR	0.001367
OCR -RRB-	0.020408
Given an	0.071429
an image	0.007576
image	0.000084
image representing	0.333333
representing	0.000056
representing printed	0.500000
printed	0.000335
printed text	0.250000
determine the	0.391304
corresponding text	0.166667
<s> Part-of-speech	0.000769
Part-of-speech	0.000056
Part-of-speech tagging	0.500000
tagging :	0.040000
Many words	0.166667
especially common	0.133333
common ones	0.040000
ones	0.000279
ones ,	0.300000
, can	0.003369
can serve	0.011050
as multiple	0.003484
multiple parts	0.076923
parts	0.000446
parts of	1.000000
speech .	0.065789
`` book	0.005291
book	0.000223
book ''	0.125000
'' can	0.025773
a noun	0.007362
noun	0.000391
noun -LRB-	0.071429
the book	0.001384
book on	0.125000
the table	0.001384
table	0.000195
table ''	0.142857
-RRB- or	0.010840
or verb	0.004505
verb	0.000363
verb -LRB-	0.153846
`` to	0.010582
to book	0.001328
book a	0.125000
a flight	0.001227
flight	0.000056
flight ''	0.500000
-RRB- ;	0.021680
; ``	0.021277
`` set	0.005291
set ''	0.051282
noun ,	0.428571
, verb	0.001123
verb or	0.230769
or adjective	0.004505
adjective	0.000195
adjective ;	0.142857
; and	0.085106
and ``	0.028902
`` out	0.005291
out	0.000391
out ''	0.071429
be any	0.004219
any of	0.064516
of at	0.000891
at least	0.073529
least	0.000140
least five	0.200000
five different	0.200000
different parts	0.040816
some languages	0.024096
languages have	0.040000
have more	0.028846
more such	0.010526
such ambiguity	0.024390
ambiguity	0.000223
ambiguity than	0.125000
than others	0.044444
others .	0.250000
<s> Languages	0.002306
Languages	0.000084
Languages with	0.333333
with little	0.005464
little	0.000084
little inflectional	0.333333
English are	0.027027
are particularly	0.004149
particularly	0.000140
particularly prone	0.200000
prone	0.000056
prone to	1.000000
to such	0.002656
ambiguity .	0.125000
<s> Chinese	0.000769
Chinese is	0.142857
is prone	0.002033
ambiguity because	0.125000
a tonal	0.001227
tonal	0.000028
tonal language	1.000000
language during	0.006757
during verbalization	0.100000
verbalization	0.000028
verbalization .	1.000000
Such inflection	0.125000
inflection	0.000028
inflection is	1.000000
not readily	0.008929
readily	0.000084
readily conveyed	0.333333
conveyed	0.000028
conveyed via	1.000000
via	0.000028
via the	1.000000
the entities	0.000692
entities employed	0.142857
employed	0.000028
employed within	1.000000
within the	0.166667
the orthography	0.000692
orthography	0.000056
orthography to	0.500000
to convey	0.003984
convey	0.000084
convey intended	0.333333
intended meaning	0.200000
meaning	0.000642
meaning .	0.086957
<s> Parsing	0.003075
Parsing	0.000140
Parsing :	0.200000
: Determine	0.009804
Determine	0.000028
Determine the	1.000000
the parse	0.000692
parse	0.000251
parse tree	0.111111
tree	0.000028
tree -LRB-	1.000000
-LRB- grammatical	0.002710
grammatical	0.000307
grammatical analysis	0.090909
analysis -RRB-	0.030769
sentence .	0.145833
The grammar	0.010417
grammar for	0.027027
for natural	0.014440
languages is	0.020000
is ambiguous	0.002033
ambiguous	0.000335
ambiguous and	0.166667
and typical	0.001445
typical sentences	0.111111
sentences have	0.026316
have multiple	0.009615
possible analyses	0.083333
analyses	0.000140
analyses .	0.400000
In fact	0.038095
fact ,	0.454545
, perhaps	0.001684
perhaps	0.000167
perhaps surprisingly	0.166667
surprisingly	0.000084
surprisingly ,	0.333333
a typical	0.002454
typical sentence	0.111111
sentence there	0.020833
there may	0.025000
may be	0.403846
be thousands	0.004219
of potential	0.001783
potential parses	0.142857
parses	0.000056
parses -LRB-	0.500000
-LRB- most	0.010840
most of	0.086207
which will	0.021739
will seem	0.028571
seem	0.000056
seem completely	0.500000
completely	0.000028
completely nonsensical	1.000000
nonsensical	0.000028
nonsensical to	1.000000
to a	0.037185
human -RRB-	0.043478
<s> Question	0.003843
Question	0.000195
Question answering	0.285714
answering	0.000335
answering :	0.083333
a human-language	0.001227
human-language	0.000028
human-language question	1.000000
determine its	0.086957
its answer	0.028571
answer	0.000837
answer .	0.233333
<s> Typical	0.001537
Typical	0.000056
Typical questions	0.500000
questions	0.000725
questions have	0.038462
have a	0.125000
specific right	0.047619
right answer	0.100000
answer -LRB-	0.033333
-LRB- such	0.021680
`` What	0.015873
What is	0.272727
the capital	0.001384
capital	0.000084
capital of	0.666667
Canada ?	0.166667
<s> ,	0.000769
, but	0.026951
but sometimes	0.014706
sometimes open-ended	0.076923
open-ended	0.000028
open-ended questions	1.000000
questions are	0.076923
are also	0.033195
also considered	0.014493
considered -LRB-	0.111111
the meaning	0.006920
meaning of	0.304348
of life	0.000891
life	0.000112
life ?	0.250000
<s> Relationship	0.000769
Relationship	0.000028
Relationship extraction	1.000000
extraction	0.000865
extraction :	0.064516
, identify	0.001123
the relationships	0.000692
relationships among	0.166667
among named	0.125000
entities -LRB-	0.142857
e.g. who	0.017857
who	0.000279
who is	0.200000
the wife	0.000692
wife	0.000028
wife of	1.000000
of whom	0.000891
whom	0.000056
whom -RRB-	0.500000
<s> Sentence	0.002306
Sentence	0.000140
Sentence breaking	0.200000
breaking	0.000056
breaking -LRB-	0.500000
also known	0.086957
known as	0.384615
as sentence	0.006969
sentence boundary	0.062500
boundary	0.000167
boundary disambiguation	0.333333
disambiguation	0.000279
disambiguation -RRB-	0.200000
, find	0.001123
find	0.000363
find the	0.307692
the sentence	0.004152
sentence boundaries	0.083333
boundaries	0.000307
boundaries .	0.363636
Sentence boundaries	0.200000
boundaries are	0.090909
are often	0.016598
often marked	0.022727
marked	0.000084
marked by	0.333333
by periods	0.005714
periods	0.000084
periods or	0.333333
or other	0.009009
other punctuation	0.014286
punctuation	0.000195
punctuation marks	0.285714
marks	0.000112
marks ,	0.250000
but these	0.014706
these same	0.023810
same characters	0.040000
characters	0.000446
characters can	0.187500
serve other	0.200000
other purposes	0.014286
purposes	0.000112
purposes -LRB-	0.250000
e.g. marking	0.017857
marking	0.000056
marking abbreviations	0.500000
abbreviations	0.000140
abbreviations -RRB-	0.200000
<s> Sentiment	0.003843
Sentiment	0.000167
Sentiment analysis	0.833333
: Extract	0.009804
Extract	0.000028
Extract subjective	1.000000
subjective information	0.333333
information usually	0.021739
usually from	0.031250
documents ,	0.236842
, often	0.001684
often using	0.022727
using online	0.016949
online reviews	0.250000
reviews	0.000167
reviews to	0.166667
to determine	0.014608
determine ``	0.043478
`` polarity	0.005291
polarity	0.000223
polarity ''	0.250000
'' about	0.005155
about specific	0.025000
specific objects	0.047619
objects .	0.200000
is especially	0.004065
especially useful	0.066667
useful	0.000391
useful for	0.214286
for identifying	0.003610
identifying trends	0.166667
trends	0.000028
trends of	1.000000
of public	0.000891
public	0.000028
public opinion	1.000000
opinion	0.000140
opinion in	0.400000
the social	0.002076
social	0.000391
social media	0.285714
media	0.000167
media ,	0.500000
the purpose	0.001384
purpose	0.000140
purpose of	0.200000
of marketing	0.000891
marketing	0.000028
marketing .	1.000000
<s> Speech	0.011530
Speech	0.000865
Speech recognition	0.290323
recognition :	0.016529
a sound	0.008589
sound	0.000558
sound clip	0.100000
clip	0.000056
clip of	1.000000
a person	0.013497
person or	0.105263
or people	0.009009
people speaking	0.125000
speaking	0.000223
speaking ,	0.625000
the textual	0.000692
textual representation	0.200000
representation	0.000530
representation of	0.105263
the opposite	0.001384
opposite	0.000056
opposite of	1.000000
text to	0.044025
to speech	0.003984
speech and	0.013158
the extremely	0.000692
-LRB- see	0.032520
see above	0.050000
above -RRB-	0.076923
In natural	0.009524
natural speech	0.026667
speech there	0.006579
there are	0.375000
are hardly	0.004149
hardly	0.000028
hardly any	1.000000
any pauses	0.032258
pauses	0.000112
pauses between	0.500000
between successive	0.025641
successive	0.000056
successive words	0.500000
thus speech	0.100000
speech segmentation	0.032895
segmentation is	0.272727
a necessary	0.001227
necessary	0.000279
necessary subtask	0.100000
subtask	0.000056
subtask of	1.000000
see below	0.050000
below	0.000140
below -RRB-	0.400000
Note also	0.111111
also that	0.014493
that in	0.007092
in most	0.007491
most spoken	0.034483
spoken	0.000391
spoken languages	0.142857
languages ,	0.220000
the sounds	0.001384
sounds	0.000419
sounds representing	0.066667
representing successive	0.500000
successive letters	0.500000
letters	0.000279
letters blend	0.100000
blend	0.000084
blend into	0.333333
into each	0.012821
each other	0.133333
other in	0.014286
a process	0.004908
process termed	0.027778
termed coarticulation	0.250000
coarticulation	0.000028
coarticulation ,	1.000000
, so	0.006738
so the	0.233333
the conversion	0.000692
conversion	0.000084
conversion of	0.666667
the analog	0.000692
analog	0.000056
analog signal	0.500000
signal	0.000167
signal to	0.166667
to discrete	0.001328
discrete	0.000084
discrete characters	0.333333
very difficult	0.048780
difficult process	0.035714
Speech segmentation	0.096774
, separate	0.001123
separate it	0.200000
it into	0.042735
into words	0.038462
A subtask	0.020000
recognition and	0.057851
typically grouped	0.055556
grouped	0.000056
grouped with	0.500000
with it	0.010929
<s> Topic	0.000769
Topic	0.000028
Topic segmentation	1.000000
segmentation and	0.060606
and recognition	0.001445
into segments	0.025641
segments	0.000140
segments each	0.200000
is devoted	0.002033
a topic	0.001227
topic	0.000223
topic ,	0.250000
the topic	0.001384
topic of	0.125000
the segment	0.000692
segment	0.000251
segment .	0.111111
<s> Word	0.003843
Word	0.000195
Word segmentation	0.142857
Separate a	0.500000
of continuous	0.001783
continuous	0.000167
continuous text	0.166667
into separate	0.025641
a language	0.007362
language like	0.006757
like	0.000781
like English	0.071429
this is	0.098901
is fairly	0.002033
fairly trivial	0.250000
trivial	0.000112
trivial ,	0.250000
, since	0.002807
since	0.000279
since words	0.100000
words are	0.091743
are usually	0.012448
usually separated	0.031250
separated	0.000084
separated by	0.666667
by spaces	0.005714
spaces	0.000140
spaces .	0.200000
, some	0.005053
some written	0.024096
written languages	0.192308
languages like	0.020000
like Chinese	0.071429
Chinese ,	0.285714
, Japanese	0.001123
Japanese	0.000223
Japanese and	0.250000
and Thai	0.001445
Thai	0.000056
Thai do	0.500000
not mark	0.008929
mark	0.000084
mark word	0.333333
word boundaries	0.016667
boundaries in	0.090909
in such	0.005618
a fashion	0.001227
fashion	0.000028
fashion ,	1.000000
in those	0.001873
those languages	0.090909
languages text	0.020000
text segmentation	0.044025
a significant	0.001227
significant task	0.111111
task requiring	0.023810
requiring knowledge	0.500000
knowledge of	0.148148
the vocabulary	0.000692
vocabulary	0.000223
vocabulary and	0.250000
and morphology	0.001445
morphology of	0.142857
words in	0.091743
Word sense	0.285714
sense	0.000223
sense disambiguation	0.250000
disambiguation :	0.100000
: Many	0.009804
words have	0.009174
than one	0.066667
one meaning	0.030769
meaning ;	0.043478
; we	0.021277
we	0.001256
we have	0.066667
have to	0.019231
to select	0.005312
select	0.000167
select the	0.333333
meaning which	0.043478
which makes	0.021739
makes the	0.250000
most sense	0.017241
sense in	0.125000
in context	0.007491
context	0.000921
context .	0.212121
For this	0.049180
this problem	0.076923
problem ,	0.090909
, we	0.009545
we are	0.044444
are typically	0.012448
typically given	0.055556
given a	0.166667
words and	0.064220
and associated	0.001445
associated	0.000112
associated word	0.250000
word senses	0.016667
senses	0.000056
senses ,	0.500000
, e.g.	0.005615
e.g. from	0.017857
a dictionary	0.003681
dictionary or	0.142857
or from	0.004505
from an	0.009615
an online	0.007576
online resource	0.125000
resource	0.000140
resource such	0.200000
as WordNet	0.003484
WordNet	0.000056
WordNet .	0.500000
In some	0.038095
some cases	0.048193
, sets	0.000561
tasks are	0.125000
are grouped	0.004149
grouped into	0.500000
into subfields	0.012821
subfields	0.000028
subfields of	1.000000
NLP that	0.021277
often considered	0.022727
considered separately	0.111111
separately	0.000028
separately from	1.000000
from NLP	0.009615
NLP as	0.021277
a whole	0.002454
whole	0.000251
whole .	0.111111
Examples include	0.333333
include	0.000753
include :	0.111111
: Information	0.009804
Information	0.000140
Information retrieval	0.200000
retrieval	0.000195
retrieval -LRB-	0.142857
-LRB- IR	0.002710
IR	0.000084
IR -RRB-	0.333333
is concerned	0.004065
with storing	0.005464
storing	0.000028
storing ,	1.000000
, searching	0.000561
searching	0.000084
searching and	0.333333
and retrieving	0.001445
retrieving	0.000028
retrieving information	1.000000
information .	0.086957
a separate	0.002454
separate field	0.100000
field within	0.037037
within computer	0.055556
science -LRB-	0.100000
-LRB- closer	0.002710
closer	0.000056
closer to	1.000000
to databases	0.001328
databases -RRB-	0.125000
but IR	0.014706
IR relies	0.333333
relies	0.000028
relies on	1.000000
on some	0.042453
some NLP	0.012048
NLP methods	0.021277
methods -LRB-	0.045455
-LRB- for	0.018970
, stemming	0.000561
stemming	0.000056
stemming -RRB-	0.500000
Some current	0.095238
current	0.000195
current research	0.142857
research and	0.119048
and applications	0.001445
applications seek	0.040000
seek	0.000028
seek to	1.000000
to bridge	0.001328
bridge	0.000028
bridge the	1.000000
the gap	0.000692
gap	0.000028
gap between	1.000000
between IR	0.025641
IR and	0.333333
and NLP	0.002890
<s> Information	0.000769
Information extraction	0.200000
extraction -LRB-	0.064516
-LRB- IE	0.005420
IE	0.000084
IE -RRB-	0.666667
concerned in	0.200000
in general	0.009363
general with	0.045455
the extraction	0.002768
extraction of	0.096774
of semantic	0.004456
semantic information	0.095238
from text	0.019231
This covers	0.031746
covers	0.000112
covers tasks	0.250000
tasks such	0.062500
as named	0.003484
recognition ,	0.115702
, coreference	0.000561
coreference	0.000028
coreference resolution	1.000000
resolution ,	0.250000
, relationship	0.000561
relationship extraction	0.333333
extraction ,	0.193548
etc. .	0.409091
Speech processing	0.032258
processing :	0.018519
covers speech	0.250000
, text-to-speech	0.001123
text-to-speech	0.000112
text-to-speech and	0.500000
and related	0.004335
<s> Other	0.005380
Other	0.000195
Other tasks	0.142857
tasks include	0.031250
: Stemming	0.009804
Stemming	0.000028
Stemming Text	1.000000
Text	0.000167
Text simplification	0.166667
simplification	0.000028
simplification Text-to-speech	1.000000
Text-to-speech	0.000028
Text-to-speech Text-proofing	1.000000
Text-proofing	0.000028
Text-proofing Natural	1.000000
language search	0.006757
search	0.000307
search Query	0.090909
Query	0.000028
Query expansion	1.000000
expansion	0.000084
expansion Automated	0.333333
Automated	0.000056
Automated essay	0.500000
essay	0.000028
essay scoring	1.000000
scoring	0.000056
scoring Truecasing	0.500000
Truecasing	0.000028
Truecasing Statistical	1.000000
Statistical	0.000251
Statistical NLP	0.222222
NLP Main	0.021277
Main	0.000335
Main article	1.000000
article :	0.448276
: statistical	0.009804
statistical natural	0.030303
processing Statistical	0.018519
Statistical natural-language	0.111111
natural-language	0.000028
natural-language processing	1.000000
processing uses	0.018519
uses	0.000391
uses stochastic	0.071429
stochastic	0.000223
stochastic ,	0.250000
probabilistic and	0.142857
and statistical	0.004335
statistical methods	0.121212
methods to	0.090909
to resolve	0.003984
resolve	0.000112
resolve some	0.250000
the difficulties	0.000692
difficulties	0.000056
difficulties discussed	0.500000
discussed	0.000195
discussed above	0.142857
especially those	0.200000
those which	0.045455
which arise	0.007246
arise	0.000028
arise because	1.000000
because longer	0.033333
longer	0.000028
longer sentences	1.000000
sentences are	0.092105
are highly	0.004149
highly	0.000251
highly ambiguous	0.111111
ambiguous when	0.083333
when processed	0.028571
processed with	0.333333
with realistic	0.005464
realistic	0.000028
realistic grammars	1.000000
grammars	0.000391
grammars ,	0.142857
, yielding	0.000561
yielding	0.000028
yielding thousands	1.000000
thousands or	0.333333
or millions	0.004505
millions	0.000056
millions of	1.000000
<s> Methods	0.002306
Methods	0.000112
Methods for	0.500000
for disambiguation	0.003610
disambiguation often	0.100000
often involve	0.022727
involve	0.000167
involve the	0.166667
the use	0.010381
of corpora	0.000891
corpora and	0.090909
and Markov	0.001445
Markov	0.000502
Markov models	0.333333
<s> Statistical	0.002306
NLP comprises	0.021277
comprises	0.000028
comprises all	1.000000
all quantitative	0.023256
quantitative	0.000112
quantitative approaches	0.250000
to automated	0.001328
automated language	0.142857
processing ,	0.166667
including probabilistic	0.071429
probabilistic modeling	0.142857
modeling	0.000195
modeling ,	0.142857
, information	0.001123
information theory	0.021739
and linear	0.001445
linear	0.000195
linear algebra	0.142857
algebra	0.000056
algebra .	0.500000
The technology	0.005208
technology	0.000614
technology for	0.090909
for statistical	0.007220
NLP comes	0.021277
comes	0.000140
comes mainly	0.200000
mainly	0.000167
mainly from	0.166667
from machine	0.019231
learning and	0.023256
and data	0.005780
data mining	0.025974
mining	0.000140
mining ,	0.200000
, both	0.001684
both of	0.064516
are fields	0.004149
fields of	0.333333
of artificial	0.000891
intelligence that	0.250000
that involve	0.003546
involve learning	0.166667
<s> Evaluation	0.003843
Evaluation	0.000251
Evaluation of	0.111111
processing Objectives	0.018519
Objectives	0.000028
Objectives The	1.000000
The goal	0.005208
goal	0.000195
goal of	0.285714
NLP evaluation	0.063830
evaluation is	0.074074
is to	0.038618
to measure	0.005312
measure	0.000307
measure one	0.090909
one or	0.030769
more qualities	0.010526
qualities	0.000056
qualities of	0.500000
an algorithm	0.022727
algorithm or	0.035714
or a	0.085586
a system	0.012270
system ,	0.107527
determine whether	0.043478
whether -LRB-	0.076923
or to	0.009009
to what	0.005312
what extent	0.031250
extent	0.000112
extent -RRB-	0.250000
-RRB- the	0.002710
the system	0.013841
system answers	0.010753
answers the	0.083333
the goals	0.000692
goals	0.000028
goals of	1.000000
of its	0.007130
its designers	0.028571
designers	0.000028
designers ,	1.000000
or meets	0.004505
meets the	0.500000
the needs	0.000692
needs	0.000279
needs of	0.100000
its users	0.028571
users	0.000251
users .	0.222222
Research in	0.125000
evaluation has	0.018519
has received	0.011905
received	0.000056
received considerable	0.500000
considerable	0.000140
considerable attention	0.200000
attention	0.000056
attention ,	0.500000
, because	0.004492
because the	0.133333
the definition	0.002076
definition	0.000140
definition of	0.600000
of proper	0.000891
proper evaluation	0.142857
evaluation criteria	0.037037
criteria	0.000112
criteria is	0.250000
one way	0.015385
way	0.000670
way to	0.416667
to specify	0.001328
specify	0.000028
specify precisely	1.000000
precisely	0.000028
precisely an	1.000000
an NLP	0.022727
NLP problem	0.042553
, going	0.000561
going	0.000112
going thus	0.250000
thus beyond	0.100000
beyond the	0.500000
the vagueness	0.000692
vagueness	0.000028
vagueness of	1.000000
of tasks	0.000891
tasks defined	0.031250
defined	0.000167
defined only	0.166667
only as	0.026316
as language	0.003484
understanding or	0.030303
or language	0.004505
generation .	0.222222
A precise	0.020000
precise	0.000084
precise set	0.333333
of evaluation	0.004456
criteria ,	0.250000
which includes	0.014493
includes mainly	0.142857
mainly evaluation	0.166667
evaluation data	0.018519
data and	0.025974
and evaluation	0.004335
evaluation metrics	0.018519
metrics	0.000251
metrics ,	0.111111
, enables	0.000561
enables	0.000028
enables several	1.000000
several teams	0.045455
teams	0.000056
teams to	0.500000
to compare	0.005312
compare	0.000195
compare their	0.142857
their	0.000949
their solutions	0.029412
solutions to	0.500000
given NLP	0.041667
<s> Short	0.000769
Short	0.000028
Short history	1.000000
history	0.000112
history of	0.500000
evaluation in	0.055556
first evaluation	0.030303
evaluation campaign	0.018519
campaign	0.000140
campaign on	0.200000
on written	0.004717
written texts	0.076923
texts seems	0.058824
a campaign	0.001227
campaign dedicated	0.200000
dedicated	0.000084
dedicated to	0.666667
to message	0.001328
message	0.000056
message understanding	0.500000
understanding in	0.030303
in 1987	0.003745
1987	0.000084
1987 -LRB-	0.333333
-LRB- Pallet	0.002710
Pallet	0.000056
Pallet 1998	0.500000
1998	0.000112
1998 -RRB-	0.500000
<s> Then	0.003843
Then	0.000140
Then ,	0.400000
the Parseval\/GEIG	0.000692
Parseval\/GEIG	0.000028
Parseval\/GEIG project	1.000000
project	0.000363
project compared	0.153846
compared	0.000195
compared phrase-structure	0.142857
phrase-structure	0.000028
phrase-structure grammars	1.000000
grammars -LRB-	0.071429
-LRB- Black	0.002710
Black	0.000056
Black 1991	0.500000
1991	0.000084
1991 -RRB-	0.666667
A series	0.020000
series	0.000223
series of	0.875000
of campaigns	0.000891
campaigns	0.000056
campaigns within	0.500000
within Tipster	0.055556
Tipster	0.000028
Tipster project	1.000000
project were	0.076923
were realized	0.024390
realized	0.000028
realized on	1.000000
on tasks	0.009434
tasks like	0.062500
like summarization	0.035714
summarization ,	0.080000
, translation	0.001123
translation and	0.040541
and searching	0.001445
searching -LRB-	0.333333
-LRB- Hirschman	0.002710
Hirschman	0.000056
Hirschman 1998	0.500000
In 1994	0.009524
1994	0.000028
1994 ,	1.000000
in Germany	0.003745
Germany	0.000056
Germany ,	0.500000
the Morpholympics	0.000692
Morpholympics	0.000028
Morpholympics compared	1.000000
compared German	0.142857
German taggers	0.250000
taggers	0.000195
taggers .	0.142857
the Senseval	0.000692
Senseval	0.000028
Senseval and	1.000000
and Romanseval	0.001445
Romanseval	0.000028
Romanseval campaigns	1.000000
campaigns were	0.500000
were conducted	0.024390
conducted with	0.200000
the objectives	0.000692
objectives of	0.500000
semantic disambiguation	0.047619
disambiguation .	0.100000
In 1996	0.009524
1996	0.000028
1996 ,	1.000000
the Sparkle	0.000692
Sparkle	0.000028
Sparkle campaign	1.000000
campaign compared	0.200000
compared syntactic	0.142857
syntactic	0.000363
syntactic parsers	0.076923
parsers	0.000363
parsers in	0.076923
in four	0.003745
four	0.000195
four different	0.285714
different languages	0.020408
languages -LRB-	0.040000
-LRB- English	0.002710
, French	0.000561
French ,	0.125000
German and	0.250000
and Italian	0.001445
Italian	0.000056
Italian -RRB-	0.500000
In France	0.019048
France	0.000112
France ,	0.500000
the Grace	0.000692
Grace	0.000028
Grace project	1.000000
compared a	0.142857
of 21	0.000891
21	0.000028
21 taggers	1.000000
taggers for	0.142857
for French	0.010830
French in	0.125000
in 1997	0.003745
1997	0.000056
1997 -LRB-	0.500000
-LRB- Adda	0.002710
Adda	0.000056
Adda 1999	0.500000
1999	0.000056
1999 -RRB-	0.500000
In 2004	0.009524
2004	0.000084
2004 ,	0.333333
, during	0.000561
during the	0.400000
the Technolangue\/Easy	0.000692
Technolangue\/Easy	0.000056
Technolangue\/Easy project	0.500000
project ,	0.384615
, 13	0.000561
13	0.000056
13 parsers	0.500000
parsers for	0.153846
French were	0.250000
were compared	0.048780
compared .	0.285714
<s> Large-scale	0.000769
Large-scale	0.000028
Large-scale evaluation	1.000000
evaluation of	0.074074
of dependency	0.000891
dependency	0.000140
dependency parsers	0.200000
parsers were	0.076923
were performed	0.024390
performed	0.000279
performed in	0.200000
the context	0.004152
context of	0.151515
the CoNLL	0.000692
CoNLL	0.000028
CoNLL shared	1.000000
shared	0.000056
shared tasks	0.500000
in 2006	0.001873
2006	0.000084
2006 and	0.333333
and 2007	0.001445
2007	0.000140
2007 .	0.400000
In Italy	0.009524
Italy	0.000056
Italy ,	1.000000
the EVALITA	0.000692
EVALITA	0.000056
EVALITA campaign	0.500000
campaign was	0.200000
conducted in	0.400000
in 2007	0.001873
2007 and	0.200000
and 2009	0.001445
2009	0.000084
2009 to	0.333333
compare various	0.142857
various NLP	0.055556
NLP and	0.021277
and speech	0.001445
speech tools	0.006579
tools	0.000167
tools for	0.166667
for Italian	0.003610
Italian ;	0.500000
; the	0.085106
the 2011	0.000692
2011	0.000056
2011 campaign	0.500000
campaign is	0.200000
is in	0.006098
in full	0.001873
full	0.000140
full progress	0.200000
progress -	0.142857
-	0.000446
- EVALITA	0.062500
EVALITA web	0.500000
web site	0.250000
site	0.000056
site .	1.000000
, within	0.000561
the ANR-Passage	0.000692
ANR-Passage	0.000028
ANR-Passage project	1.000000
project -LRB-	0.076923
-LRB- end	0.002710
end	0.000223
end of	0.250000
of 2007	0.001783
2007 -RRB-	0.200000
, 10	0.001123
10	0.000223
10 parsers	0.125000
compared -	0.142857
- passage	0.062500
passage	0.000028
passage web	1.000000
<s> Adda	0.000769
Adda G.	0.500000
G.	0.000056
G. ,	0.500000
, Mariani	0.000561
Mariani	0.000028
Mariani J.	1.000000
J.	0.000084
J. ,	0.666667
, Paroubek	0.000561
Paroubek	0.000028
Paroubek P.	1.000000
P.	0.000056
P. ,	1.000000
, Rajman	0.000561
Rajman	0.000028
Rajman M.	1.000000
M.	0.000112
M. 1999	0.250000
1999 L'action	0.500000
L'action	0.000028
L'action GRACE	1.000000
GRACE	0.000028
GRACE d'évaluation	1.000000
d'évaluation	0.000028
d'évaluation de	1.000000
de	0.000056
de l'assignation	0.500000
l'assignation	0.000028
l'assignation des	1.000000
des	0.000028
des parties	1.000000
parties	0.000028
parties du	1.000000
du	0.000028
du discors	1.000000
discors	0.000028
discors pour	1.000000
pour	0.000028
pour le	1.000000
le	0.000028
le français	1.000000
français	0.000028
français .	1.000000
<s> Langues	0.000769
Langues	0.000028
Langues vol-2	1.000000
vol-2	0.000028
vol-2 Black	1.000000
Black E.	0.500000
E.	0.000112
E. ,	0.250000
, Abney	0.000561
Abney	0.000028
Abney S.	1.000000
S.	0.000056
S. ,	1.000000
, Flickinger	0.000561
Flickinger	0.000028
Flickinger D.	1.000000
D.	0.000140
D. ,	0.400000
, Gdaniec	0.000561
Gdaniec	0.000028
Gdaniec C.	1.000000
C.	0.000028
C. ,	1.000000
, Grishman	0.000561
Grishman	0.000028
Grishman R.	1.000000
R.	0.000167
R. ,	0.333333
, Harrison	0.000561
Harrison	0.000028
Harrison P.	1.000000
, Hindle	0.000561
Hindle	0.000028
Hindle D.	1.000000
, Ingria	0.000561
Ingria	0.000028
Ingria R.	1.000000
, Jelinek	0.000561
Jelinek	0.000056
Jelinek F.	0.500000
F.	0.000028
F. ,	1.000000
, Klavans	0.000561
Klavans	0.000028
Klavans J.	1.000000
, Liberman	0.000561
Liberman	0.000028
Liberman M.	1.000000
M. ,	0.500000
, Marcus	0.000561
Marcus	0.000028
Marcus M.	1.000000
, Reukos	0.000561
Reukos	0.000028
Reukos S.	1.000000
, Santoni	0.000561
Santoni	0.000028
Santoni B.	1.000000
B.	0.000028
B. ,	1.000000
, Strzalkowski	0.000561
Strzalkowski	0.000028
Strzalkowski T.	1.000000
T.	0.000028
T. 1991	1.000000
1991 A	0.333333
A procedure	0.020000
procedure	0.000084
procedure for	0.333333
for quantitatively	0.003610
quantitatively	0.000028
quantitatively comparing	1.000000
comparing	0.000056
comparing the	0.500000
the syntactic	0.000692
syntactic coverage	0.076923
coverage	0.000084
coverage of	0.333333
of English	0.002674
English grammars	0.027027
grammars .	0.142857
<s> DARPA	0.000769
DARPA	0.000112
DARPA Speech	0.250000
Speech and	0.161290
and Natural	0.001445
Natural Language	0.230769
Language	0.000335
Language Workshop	0.083333
Workshop	0.000028
Workshop Hirschman	1.000000
Hirschman L.	0.500000
L.	0.000028
L. 1998	1.000000
1998 Language	0.250000
Language understanding	0.083333
understanding evaluation	0.030303
evaluation :	0.037037
: lessons	0.009804
lessons	0.000028
lessons learned	1.000000
learned from	0.200000
from MUC	0.009615
MUC	0.000028
MUC and	1.000000
and ATIS	0.001445
ATIS	0.000028
ATIS .	1.000000
<s> LREC	0.001537
LREC	0.000056
LREC Granada	1.000000
Granada	0.000056
Granada Pallet	0.500000
Pallet D.S.	0.500000
D.S.	0.000028
D.S. 1998	1.000000
1998 The	0.250000
The NIST	0.005208
NIST	0.000056
NIST role	0.500000
role	0.000112
role in	0.250000
in automatic	0.003745
automatic speech	0.130435
recognition benchmark	0.008264
benchmark	0.000028
benchmark tests	1.000000
tests	0.000112
tests .	0.250000
Granada Different	0.500000
Different	0.000056
Different types	1.000000
evaluation Depending	0.018519
Depending	0.000028
Depending on	1.000000
evaluation procedures	0.018519
procedures ,	0.250000
of distinctions	0.000891
distinctions	0.000056
distinctions are	0.500000
are traditionally	0.008299
traditionally	0.000056
traditionally made	0.500000
made in	0.125000
evaluation .	0.055556
<s> Intrinsic	0.001537
Intrinsic	0.000084
Intrinsic vs.	0.333333
vs. extrinsic	0.083333
extrinsic	0.000167
extrinsic evaluation	0.500000
evaluation Intrinsic	0.018519
Intrinsic evaluation	0.333333
evaluation considers	0.018519
considers	0.000056
considers an	0.500000
an isolated	0.007576
isolated	0.000140
isolated NLP	0.200000
NLP system	0.085106
system and	0.032258
and characterizes	0.001445
characterizes	0.000028
characterizes its	1.000000
its performance	0.028571
performance	0.000502
performance mainly	0.055556
mainly with	0.166667
with respect	0.038251
respect	0.000195
respect to	1.000000
a gold	0.002454
gold	0.000167
gold standard	0.833333
standard result	0.071429
, pre-defined	0.000561
pre-defined	0.000056
pre-defined by	0.500000
the evaluators	0.000692
evaluators	0.000028
evaluators .	1.000000
<s> Extrinsic	0.001537
Extrinsic	0.000056
Extrinsic evaluation	0.500000
evaluation ,	0.055556
, also	0.002807
called evaluation	0.111111
in use	0.003745
use considers	0.013889
considers the	0.500000
the NLP	0.000692
system in	0.021505
a more	0.004908
more complex	0.084211
complex setting	0.041667
, either	0.001123
either	0.000279
either as	0.300000
an embedded	0.007576
embedded	0.000112
embedded system	0.250000
system or	0.021505
or serving	0.004505
serving	0.000028
serving a	1.000000
a precise	0.001227
precise function	0.333333
function for	0.125000
human user	0.043478
user	0.000391
user .	0.214286
The extrinsic	0.005208
extrinsic performance	0.166667
performance of	0.111111
system is	0.096774
is then	0.010163
then characterized	0.028571
characterized	0.000112
characterized in	0.250000
in terms	0.011236
terms	0.000363
terms of	0.538462
its utility	0.028571
utility	0.000056
utility with	0.500000
the overall	0.002076
overall	0.000167
overall task	0.166667
the complex	0.000692
complex system	0.083333
or the	0.040541
the human	0.002076
, consider	0.000561
consider	0.000112
consider a	0.250000
a syntactic	0.001227
syntactic parser	0.076923
parser	0.000446
parser that	0.062500
is based	0.008130
the output	0.002076
output of	0.153846
some new	0.012048
new part	0.041667
speech -LRB-	0.026316
-LRB- POS	0.005420
POS	0.000363
POS -RRB-	0.076923
-RRB- tagger	0.002710
tagger .	0.111111
An intrinsic	0.125000
intrinsic	0.000112
intrinsic evaluation	0.500000
evaluation would	0.055556
would run	0.037736
run	0.000140
run the	0.400000
the POS	0.002076
POS tagger	0.307692
tagger on	0.111111
some labeled	0.012048
labeled	0.000084
labeled data	0.333333
and compare	0.002890
compare the	0.285714
system output	0.010753
tagger to	0.111111
the gold	0.002076
standard -LRB-	0.142857
-LRB- correct	0.002710
correct -RRB-	0.066667
-RRB- output	0.002710
An extrinsic	0.062500
the parser	0.002768
parser with	0.062500
with some	0.021858
other POS	0.014286
and then	0.010116
then with	0.028571
the new	0.000692
new POS	0.041667
the parsing	0.001384
parsing	0.000781
parsing accuracy	0.035714
accuracy .	0.225806
<s> Black-box	0.000769
Black-box	0.000056
Black-box vs.	0.500000
vs. glass-box	0.083333
glass-box	0.000056
glass-box evaluation	1.000000
evaluation Black-box	0.018519
Black-box evaluation	0.500000
evaluation requires	0.018519
requires one	0.062500
one to	0.030769
to run	0.001328
run an	0.200000
system on	0.010753
given data	0.041667
data set	0.012987
set and	0.025641
measure a	0.090909
of parameters	0.000891
parameters	0.000112
parameters related	0.250000
related to	0.266667
the quality	0.003460
quality	0.000279
quality of	0.500000
process -LRB-	0.027778
-LRB- speed	0.002710
speed	0.000195
speed ,	0.285714
, reliability	0.000561
reliability	0.000056
reliability ,	0.500000
, resource	0.000561
resource consumption	0.200000
consumption	0.000028
consumption -RRB-	1.000000
and ,	0.004335
most importantly	0.017241
importantly	0.000028
importantly ,	1.000000
, to	0.007299
the result	0.002768
result -LRB-	0.090909
e.g. the	0.053571
the accuracy	0.000692
accuracy of	0.129032
data annotation	0.012987
annotation or	0.250000
the fidelity	0.000692
fidelity	0.000028
fidelity of	1.000000
a translation	0.002454
translation -RRB-	0.027027
<s> Glass-box	0.000769
Glass-box	0.000028
Glass-box evaluation	1.000000
evaluation looks	0.018519
looks	0.000112
looks at	0.250000
at the	0.220588
design of	0.250000
the algorithms	0.001384
algorithms that	0.057143
are implemented	0.004149
implemented ,	0.200000
the linguistic	0.000692
linguistic	0.000446
linguistic resources	0.062500
resources	0.000167
resources it	0.166667
it uses	0.017094
uses -LRB-	0.071429
e.g. vocabulary	0.017857
vocabulary size	0.125000
size	0.000167
size -RRB-	0.166667
<s> Given	0.002306
Given the	0.071429
NLP problems	0.042553
often difficult	0.022727
difficult to	0.392857
to predict	0.002656
predict	0.000167
predict performance	0.166667
performance only	0.055556
only on	0.052632
of glass-box	0.000891
but this	0.058824
this type	0.032967
is more	0.006098
more informative	0.010526
informative	0.000056
informative with	0.500000
to error	0.001328
error	0.000335
error analysis	0.083333
analysis or	0.046154
or future	0.004505
future	0.000084
future developments	0.333333
developments	0.000084
developments of	0.333333
Automatic vs.	0.111111
vs. manual	0.083333
manual	0.000056
manual evaluation	0.500000
evaluation In	0.018519
In many	0.019048
many cases	0.038462
, automatic	0.001684
automatic procedures	0.043478
be defined	0.004219
defined to	0.166667
to evaluate	0.005312
evaluate	0.000112
evaluate an	0.250000
system by	0.010753
by comparing	0.005714
comparing its	0.500000
its output	0.085714
output with	0.038462
or desired	0.004505
desired -RRB-	0.200000
-RRB- one	0.002710
one .	0.030769
<s> Although	0.005380
Although	0.000223
Although the	0.375000
the cost	0.000692
cost	0.000056
cost of	0.500000
of producing	0.000891
producing the	0.333333
standard can	0.071429
be quite	0.004219
quite	0.000223
quite high	0.125000
high ,	0.055556
automatic evaluation	0.130435
evaluation can	0.037037
be repeated	0.004219
repeated	0.000056
repeated as	0.500000
as often	0.003484
often as	0.045455
as needed	0.003484
needed	0.000586
needed without	0.047619
without much	0.076923
much additional	0.045455
additional	0.000167
additional costs	0.166667
costs	0.000028
costs -LRB-	1.000000
-LRB- on	0.005420
same input	0.080000
for many	0.007220
many NLP	0.019231
standard is	0.071429
a complex	0.006135
complex task	0.041667
and can	0.011561
can prove	0.005525
prove	0.000028
prove impossible	1.000000
impossible	0.000056
impossible when	0.500000
when inter-annotator	0.028571
inter-annotator	0.000028
inter-annotator agreement	1.000000
agreement	0.000084
agreement is	0.333333
is insufficient	0.002033
<s> Manual	0.001537
Manual	0.000084
Manual evaluation	0.666667
is performed	0.004065
performed by	0.200000
by human	0.017143
human judges	0.043478
judges	0.000056
judges ,	0.500000
are instructed	0.004149
instructed	0.000028
instructed to	1.000000
to estimate	0.003984
estimate	0.000112
estimate the	0.500000
or most	0.004505
most often	0.017241
often of	0.022727
a sample	0.001227
sample	0.000084
sample of	0.333333
output ,	0.038462
, based	0.001123
of criteria	0.000891
criteria .	0.250000
Although ,	0.125000
, thanks	0.000561
thanks	0.000028
thanks to	1.000000
to their	0.002656
their linguistic	0.029412
linguistic competence	0.062500
competence	0.000028
competence ,	1.000000
, human	0.003369
judges can	0.500000
be considered	0.008439
considered as	0.111111
the reference	0.000692
reference	0.000223
reference for	0.125000
processing tasks	0.018519
tasks ,	0.125000
also considerable	0.014493
considerable variation	0.200000
variation	0.000028
variation across	1.000000
across	0.000140
across their	0.200000
their ratings	0.029412
ratings	0.000251
ratings .	0.111111
is why	0.002033
why	0.000195
why automatic	0.142857
as objective	0.003484
objective evaluation	0.200000
while the	0.050000
human kind	0.021739
kind	0.000307
kind appears	0.090909
appears	0.000140
appears to	0.200000
be more	0.021097
more subjective	0.010526
subjective .	0.333333
<s> Shared	0.000769
Shared	0.000028
Shared tasks	1.000000
tasks -LRB-	0.031250
-LRB- Campaigns	0.002710
Campaigns	0.000028
Campaigns -RRB-	1.000000
-RRB- BioCreative	0.002710
BioCreative	0.000028
BioCreative Message	1.000000
Message	0.000028
Message Understanding	1.000000
Understanding	0.000056
Understanding Conference	0.500000
Conference	0.000056
Conference Technolangue\/Easy	0.500000
Technolangue\/Easy Text	0.500000
Text Retrieval	0.166667
Retrieval	0.000028
Retrieval Conference	1.000000
Conference Evaluation	0.500000
Evaluation exercises	0.111111
exercises	0.000028
exercises on	1.000000
on Semantic	0.004717
Semantic	0.000084
Semantic Evaluation	0.333333
Evaluation -LRB-	0.111111
-LRB- SemEval	0.002710
SemEval	0.000028
SemEval -RRB-	1.000000
-RRB- MorphoChallenge	0.002710
MorphoChallenge	0.000028
MorphoChallenge Semi-supervised	1.000000
Semi-supervised	0.000028
Semi-supervised and	1.000000
and Unsupervised	0.001445
Unsupervised	0.000167
Unsupervised Morpheme	0.166667
Morpheme	0.000028
Morpheme Analysis	1.000000
Analysis	0.000140
Analysis Standardization	0.200000
Standardization	0.000028
Standardization in	1.000000
NLP An	0.021277
An ISO	0.062500
ISO	0.000056
ISO sub-committee	0.500000
sub-committee	0.000056
sub-committee is	1.000000
is working	0.004065
to ease	0.001328
ease	0.000028
ease interoperability	1.000000
interoperability	0.000028
interoperability between	1.000000
between lexical	0.051282
lexical	0.000363
lexical resources	0.076923
resources and	0.166667
NLP programs	0.021277
programs .	0.272727
The sub-committee	0.005208
is part	0.002033
of ISO\/TC37	0.000891
ISO\/TC37	0.000028
ISO\/TC37 and	1.000000
is called	0.012195
called ISO\/TC37\/SC4	0.055556
ISO\/TC37\/SC4	0.000028
ISO\/TC37\/SC4 .	1.000000
Some ISO	0.047619
ISO standards	0.500000
standards	0.000140
standards are	0.200000
are already	0.004149
already published	0.200000
published but	0.142857
but most	0.029412
of them	0.001783
them are	0.105263
are under	0.004149
under	0.000140
under construction	0.200000
construction ,	0.333333
, mainly	0.000561
mainly on	0.166667
on lexicon	0.004717
lexicon	0.000251
lexicon representation	0.111111
representation -LRB-	0.105263
see LMF	0.050000
LMF	0.000028
LMF -RRB-	1.000000
, annotation	0.000561
annotation and	0.250000
data category	0.012987
category	0.000056
category registry	0.500000
registry	0.000028
registry .	1.000000
summarization is	0.120000
the creation	0.000692
a shortened	0.001227
shortened	0.000028
shortened version	1.000000
version	0.000084
version of	0.666667
a text	0.017178
text by	0.006289
by a	0.102857
program .	0.136364
The product	0.010417
product	0.000195
product of	0.142857
this procedure	0.010989
procedure still	0.333333
still	0.000419
still contains	0.066667
contains the	0.200000
most important	0.034483
important points	0.062500
points	0.000056
points of	0.500000
the original	0.006920
original	0.000363
original text	0.461538
analysis -LRB-	0.061538
-LRB- DA	0.005420
DA	0.000084
DA -RRB-	0.666667
or discourse	0.009009
discourse studies	0.027778
studies	0.000112
studies ,	0.500000
, is	0.007299
a general	0.003681
general term	0.045455
term	0.000502
term for	0.166667
of approaches	0.000891
to analyzing	0.001328
analyzing	0.000140
analyzing written	0.200000
written ,	0.038462
, spoken	0.000561
spoken ,	0.142857
, signed	0.000561
signed	0.000028
signed language	1.000000
language use	0.027027
use or	0.027778
or any	0.013514
any significant	0.032258
significant semiotic	0.111111
semiotic	0.000028
semiotic event	1.000000
event	0.000084
event .	0.333333
, sometimes	0.000561
to by	0.002656
the abbreviation	0.000692
abbreviation	0.000056
abbreviation MT	0.500000
MT	0.000140
MT -LRB-	0.200000
-LRB- not	0.002710
not to	0.008929
be confused	0.004219
confused	0.000056
confused with	1.000000
with computer-aided	0.005464
computer-aided	0.000084
computer-aided translation	0.333333
, machine-aided	0.000561
machine-aided	0.000028
machine-aided human	1.000000
human translation	0.043478
translation MAHT	0.013514
MAHT	0.000028
MAHT and	1.000000
and interactive	0.001445
interactive	0.000112
interactive translation	0.250000
a sub-field	0.001227
sub-field	0.000028
sub-field of	1.000000
that investigates	0.003546
investigates	0.000028
investigates the	1.000000
of software	0.000891
software	0.000753
software to	0.074074
to translate	0.003984
text or	0.018868
or speech	0.004505
speech from	0.006579
one natural	0.030769
On a	0.166667
a basic	0.002454
basic	0.000363
basic level	0.076923
level	0.000558
level ,	0.200000
, MT	0.001123
MT performs	0.200000
performs	0.000028
performs simple	1.000000
simple substitution	0.038462
substitution	0.000056
substitution of	1.000000
in one	0.007491
language for	0.020270
for words	0.003610
in another	0.007491
another ,	0.076923
but that	0.044118
that alone	0.003546
alone usually	0.250000
usually can	0.031250
not produce	0.008929
produce a	0.136364
a good	0.004908
good	0.000363
good translation	0.076923
because recognition	0.033333
recognition of	0.074380
of whole	0.001783
whole phrases	0.111111
phrases	0.000446
phrases and	0.187500
and their	0.008671
their closest	0.029412
closest	0.000056
closest counterparts	0.500000
counterparts	0.000028
counterparts in	1.000000
the target	0.006228
target	0.000307
target language	0.727273
language is	0.033784
is needed	0.002033
needed .	0.095238
<s> Solving	0.000769
Solving	0.000056
Solving this	0.500000
problem with	0.022727
with corpus	0.005464
corpus and	0.032258
statistical techniques	0.060606
techniques	0.000642
techniques is	0.043478
a rapidly	0.001227
rapidly	0.000056
rapidly growing	0.500000
growing	0.000112
growing field	0.500000
field that	0.074074
is leading	0.002033
leading	0.000056
leading to	1.000000
to better	0.003984
better	0.000251
better translations	0.111111
translations	0.000056
translations ,	0.500000
handling differences	0.500000
differences	0.000084
differences in	0.333333
in linguistic	0.003745
linguistic typology	0.062500
typology	0.000028
typology ,	1.000000
of idioms	0.000891
idioms	0.000056
idioms ,	1.000000
the isolation	0.000692
isolation	0.000056
isolation of	0.500000
of anomalies	0.000891
anomalies	0.000028
anomalies .	1.000000
-LRB- citation	0.035230
citation	0.000363
citation needed	1.000000
needed -RRB-	0.619048
-RRB- Current	0.002710
Current	0.000140
Current machine	0.200000
translation software	0.040541
software often	0.037037
often allows	0.022727
allows	0.000223
allows for	0.125000
for customisation	0.003610
customisation	0.000028
customisation by	1.000000
by domain	0.005714
domain	0.000558
domain or	0.050000
or profession	0.004505
profession	0.000028
profession -LRB-	1.000000
as weather	0.003484
weather	0.000195
weather reports	0.285714
reports	0.000140
reports -RRB-	0.400000
, improving	0.000561
improving	0.000028
improving output	1.000000
output by	0.038462
by limiting	0.005714
limiting	0.000028
limiting the	1.000000
the scope	0.001384
scope	0.000056
scope of	1.000000
of allowable	0.000891
allowable	0.000056
allowable substitutions	0.500000
substitutions	0.000028
substitutions .	1.000000
This technique	0.015873
technique	0.000195
technique is	0.142857
is particularly	0.004065
particularly effective	0.200000
effective	0.000167
effective in	0.333333
in domains	0.001873
domains	0.000223
domains where	0.125000
where formal	0.028571
formal or	0.111111
or formulaic	0.004505
formulaic	0.000028
formulaic language	1.000000
used .	0.044248
It follows	0.026316
follows	0.000056
follows that	0.500000
that machine	0.010638
government and	0.333333
and legal	0.001445
legal	0.000084
legal documents	0.333333
documents more	0.026316
more readily	0.010526
readily produces	0.333333
produces usable	0.250000
usable	0.000028
usable output	1.000000
output than	0.038462
than conversation	0.022222
conversation or	0.250000
or less	0.018018
less standardised	0.083333
standardised	0.000028
standardised text	1.000000
<s> Improved	0.000769
Improved	0.000028
Improved output	1.000000
output quality	0.038462
quality can	0.100000
can also	0.044199
be achieved	0.021097
achieved	0.000279
achieved by	0.200000
human intervention	0.021739
intervention	0.000028
intervention :	1.000000
: for	0.009804
some systems	0.024096
systems are	0.116071
translate more	0.166667
more accurately	0.010526
accurately	0.000056
accurately if	0.500000
if	0.000781
if the	0.357143
the user	0.003460
user has	0.071429
has unambiguously	0.011905
unambiguously	0.000028
unambiguously identified	1.000000
identified which	0.200000
text are	0.006289
are names	0.004149
<s> With	0.003843
With	0.000195
With the	0.285714
the assistance	0.000692
assistance	0.000028
assistance of	1.000000
these techniques	0.023810
techniques ,	0.086957
MT has	0.200000
has proven	0.011905
proven	0.000028
proven useful	1.000000
useful as	0.071429
a tool	0.002454
tool	0.000056
tool to	0.500000
to assist	0.001328
assist	0.000028
assist human	1.000000
human translators	0.021739
translators	0.000028
translators and	1.000000
very limited	0.048780
limited number	0.200000
of cases	0.000891
can even	0.005525
even produce	0.037037
produce output	0.090909
output that	0.076923
be used	0.080169
used as	0.044248
e.g. ,	0.464286
, weather	0.000561
The progress	0.005208
progress and	0.142857
and potential	0.001445
potential of	0.285714
translation has	0.027027
been debated	0.014706
debated	0.000028
debated much	1.000000
much through	0.045455
through its	0.125000
its history	0.028571
history .	0.250000
<s> Since	0.003075
Since	0.000140
Since the	0.200000
the 1950s	0.002076
1950s	0.000112
1950s ,	0.500000
of scholars	0.000891
scholars	0.000056
scholars have	0.500000
have questioned	0.009615
questioned	0.000028
questioned the	1.000000
the possibility	0.002768
possibility	0.000112
possibility of	0.750000
of achieving	0.001783
achieving	0.000056
achieving fully	0.500000
automatic machine	0.043478
of high	0.000891
high quality	0.055556
quality .	0.100000
Some critics	0.047619
critics	0.000028
critics claim	1.000000
claim	0.000028
claim that	1.000000
that there	0.007092
are in-principle	0.004149
in-principle	0.000028
in-principle obstacles	1.000000
obstacles	0.000028
obstacles to	1.000000
to automatizing	0.001328
automatizing	0.000028
automatizing the	1.000000
translation process	0.027027
In 1629	0.009524
1629	0.000028
1629 ,	1.000000
, René	0.000561
René	0.000028
René Descartes	1.000000
Descartes	0.000028
Descartes proposed	1.000000
proposed a	0.222222
a universal	0.001227
universal	0.000084
universal language	0.333333
language ,	0.040541
, with	0.004492
with equivalent	0.010929
equivalent	0.000140
equivalent ideas	0.200000
ideas	0.000112
ideas in	0.500000
in different	0.005618
different tongues	0.020408
tongues	0.000028
tongues sharing	1.000000
sharing	0.000028
sharing one	1.000000
one symbol	0.015385
symbol	0.000112
symbol .	0.500000
, The	0.000561
experiment -LRB-	0.200000
-LRB- 1954	0.002710
1954 -RRB-	0.333333
-RRB- involved	0.002710
of over	0.000891
over sixty	0.083333
The experiment	0.005208
experiment was	0.400000
great success	0.333333
success and	0.200000
and ushered	0.001445
ushered	0.000028
ushered in	1.000000
in an	0.014981
an era	0.007576
era	0.000028
era of	1.000000
of substantial	0.000891
substantial	0.000140
substantial funding	0.200000
for machine-translation	0.003610
machine-translation	0.000056
machine-translation research	0.500000
research .	0.095238
three to	0.333333
to five	0.001328
<s> Real	0.001537
Real	0.000056
Real progress	0.500000
report -LRB-	0.250000
-LRB- 1966	0.002710
1966 -RRB-	0.333333
the ten-year-long	0.000692
ten-year-long	0.000028
ten-year-long research	1.000000
fulfill expectations	0.500000
funding was	0.125000
was greatly	0.012987
greatly reduced	0.142857
<s> Beginning	0.000769
Beginning	0.000056
Beginning in	0.500000
as computational	0.003484
power increased	0.250000
increased	0.000140
increased and	0.200000
and became	0.001445
became	0.000140
became less	0.200000
less expensive	0.083333
expensive	0.000195
expensive ,	0.428571
, more	0.002246
more interest	0.010526
interest	0.000307
interest was	0.090909
was shown	0.025974
shown	0.000140
shown in	0.400000
models for	0.230769
translation .	0.054054
The idea	0.010417
idea	0.000195
idea of	0.285714
of using	0.000891
using digital	0.016949
digital	0.000195
digital computers	0.142857
computers for	0.111111
for translation	0.003610
languages was	0.020000
was proposed	0.025974
proposed as	0.111111
as early	0.003484
early as	0.100000
as 1946	0.003484
1946	0.000028
1946 by	1.000000
by A.	0.005714
A.	0.000140
A. D.	0.200000
D. Booth	0.200000
Booth	0.000028
Booth and	1.000000
and possibly	0.001445
possibly	0.000056
possibly others	0.500000
<s> Warren	0.000769
Warren	0.000028
Warren Weaver	1.000000
Weaver	0.000028
Weaver wrote	1.000000
wrote	0.000167
wrote an	0.333333
an important	0.015152
important memorandum	0.062500
memorandum	0.000028
memorandum ``	1.000000
`` Translation	0.005291
Translation	0.000084
Translation ''	0.333333
'' in	0.036082
in 1949	0.001873
1949	0.000056
1949 .	0.500000
was by	0.025974
by no	0.005714
no means	0.076923
means	0.000167
means the	0.166667
first such	0.030303
such application	0.008130
application ,	0.142857
a demonstration	0.003681
demonstration	0.000140
demonstration was	0.200000
was made	0.012987
1954 on	0.333333
the APEXC	0.000692
APEXC	0.000028
APEXC machine	1.000000
machine at	0.012658
at Birkbeck	0.029412
Birkbeck	0.000056
Birkbeck College	1.000000
College	0.000056
College -LRB-	0.500000
-LRB- University	0.002710
University	0.000251
University of	0.333333
of London	0.000891
London	0.000028
London -RRB-	1.000000
a rudimentary	0.001227
rudimentary	0.000056
rudimentary translation	0.500000
English into	0.027027
into French	0.025641
French .	0.250000
<s> Several	0.002306
Several	0.000084
Several papers	0.333333
papers	0.000084
papers on	0.666667
topic were	0.125000
were published	0.024390
published at	0.142857
the time	0.004152
even articles	0.037037
in popular	0.001873
popular	0.000251
popular journals	0.111111
journals	0.000056
journals -LRB-	0.500000
see for	0.050000
example Wireless	0.012346
Wireless	0.000028
Wireless World	1.000000
World ,	0.142857
, Sept.	0.000561
Sept.	0.000028
Sept. 1955	1.000000
1955	0.000056
1955 ,	1.000000
, Cleave	0.000561
Cleave	0.000028
Cleave and	1.000000
and Zacharov	0.001445
Zacharov	0.000028
Zacharov -RRB-	1.000000
A similar	0.020000
similar application	0.037037
also pioneered	0.014493
pioneered	0.000084
pioneered at	0.333333
College at	0.500000
, was	0.002246
was reading	0.012987
reading	0.000223
reading and	0.250000
and composing	0.001445
composing	0.000028
composing Braille	1.000000
Braille	0.000028
Braille texts	1.000000
texts by	0.058824
by computer	0.017143
computer .	0.068182
<s> Translation	0.000769
Translation process	0.666667
process Main	0.027778
: Translation	0.009804
process The	0.027778
The human	0.005208
process may	0.027778
be described	0.004219
described as	0.333333
as :	0.003484
: Decoding	0.009804
Decoding	0.000056
Decoding the	0.500000
the source	0.008304
source	0.000670
source text	0.208333
text ;	0.006289
and Re-encoding	0.001445
Re-encoding	0.000028
Re-encoding this	1.000000
this meaning	0.010989
meaning in	0.086957
<s> Behind	0.000769
Behind	0.000028
Behind this	1.000000
this ostensibly	0.010989
ostensibly	0.000028
ostensibly simple	1.000000
simple procedure	0.038462
procedure lies	0.333333
lies	0.000056
lies a	0.500000
complex cognitive	0.041667
cognitive	0.000056
cognitive operation	0.500000
operation	0.000056
operation .	0.500000
<s> To	0.006149
To decode	0.111111
decode	0.000028
decode the	1.000000
text in	0.050314
in its	0.003745
its entirety	0.028571
entirety	0.000028
entirety ,	1.000000
the translator	0.000692
translator	0.000195
translator must	0.142857
must	0.000391
must interpret	0.071429
interpret	0.000028
interpret and	1.000000
and analyze	0.001445
analyze	0.000112
analyze all	0.250000
all the	0.139535
features of	0.153846
process that	0.055556
that requires	0.007092
requires in-depth	0.062500
in-depth	0.000084
in-depth knowledge	0.666667
the grammar	0.006228
, syntax	0.001123
syntax	0.000307
syntax ,	0.454545
, idioms	0.000561
etc. ,	0.045455
, of	0.002246
source language	0.125000
the culture	0.000692
culture	0.000028
culture of	1.000000
its speakers	0.028571
speakers	0.000112
speakers .	0.250000
The translator	0.005208
translator needs	0.142857
needs the	0.100000
same in-depth	0.040000
knowledge to	0.037037
to re-encode	0.001328
re-encode	0.000028
re-encode the	1.000000
<s> Therein	0.000769
Therein	0.000028
Therein lies	1.000000
lies the	0.500000
the challenge	0.000692
challenge	0.000028
challenge in	1.000000
: how	0.009804
how	0.000809
how to	0.103448
to program	0.001328
program a	0.090909
computer that	0.022727
that will	0.007092
will ``	0.057143
`` understand	0.005291
understand	0.000195
understand ''	0.142857
'' a	0.015464
text as	0.006289
person does	0.052632
does	0.000279
does ,	0.100000
and that	0.002890
`` create	0.005291
create	0.000474
create ''	0.058824
a new	0.007362
new text	0.083333
language that	0.006757
that ``	0.021277
`` sounds	0.005291
sounds ''	0.066667
'' as	0.025773
as if	0.003484
if it	0.071429
it has	0.034188
been written	0.014706
person .	0.052632
This problem	0.063492
problem may	0.022727
be approached	0.004219
approached	0.000056
approached in	0.500000
of ways	0.001783
ways	0.000223
ways .	0.125000
<s> Approaches	0.002306
Approaches	0.000084
Approaches Bernard	0.333333
Bernard	0.000028
Bernard Vauquois	1.000000
Vauquois	0.000028
Vauquois '	1.000000
' pyramid	0.052632
pyramid	0.000028
pyramid showing	1.000000
showing	0.000056
showing comparative	0.500000
comparative	0.000028
comparative depths	1.000000
depths	0.000028
depths of	1.000000
of intermediary	0.000891
intermediary	0.000084
intermediary representation	0.666667
representation ,	0.157895
, interlingual	0.001123
interlingual	0.000112
interlingual machine	0.750000
translation at	0.013514
the peak	0.000692
peak	0.000028
peak ,	1.000000
, followed	0.000561
followed	0.000112
followed by	0.500000
by transfer-based	0.005714
transfer-based	0.000084
transfer-based ,	0.333333
, then	0.006176
then direct	0.028571
direct translation	0.166667
translation can	0.027027
can use	0.016575
use a	0.055556
a method	0.004908
method based	0.125000
on linguistic	0.004717
linguistic rules	0.062500
which means	0.028986
means that	0.666667
that words	0.007092
words will	0.018349
be translated	0.012658
translated	0.000112
translated in	0.250000
a linguistic	0.002454
linguistic way	0.062500
way --	0.041667
most suitable	0.017241
suitable	0.000112
suitable -LRB-	0.250000
-LRB- orally	0.002710
orally	0.000028
orally speaking	1.000000
speaking -RRB-	0.125000
-RRB- words	0.002710
words of	0.027523
language will	0.006757
will replace	0.028571
replace	0.000028
replace the	1.000000
the ones	0.001384
ones in	0.100000
often argued	0.022727
argued	0.000028
argued that	1.000000
translation requires	0.013514
requires the	0.187500
the problem	0.006228
problem of	0.181818
understanding to	0.060606
be solved	0.004219
solved first	0.200000
first .	0.030303
, rule-based	0.000561
rule-based	0.000195
rule-based methods	0.142857
methods parse	0.022727
parse a	0.111111
, usually	0.002807
usually creating	0.031250
creating an	0.285714
an intermediary	0.007576
intermediary ,	0.333333
, symbolic	0.000561
symbolic	0.000028
symbolic representation	1.000000
, from	0.000561
from which	0.028846
<s> According	0.000769
According	0.000028
According to	1.000000
the intermediary	0.000692
is described	0.002033
as interlingual	0.003484
translation or	0.013514
or transfer-based	0.004505
transfer-based machine	0.666667
These methods	0.176471
methods require	0.022727
extensive lexicons	0.333333
lexicons	0.000056
lexicons with	0.500000
with morphological	0.005464
morphological	0.000084
morphological ,	0.333333
, syntactic	0.001684
syntactic ,	0.076923
and semantic	0.004335
information ,	0.043478
and large	0.001445
Given enough	0.071429
enough	0.000140
enough data	0.400000
translation programs	0.013514
programs often	0.090909
often work	0.022727
work well	0.041667
well enough	0.035714
enough for	0.200000
a native	0.002454
native	0.000112
native speaker	1.000000
speaker	0.000502
speaker of	0.111111
of one	0.003565
one language	0.030769
to get	0.005312
get	0.000195
get the	0.285714
the approximate	0.000692
approximate	0.000056
approximate meaning	0.500000
of what	0.003565
is written	0.002033
other native	0.014286
speaker .	0.166667
difficulty is	0.142857
is getting	0.004065
getting	0.000112
getting enough	0.250000
data of	0.012987
the right	0.002076
right kind	0.100000
kind to	0.090909
to support	0.002656
support	0.000112
support the	0.250000
the particular	0.001384
particular method	0.076923
method .	0.125000
the large	0.000692
large multilingual	0.043478
multilingual corpus	0.333333
data needed	0.012987
needed for	0.095238
work is	0.125000
not necessary	0.008929
necessary for	0.300000
the grammar-based	0.000692
grammar-based	0.000028
grammar-based methods	1.000000
methods .	0.022727
<s> But	0.004612
But	0.000167
But then	0.166667
then ,	0.085714
grammar methods	0.027027
methods need	0.045455
need	0.000586
need a	0.190476
a skilled	0.001227
skilled	0.000028
skilled linguist	1.000000
linguist	0.000056
linguist to	0.500000
to carefully	0.001328
carefully	0.000028
carefully design	1.000000
design the	0.250000
grammar that	0.027027
they use	0.025000
use .	0.041667
To translate	0.111111
translate between	0.166667
between closely	0.025641
closely	0.000140
closely related	0.400000
related languages	0.066667
a technique	0.001227
technique referred	0.142857
as shallow-transfer	0.003484
shallow-transfer	0.000028
shallow-transfer machine	1.000000
translation may	0.013514
<s> Rule-based	0.000769
Rule-based	0.000056
Rule-based The	0.500000
The rule-based	0.005208
rule-based machine	0.142857
translation paradigm	0.013514
paradigm includes	0.333333
includes transfer-based	0.142857
and dictionary-based	0.001445
dictionary-based	0.000028
dictionary-based machine	1.000000
translation paradigms	0.013514
paradigms	0.000028
paradigms .	1.000000
<s> Main	0.000769
: Rule-based	0.009804
Rule-based machine	0.500000
translation Transfer-based	0.013514
Transfer-based	0.000056
Transfer-based machine	1.000000
translation Main	0.013514
: Transfer-based	0.009804
translation Interlingual	0.027027
Interlingual	0.000084
Interlingual Main	0.333333
: Interlingual	0.009804
Interlingual machine	0.666667
translation is	0.013514
one instance	0.015385
instance	0.000391
instance of	0.142857
of rule-based	0.000891
rule-based machine-translation	0.142857
machine-translation approaches	0.500000
approaches .	0.107143
In this	0.047619
this approach	0.021978
approach ,	0.057143
translated ,	0.250000
is transformed	0.002033
transformed	0.000028
transformed into	1.000000
into an	0.025641
an interlingual	0.007576
interlingual ,	0.250000
i.e. source	0.052632
source -	0.041667
- \/	0.062500
\/	0.000084
\/ target-language-independent	0.333333
target-language-independent	0.000028
target-language-independent representation	1.000000
representation .	0.210526
The target	0.005208
then generated	0.028571
generated out	0.066667
out of	0.071429
the interlingua	0.000692
interlingua	0.000028
interlingua .	1.000000
<s> Dictionary-based	0.000769
Dictionary-based	0.000056
Dictionary-based Main	0.500000
: Dictionary-based	0.009804
Dictionary-based machine	0.500000
translation Machine	0.013514
on dictionary	0.004717
dictionary entries	0.142857
entries	0.000056
entries ,	0.500000
translated as	0.250000
as they	0.010453
they are	0.175000
are by	0.004149
dictionary .	0.142857
Statistical Main	0.111111
: Statistical	0.009804
Statistical machine	0.222222
translation Statistical	0.013514
translation tries	0.013514
tries	0.000028
tries to	1.000000
to generate	0.007968
generate	0.000502
generate translations	0.055556
translations using	0.500000
using statistical	0.033898
methods based	0.022727
on bilingual	0.004717
bilingual	0.000056
bilingual text	0.500000
text corpora	0.006289
corpora ,	0.090909
the Canadian	0.001384
Canadian	0.000056
Canadian Hansard	0.500000
Hansard	0.000028
Hansard corpus	1.000000
corpus ,	0.064516
the English-French	0.000692
English-French	0.000028
English-French record	1.000000
record	0.000056
record of	1.000000
Canadian parliament	0.500000
parliament	0.000028
parliament and	1.000000
and EUROPARL	0.001445
EUROPARL	0.000028
EUROPARL ,	1.000000
the record	0.000692
European Parliament	0.333333
Parliament .	0.500000
<s> Where	0.000769
Where	0.000028
Where such	1.000000
such corpora	0.016260
corpora are	0.181818
are available	0.008299
available ,	0.235294
, impressive	0.000561
impressive	0.000056
impressive results	0.500000
results can	0.047619
achieved translating	0.100000
translating	0.000112
translating texts	0.250000
texts of	0.117647
a similar	0.001227
similar kind	0.037037
kind ,	0.090909
but such	0.014706
are still	0.016598
still very	0.066667
very rare	0.024390
rare	0.000112
rare .	0.500000
software was	0.037037
was CANDIDE	0.012987
CANDIDE	0.000028
CANDIDE from	1.000000
from IBM	0.009615
IBM .	0.333333
<s> Google	0.000769
Google	0.000112
Google used	0.250000
used SYSTRAN	0.008850
SYSTRAN	0.000028
SYSTRAN for	1.000000
for several	0.007220
several years	0.090909
but switched	0.014706
switched	0.000028
switched to	1.000000
a statistical	0.003681
statistical translation	0.030303
translation method	0.013514
method in	0.062500
in October	0.001873
October	0.000028
October 2007	1.000000
<s> Recently	0.000769
Recently	0.000028
Recently ,	1.000000
, they	0.004492
they improved	0.025000
improved	0.000112
improved their	0.250000
their translation	0.029412
translation capabilities	0.013514
capabilities	0.000140
capabilities by	0.200000
by inputting	0.005714
inputting	0.000028
inputting approximately	1.000000
approximately	0.000056
approximately 200	0.500000
200	0.000056
200 billion	0.500000
billion	0.000028
billion words	1.000000
words from	0.018349
from United	0.009615
United	0.000251
United Nations	0.222222
Nations	0.000056
Nations materials	0.500000
materials	0.000056
materials to	0.500000
to train	0.001328
train	0.000028
train their	1.000000
their system	0.029412
<s> Accuracy	0.003843
Accuracy	0.000195
Accuracy of	0.428571
has improved	0.011905
improved .	0.250000
<s> Example-based	0.000769
Example-based	0.000084
Example-based Main	0.333333
: Example-based	0.009804
Example-based machine	0.666667
translation Example-based	0.013514
translation -LRB-	0.027027
-LRB- EBMT	0.002710
EBMT	0.000028
EBMT -RRB-	1.000000
-RRB- approach	0.002710
approach was	0.028571
proposed by	0.111111
by Makoto	0.005714
Makoto	0.000028
Makoto Nagao	1.000000
Nagao	0.000028
Nagao in	1.000000
in 1984	0.001873
1984	0.000028
1984 .	1.000000
often characterised	0.022727
characterised	0.000028
characterised by	1.000000
by its	0.005714
its use	0.028571
a bilingual	0.001227
bilingual corpus	0.500000
corpus as	0.032258
as its	0.010453
its main	0.028571
main	0.000223
main knowledge	0.125000
, at	0.001684
at run-time	0.014706
run-time	0.000028
run-time .	1.000000
is essentially	0.006098
essentially	0.000223
essentially a	0.250000
translation by	0.013514
by analogy	0.005714
analogy	0.000028
analogy and	1.000000
be viewed	0.016878
viewed	0.000112
viewed as	1.000000
an implementation	0.007576
of case-based	0.000891
case-based	0.000028
case-based reasoning	1.000000
reasoning	0.000195
reasoning approach	0.142857
approach of	0.028571
<s> Hybrid	0.000769
Hybrid	0.000056
Hybrid MT	0.500000
MT Hybrid	0.200000
Hybrid machine	0.500000
-LRB- HMT	0.002710
HMT	0.000028
HMT -RRB-	1.000000
-RRB- leverages	0.002710
leverages	0.000028
leverages the	1.000000
the strengths	0.000692
strengths	0.000056
strengths of	0.500000
statistical and	0.030303
and rule-based	0.001445
rule-based translation	0.142857
translation methodologies	0.013514
methodologies	0.000056
methodologies .	1.000000
Several MT	0.333333
MT companies	0.200000
companies	0.000056
companies -LRB-	0.500000
-LRB- Asia	0.002710
Asia	0.000028
Asia Online	1.000000
Online	0.000056
Online ,	0.500000
, LinguaSys	0.000561
LinguaSys	0.000028
LinguaSys ,	1.000000
, Systran	0.000561
Systran	0.000028
Systran ,	1.000000
, PangeaMT	0.000561
PangeaMT	0.000028
PangeaMT ,	1.000000
, UPV	0.000561
UPV	0.000028
UPV -RRB-	1.000000
are claiming	0.004149
claiming	0.000028
claiming to	1.000000
to have	0.013280
a hybrid	0.002454
hybrid	0.000056
hybrid approach	0.500000
approach using	0.028571
using both	0.016949
both rules	0.032258
rules and	0.023256
The approaches	0.005208
approaches differ	0.035714
differ in	0.333333
ways :	0.250000
: Rules	0.019608
Rules	0.000084
Rules post-processed	0.333333
post-processed	0.000028
post-processed by	1.000000
by statistics	0.005714
statistics :	0.125000
: Translations	0.009804
Translations	0.000028
Translations are	1.000000
are performed	0.004149
performed using	0.100000
a rules	0.001227
rules based	0.023256
based engine	0.018519
engine	0.000167
engine .	0.666667
<s> Statistics	0.002306
Statistics	0.000084
Statistics are	0.333333
are then	0.004149
then used	0.028571
an attempt	0.022727
attempt	0.000167
attempt to	1.000000
to adjust\/correct	0.001328
adjust\/correct	0.000028
adjust\/correct the	1.000000
output from	0.038462
rules engine	0.023256
Statistics guided	0.333333
guided	0.000028
guided by	1.000000
by rules	0.005714
Rules are	0.666667
to pre-process	0.001328
pre-process	0.000028
pre-process data	1.000000
data in	0.025974
better guide	0.111111
guide	0.000056
guide the	1.000000
the statistical	0.001384
statistical engine	0.030303
<s> Rules	0.000769
also used	0.028986
to post-process	0.001328
post-process	0.000028
post-process the	1.000000
statistical output	0.030303
output to	0.038462
to perform	0.005312
perform functions	0.090909
functions	0.000056
functions such	0.500000
as normalization	0.003484
normalization	0.000167
normalization .	0.333333
This approach	0.031746
approach has	0.028571
has a	0.047619
a lot	0.003681
lot	0.000084
lot more	0.333333
more power	0.010526
power ,	0.250000
, flexibility	0.000561
flexibility	0.000028
flexibility and	1.000000
and control	0.004335
control	0.000140
control when	0.200000
when translating	0.028571
translating .	0.250000
Major issues	0.500000
issues	0.000140
issues Disambiguation	0.200000
Disambiguation	0.000028
Disambiguation Main	1.000000
: Word	0.009804
disambiguation Word-sense	0.100000
Word-sense	0.000028
Word-sense disambiguation	1.000000
disambiguation concerns	0.100000
concerns	0.000056
concerns finding	0.500000
finding	0.000140
finding a	0.400000
a suitable	0.003681
suitable translation	0.250000
translation when	0.013514
when a	0.114286
word can	0.033333
can have	0.011050
The problem	0.015625
problem was	0.022727
was first	0.012987
first raised	0.030303
raised	0.000028
raised in	1.000000
1950s by	0.250000
by Yehoshua	0.005714
Yehoshua	0.000028
Yehoshua Bar-Hillel	1.000000
Bar-Hillel	0.000028
Bar-Hillel .	1.000000
<s> He	0.005380
He pointed	0.125000
pointed	0.000028
pointed out	1.000000
out that	0.071429
that without	0.003546
without a	0.076923
a ``	0.007362
`` universal	0.010582
universal encyclopedia	0.333333
encyclopedia	0.000028
encyclopedia ''	1.000000
a machine	0.008589
machine would	0.025316
would never	0.018868
never be	0.400000
be able	0.021097
distinguish between	0.400000
the two	0.003460
two meanings	0.034483
meanings	0.000112
meanings of	0.250000
<s> Today	0.000769
Today	0.000028
Today there	1.000000
are numerous	0.004149
numerous	0.000028
numerous approaches	1.000000
approaches designed	0.035714
designed	0.000195
designed to	0.714286
to overcome	0.001328
overcome	0.000056
overcome this	0.500000
<s> They	0.002306
They	0.000084
They can	0.333333
be approximately	0.004219
approximately divided	0.500000
divided	0.000084
divided into	0.666667
into ``	0.012821
`` shallow	0.005291
shallow	0.000167
shallow ''	0.166667
'' approaches	0.010309
approaches and	0.035714
`` deep	0.005291
deep	0.000195
deep ''	0.142857
<s> Shallow	0.001537
Shallow	0.000056
Shallow approaches	0.500000
approaches assume	0.035714
assume	0.000056
assume no	0.500000
no knowledge	0.076923
They simply	0.333333
simply apply	0.083333
apply	0.000140
apply statistical	0.200000
words surrounding	0.009174
surrounding the	0.200000
the ambiguous	0.001384
ambiguous word	0.083333
<s> Deep	0.000769
Deep	0.000028
Deep approaches	1.000000
approaches presume	0.035714
presume	0.000028
presume a	1.000000
a comprehensive	0.003681
comprehensive	0.000140
comprehensive knowledge	0.200000
<s> So	0.002306
So	0.000084
So far	0.333333
far	0.000223
far ,	0.125000
, shallow	0.000561
shallow approaches	0.166667
approaches have	0.071429
been more	0.029412
more successful	0.031579
successful .	0.111111
-RRB- The	0.005420
The late	0.005208
late Claude	0.111111
Claude	0.000028
Claude Piron	1.000000
Piron	0.000084
Piron ,	0.333333
a long-time	0.001227
long-time	0.000028
long-time translator	1.000000
translator for	0.142857
the United	0.004844
Nations and	0.500000
World Health	0.142857
Health	0.000056
Health Organization	0.500000
Organization	0.000028
Organization ,	1.000000
, wrote	0.000561
wrote that	0.166667
at its	0.014706
its best	0.028571
best ,	0.055556
, automates	0.000561
automates	0.000028
automates the	1.000000
the easier	0.000692
easier part	0.125000
a translator	0.003681
translator 's	0.285714
's job	0.039216
job	0.000056
job ;	0.500000
the harder	0.002076
harder	0.000195
harder and	0.142857
more time-consuming	0.010526
time-consuming part	0.333333
part usually	0.037037
usually involves	0.031250
involves doing	0.100000
doing	0.000056
doing extensive	0.500000
extensive research	0.333333
research to	0.023810
resolve ambiguities	0.500000
ambiguities	0.000112
ambiguities in	0.250000
the grammatical	0.002076
grammatical and	0.090909
and lexical	0.001445
lexical exigencies	0.076923
exigencies	0.000028
exigencies of	1.000000
language require	0.006757
require to	0.045455
be resolved	0.004219
resolved	0.000028
resolved :	1.000000
: Why	0.009804
Why does	0.285714
does a	0.200000
translator need	0.142857
whole workday	0.111111
workday	0.000028
workday to	1.000000
translate five	0.166667
five pages	0.200000
pages	0.000195
pages ,	0.428571
and not	0.011561
not an	0.008929
an hour	0.007576
hour	0.000028
hour or	1.000000
or two	0.009009
two ?	0.034483
? </s>	0.500000
<s> ...	0.000769
...	0.000056
... About	0.500000
About	0.000056
About 90	0.500000
90	0.000112
90 %	1.000000
%	0.001088
% of	0.205128
an average	0.007576
average	0.000056
average text	0.500000
text corresponds	0.006289
corresponds	0.000028
corresponds to	1.000000
to these	0.002656
these simple	0.023810
simple conditions	0.038462
conditions	0.000140
conditions .	0.200000
But unfortunately	0.166667
unfortunately	0.000028
unfortunately ,	1.000000
there 's	0.025000
's the	0.019608
other 10	0.014286
10 %	0.250000
% .	0.230769
It 's	0.052632
's that	0.019608
that part	0.003546
part that	0.037037
requires six	0.062500
six	0.000056
six -LRB-	0.500000
-LRB- more	0.005420
more -RRB-	0.010526
-RRB- hours	0.002710
hours	0.000056
hours of	0.500000
of work	0.000891
work .	0.083333
<s> There	0.006918
There	0.000307
There are	0.545455
are ambiguities	0.004149
ambiguities one	0.250000
one has	0.015385
has to	0.059524
resolve .	0.250000
For instance	0.114754
instance ,	0.642857
the author	0.002076
author	0.000084
author of	0.333333
an Australian	0.007576
Australian	0.000056
Australian physician	0.500000
physician	0.000028
physician ,	1.000000
, cited	0.000561
cited	0.000028
cited the	1.000000
the example	0.002076
an epidemic	0.007576
epidemic	0.000028
epidemic which	1.000000
was declared	0.012987
declared	0.000056
declared during	0.500000
during World	0.100000
World War	0.142857
War	0.000028
War II	1.000000
II	0.000056
II in	0.500000
`` Japanese	0.005291
Japanese prisoner	0.125000
prisoner	0.000028
prisoner of	1.000000
of war	0.000891
war	0.000028
war camp	1.000000
camp	0.000112
camp ''	0.250000
<s> Was	0.000769
Was	0.000028
Was he	1.000000
he	0.000195
he talking	0.142857
talking	0.000028
talking about	1.000000
about an	0.025000
an American	0.007576
American	0.000140
American camp	0.200000
camp with	0.500000
with Japanese	0.005464
Japanese prisoners	0.125000
prisoners	0.000056
prisoners or	0.500000
a Japanese	0.001227
Japanese camp	0.125000
with American	0.005464
American prisoners	0.200000
prisoners ?	0.500000
The English	0.005208
has two	0.011905
two senses	0.034483
senses .	0.500000
's necessary	0.019608
necessary therefore	0.100000
therefore	0.000140
therefore to	0.200000
to do	0.003984
do research	0.038462
research ,	0.071429
, maybe	0.000561
maybe	0.000028
maybe to	1.000000
the extent	0.001384
extent of	0.250000
a phone	0.001227
phone	0.000112
phone call	0.250000
call	0.000084
call to	0.333333
to Australia	0.001328
Australia	0.000028
Australia .	1.000000
The ideal	0.005208
ideal	0.000028
ideal deep	1.000000
deep approach	0.142857
approach would	0.028571
would require	0.056604
require the	0.181818
do all	0.038462
the research	0.002076
research necessary	0.023810
for this	0.018051
this kind	0.010989
kind of	0.727273
of disambiguation	0.000891
disambiguation on	0.100000
on its	0.009434
its own	0.142857
own	0.000167
own ;	0.166667
; but	0.042553
this would	0.010989
require a	0.227273
a higher	0.002454
higher	0.000195
higher degree	0.142857
degree	0.000167
degree of	0.500000
of AI	0.000891
AI	0.000084
AI than	0.333333
than has	0.022222
has yet	0.011905
yet	0.000056
yet been	0.500000
been attained	0.014706
attained	0.000028
attained .	1.000000
A shallow	0.040000
shallow approach	0.333333
approach which	0.057143
which simply	0.007246
simply guessed	0.083333
guessed	0.000028
guessed at	1.000000
the sense	0.000692
sense of	0.125000
ambiguous English	0.083333
English phrase	0.027027
phrase	0.000279
phrase that	0.100000
that Piron	0.003546
Piron mentions	0.333333
mentions -LRB-	0.333333
-LRB- based	0.005420
based ,	0.037037
perhaps ,	0.166667
, on	0.003930
which kind	0.007246
of prisoner-of-war	0.000891
prisoner-of-war	0.000028
prisoner-of-war camp	1.000000
camp is	0.250000
more often	0.010526
often mentioned	0.022727
mentioned	0.000167
mentioned in	0.166667
given corpus	0.041667
corpus -RRB-	0.129032
-RRB- would	0.005420
would have	0.056604
a reasonable	0.002454
reasonable	0.000056
reasonable chance	0.500000
chance	0.000028
chance of	1.000000
of guessing	0.000891
guessing	0.000028
guessing wrong	1.000000
wrong	0.000028
wrong fairly	1.000000
fairly often	0.250000
often .	0.022727
approach that	0.057143
that involves	0.003546
involves ``	0.100000
`` ask	0.005291
ask	0.000112
ask the	0.500000
user about	0.071429
about each	0.025000
each ambiguity	0.022222
ambiguity ''	0.125000
'' would	0.010309
would ,	0.018868
, by	0.002807
by Piron	0.005714
Piron 's	0.333333
's estimate	0.019608
estimate ,	0.250000
only automate	0.026316
automate	0.000084
automate about	0.333333
about 25	0.025000
25	0.000028
25 %	1.000000
a professional	0.001227
professional	0.000028
professional translator	1.000000
job ,	0.500000
, leaving	0.000561
leaving	0.000028
leaving the	1.000000
harder 75	0.142857
75	0.000028
75 %	1.000000
% still	0.025641
still to	0.066667
be done	0.021097
done	0.000307
done by	0.181818
The objects	0.005208
objects of	0.200000
of discourse	0.009804
discourse analysis	0.222222
analysis --	0.015385
-- discourse	0.040000
discourse ,	0.083333
, writing	0.001123
writing ,	0.222222
, conversation	0.000561
conversation ,	0.250000
, communicative	0.000561
communicative	0.000084
communicative event	0.333333
event ,	0.666667
etc. --	0.045455
-- are	0.080000
are variously	0.004149
variously	0.000028
variously defined	1.000000
defined in	0.166667
of coherent	0.000891
coherent	0.000140
coherent sequences	0.200000
sequences	0.000251
sequences of	0.333333
, propositions	0.001123
propositions	0.000056
propositions ,	1.000000
, speech	0.004492
acts or	0.333333
or turns-at-talk	0.004505
turns-at-talk	0.000028
turns-at-talk .	1.000000
<s> Contrary	0.001537
Contrary	0.000056
Contrary to	1.000000
to much	0.001328
much of	0.090909
of traditional	0.000891
traditional	0.000028
traditional linguistics	1.000000
, discourse	0.000561
discourse analysts	0.055556
analysts	0.000056
analysts not	0.500000
only study	0.026316
study	0.000112
study language	0.250000
use `	0.013889
` beyond	0.062500
boundary '	0.166667
but also	0.073529
also prefer	0.014493
prefer	0.000056
prefer to	0.500000
to analyze	0.001328
analyze `	0.250000
` naturally	0.062500
naturally	0.000056
naturally occurring	0.500000
occurring	0.000028
occurring '	1.000000
' language	0.052632
use ,	0.055556
not invented	0.008929
invented	0.000056
invented examples	0.500000
<s> Text	0.001537
Text linguistics	0.166667
is related	0.002033
related .	0.066667
The essential	0.005208
essential	0.000028
essential difference	1.000000
difference	0.000112
difference between	0.250000
between discourse	0.128205
analysis and	0.030769
and text	0.005780
text linguistics	0.006289
it aims	0.008547
aims	0.000084
aims at	0.333333
at revealing	0.014706
revealing	0.000028
revealing socio-psychological	1.000000
socio-psychological	0.000028
socio-psychological characteristics	1.000000
characteristics	0.000056
characteristics of	0.500000
a person\/persons	0.001227
person\/persons	0.000028
person\/persons rather	1.000000
than text	0.022222
text structure	0.006289
structure .	0.166667
analysis has	0.015385
been taken	0.014706
taken	0.000084
taken up	0.333333
up in	0.090909
a variety	0.008589
variety	0.000223
variety of	1.000000
of social	0.001783
social science	0.071429
science disciplines	0.100000
disciplines	0.000056
disciplines ,	1.000000
, sociology	0.000561
sociology	0.000028
sociology ,	1.000000
, anthropology	0.000561
anthropology	0.000028
anthropology ,	1.000000
, social	0.001123
social work	0.071429
work ,	0.125000
, cognitive	0.000561
cognitive psychology	0.500000
psychology	0.000112
psychology ,	0.750000
social psychology	0.071429
, international	0.000561
international	0.000056
international relations	0.500000
relations	0.000335
relations ,	0.166667
human geography	0.021739
geography	0.000028
geography ,	1.000000
, communication	0.000561
communication	0.000140
communication studies	0.200000
studies and	0.250000
and translation	0.004335
translation studies	0.013514
is subject	0.002033
subject	0.000223
subject to	0.125000
to its	0.001328
own assumptions	0.166667
assumptions ,	0.200000
, dimensions	0.000561
dimensions	0.000084
dimensions of	0.666667
of analysis	0.000891
analysis ,	0.107692
and methodologies	0.001445
The examples	0.005208
examples and	0.166667
and perspective	0.001445
perspective in	0.250000
in this	0.018727
this article	0.043956
article deal	0.034483
deal primarily	0.250000
primarily	0.000056
primarily with	0.500000
United States	0.777778
States	0.000195
States and	0.142857
and do	0.001445
not represent	0.008929
represent	0.000251
represent a	0.222222
a worldwide	0.001227
worldwide	0.000028
worldwide view	1.000000
view	0.000084
view of	0.333333
the subject	0.003460
subject .	0.250000
<s> Please	0.002306
Please	0.000084
Please improve	0.333333
improve	0.000363
improve this	0.153846
article and	0.068966
and discuss	0.001445
discuss	0.000028
discuss the	1.000000
the issue	0.002768
issue	0.000223
issue on	0.125000
the talk	0.000692
talk	0.000028
talk page	1.000000
page .	0.142857
-LRB- December	0.002710
December	0.000028
December 2010	1.000000
2010	0.000084
2010 -RRB-	0.333333
-RRB- Some	0.002710
Some scholars	0.047619
scholars -LRB-	0.500000
-LRB- which	0.008130
which ?	0.007246
? -RRB-	0.125000
<s> consider	0.000769
consider the	0.500000
the Austrian	0.000692
Austrian	0.000028
Austrian emigre	1.000000
emigre	0.000028
emigre Leo	1.000000
Leo	0.000028
Leo Spitzer	1.000000
Spitzer	0.000028
Spitzer 's	1.000000
's Stilstudien	0.019608
Stilstudien	0.000028
Stilstudien -LRB-	1.000000
-LRB- Style	0.002710
Style	0.000028
Style Studies	1.000000
Studies	0.000028
Studies -RRB-	1.000000
of 1928	0.000891
1928	0.000028
1928 the	1.000000
the earliest	0.000692
earliest	0.000056
earliest example	0.500000
; Michel	0.021277
Michel	0.000084
Michel Foucault	1.000000
Foucault	0.000084
Foucault himself	0.333333
himself	0.000056
himself translated	0.500000
translated it	0.250000
But the	0.166667
the term	0.003460
term first	0.055556
first came	0.030303
came	0.000056
came into	0.500000
into general	0.012821
general use	0.045455
use following	0.013889
following the	0.066667
the publication	0.001384
publication of	0.666667
a series	0.007362
of papers	0.000891
papers by	0.333333
by Zellig	0.005714
Zellig	0.000084
Zellig Harris	1.000000
Harris	0.000251
Harris beginning	0.111111
beginning	0.000056
beginning in	0.500000
in 1952	0.001873
1952	0.000056
1952 and	0.500000
and reporting	0.001445
reporting	0.000084
reporting on	0.333333
on work	0.004717
work from	0.041667
which he	0.007246
he developed	0.142857
developed transformational	0.038462
grammar in	0.027027
late 1930s	0.111111
1930s	0.000028
1930s .	1.000000
<s> Formal	0.000769
Formal	0.000028
Formal equivalence	1.000000
equivalence	0.000056
equivalence relations	0.500000
relations among	0.166667
among the	0.125000
the sentences	0.005536
sentences of	0.026316
a coherent	0.002454
coherent discourse	0.200000
discourse are	0.027778
are made	0.012448
made explicit	0.062500
explicit by	0.200000
by using	0.017143
using sentence	0.016949
sentence transformations	0.020833
transformations	0.000056
transformations to	0.500000
to put	0.002656
put	0.000112
put the	0.250000
a canonical	0.001227
canonical	0.000028
canonical form	1.000000
form .	0.100000
<s> Words	0.001537
Words	0.000112
Words and	0.250000
and sentences	0.002890
sentences with	0.013158
equivalent information	0.200000
information then	0.021739
then appear	0.028571
appear	0.000446
appear in	0.437500
same column	0.040000
column	0.000028
column of	1.000000
an array	0.007576
array	0.000028
array .	1.000000
This work	0.031746
work progressed	0.041667
progressed	0.000028
progressed over	1.000000
over the	0.250000
the next	0.004152
next	0.000195
next four	0.142857
four decades	0.142857
decades	0.000028
decades -LRB-	1.000000
see references	0.050000
references	0.000112
references -RRB-	0.500000
-RRB- into	0.005420
a science	0.002454
science of	0.100000
of sublanguage	0.001783
sublanguage	0.000084
sublanguage analysis	0.333333
-LRB- Kittredge	0.002710
Kittredge	0.000056
Kittredge &	0.500000
&	0.000223
& Lehrberger	0.125000
Lehrberger	0.000028
Lehrberger 1982	1.000000
1982	0.000084
1982 -RRB-	0.333333
, culminating	0.000561
culminating	0.000028
culminating in	1.000000
demonstration of	0.400000
the informational	0.000692
informational	0.000056
informational structures	0.500000
structures in	0.200000
in texts	0.001873
a sublanguage	0.001227
sublanguage of	0.333333
of science	0.000891
, that	0.002246
of immunology	0.000891
immunology	0.000028
immunology ,	1.000000
, -LRB-	0.001684
-LRB- Harris	0.005420
Harris et	0.111111
et	0.000028
et al.	1.000000
al.	0.000028
al. 1989	1.000000
1989	0.000056
1989 -RRB-	0.500000
a fully	0.001227
fully articulated	0.166667
articulated	0.000028
articulated theory	1.000000
theory of	0.076923
of linguistic	0.001783
linguistic informational	0.062500
informational content	0.500000
content -LRB-	0.083333
Harris 1991	0.111111
most linguists	0.017241
linguists	0.000084
linguists decided	0.333333
decided	0.000084
decided a	0.333333
a succession	0.001227
succession	0.000028
succession of	1.000000
of elaborate	0.000891
elaborate	0.000028
elaborate theories	1.000000
of sentence-level	0.000891
sentence-level	0.000028
sentence-level syntax	1.000000
syntax and	0.090909
and semantics	0.004335
semantics .	0.071429
Although Harris	0.125000
Harris had	0.111111
had mentioned	0.071429
mentioned the	0.166667
whole discourses	0.111111
discourses	0.000056
discourses ,	0.500000
, he	0.001123
he had	0.142857
had not	0.071429
not worked	0.008929
worked out	0.200000
out a	0.071429
comprehensive model	0.200000
model ,	0.100000
as of	0.006969
of January	0.000891
January	0.000112
January ,	0.250000
, 1952	0.000561
1952 .	0.500000
A linguist	0.020000
linguist working	0.500000
working for	0.142857
the American	0.001384
American Bible	0.200000
Bible	0.000028
Bible Society	1.000000
Society	0.000028
Society ,	1.000000
, James	0.002246
James	0.000112
James A.	0.500000
A. Lauriault\/Loriot	0.400000
Lauriault\/Loriot	0.000056
Lauriault\/Loriot ,	1.000000
, needed	0.000561
needed to	0.095238
to find	0.010624
find answers	0.076923
answers to	0.083333
to some	0.006640
some fundamental	0.012048
fundamental	0.000056
fundamental errors	0.500000
errors in	0.200000
in translating	0.001873
translating Quechua	0.250000
Quechua	0.000056
Quechua ,	1.000000
the Cuzco	0.000692
Cuzco	0.000028
Cuzco area	1.000000
area	0.000307
area of	0.454545
of Peru	0.000891
Peru	0.000056
Peru .	0.500000
He took	0.125000
took	0.000028
took Harris	1.000000
Harris 's	0.222222
's idea	0.019608
idea ,	0.142857
, recorded	0.000561
recorded	0.000056
recorded all	0.500000
the legends	0.000692
legends	0.000028
legends and	1.000000
, after	0.000561
after going	0.083333
going over	0.250000
meaning and	0.043478
and placement	0.001445
placement	0.000028
placement of	1.000000
word with	0.016667
of Quechua	0.000891
was able	0.051948
to form	0.005312
form logical	0.050000
logical	0.000167
logical ,	0.166667
, mathematical	0.000561
mathematical	0.000056
mathematical rules	0.500000
that transcended	0.003546
transcended	0.000028
transcended the	1.000000
the simple	0.000692
simple sentence	0.038462
sentence structure	0.020833
He then	0.125000
then applied	0.057143
applied the	0.066667
process to	0.111111
another language	0.230769
language of	0.006757
of Eastern	0.000891
Eastern	0.000028
Eastern Peru	1.000000
Peru ,	0.500000
, Shipibo	0.000561
Shipibo	0.000056
Shipibo .	0.500000
He taught	0.125000
taught	0.000084
taught the	0.666667
the theory	0.002076
theory in	0.076923
in Norman	0.001873
Norman	0.000056
Norman ,	0.500000
, Oklahoma	0.000561
Oklahoma	0.000028
Oklahoma ,	1.000000
the summers	0.000692
summers	0.000028
summers of	1.000000
of 1956	0.000891
1956	0.000028
1956 and	1.000000
and 1957	0.001445
1957	0.000028
1957 and	1.000000
and entered	0.001445
entered the	0.500000
the University	0.000692
of Pennsylvania	0.000891
Pennsylvania	0.000028
Pennsylvania in	1.000000
the interim	0.000692
interim	0.000028
interim year	1.000000
year	0.000167
year .	0.500000
He tried	0.125000
tried	0.000084
tried to	0.333333
to publish	0.001328
publish	0.000028
publish a	1.000000
a paper	0.001227
paper	0.000307
paper Shipibo	0.090909
Shipibo Paragraph	0.500000
Paragraph	0.000028
Paragraph Structure	1.000000
Structure	0.000028
Structure ,	1.000000
but it	0.058824
it was	0.025641
was delayed	0.012987
delayed	0.000028
delayed until	1.000000
until 1970	0.500000
1970	0.000084
1970 -LRB-	0.333333
-LRB- Loriot	0.002710
Loriot	0.000028
Loriot &	1.000000
& Hollenbach	0.125000
Hollenbach	0.000028
Hollenbach 1970	1.000000
1970 -RRB-	0.333333
the meantime	0.000692
meantime	0.000028
meantime ,	1.000000
, Dr.	0.000561
Dr.	0.000028
Dr. Kenneth	1.000000
Kenneth	0.000028
Kenneth Lee	1.000000
Lee	0.000028
Lee Pike	1.000000
Pike	0.000028
Pike ,	1.000000
a professor	0.001227
professor	0.000028
professor at	1.000000
at University	0.014706
of Michigan	0.000891
Michigan	0.000028
Michigan ,	1.000000
, Ann	0.000561
Ann	0.000028
Ann Arbor	1.000000
Arbor	0.000028
Arbor ,	1.000000
, taught	0.000561
and one	0.001445
of his	0.002674
his students	0.166667
students	0.000084
students ,	0.333333
, Robert	0.001684
Robert	0.000112
Robert E.	0.500000
E. Longacre	0.500000
Longacre	0.000056
Longacre ,	1.000000
to disseminate	0.001328
disseminate	0.000028
disseminate it	1.000000
it in	0.008547
a dissertation	0.001227
dissertation	0.000084
dissertation .	0.333333
<s> Harris	0.000769
's methodology	0.019608
methodology	0.000056
methodology was	0.500000
was developed	0.012987
developed into	0.038462
system for	0.021505
the computer-aided	0.000692
computer-aided analysis	0.333333
language by	0.006757
a team	0.001227
team	0.000028
team led	1.000000
led	0.000084
led by	0.333333
by Naomi	0.005714
Naomi	0.000056
Naomi Sager	1.000000
Sager	0.000056
Sager at	0.500000
at NYU	0.014706
NYU	0.000028
NYU ,	1.000000
sublanguage domains	0.333333
domains ,	0.125000
most notably	0.017241
notably to	0.333333
to medical	0.001328
medical	0.000167
medical informatics	0.166667
informatics	0.000028
informatics .	1.000000
The software	0.005208
software for	0.037037
the Medical	0.000692
Medical	0.000056
Medical Language	0.500000
Language Processor	0.083333
Processor	0.000028
Processor is	1.000000
is publicly	0.002033
publicly	0.000028
publicly available	1.000000
available on	0.058824
on SourceForge	0.004717
SourceForge	0.000028
SourceForge .	1.000000
late 1960s	0.111111
1960s and	0.333333
and 1970s	0.001445
1970s	0.000084
1970s ,	0.333333
and without	0.002890
without reference	0.076923
reference to	0.250000
to this	0.007968
this prior	0.010989
prior work	0.333333
of other	0.003565
other approaches	0.014286
new cross-discipline	0.041667
cross-discipline	0.000028
cross-discipline of	1.000000
of DA	0.000891
DA began	0.333333
to develop	0.006640
develop	0.000140
develop in	0.200000
the humanities	0.000692
humanities	0.000028
humanities and	1.000000
and social	0.004335
social sciences	0.142857
sciences	0.000056
sciences concurrently	0.500000
concurrently	0.000028
concurrently with	1.000000
with ,	0.005464
, other	0.000561
other disciplines	0.014286
as semiotics	0.003484
semiotics	0.000028
semiotics ,	1.000000
, psycholinguistics	0.000561
psycholinguistics	0.000056
psycholinguistics ,	0.500000
, sociolinguistics	0.000561
sociolinguistics	0.000056
sociolinguistics ,	0.500000
and pragmatics	0.001445
pragmatics	0.000084
pragmatics .	0.333333
these approaches	0.047619
approaches ,	0.035714
those influenced	0.045455
influenced	0.000084
influenced by	1.000000
sciences ,	0.500000
, favor	0.000561
favor	0.000056
favor a	0.500000
more dynamic	0.010526
dynamic	0.000140
dynamic study	0.200000
study of	0.250000
of oral	0.000891
oral	0.000028
oral talk-in-interaction	1.000000
talk-in-interaction	0.000028
talk-in-interaction .	1.000000
<s> Mention	0.000769
Mention	0.000028
Mention must	1.000000
must also	0.071429
made of	0.187500
term ``	0.111111
`` Conversational	0.005291
Conversational	0.000028
Conversational analysis	1.000000
analysis ''	0.015385
was influenced	0.012987
the Sociologist	0.000692
Sociologist	0.000028
Sociologist Harold	1.000000
Harold	0.000028
Harold Garfinkel	1.000000
Garfinkel	0.000028
Garfinkel who	1.000000
the founder	0.000692
founder	0.000028
founder of	1.000000
of Ethnomethodology	0.000891
Ethnomethodology	0.000028
Ethnomethodology .	1.000000
In Europe	0.019048
Europe	0.000140
Europe ,	0.600000
, Michel	0.001123
Foucault became	0.333333
became one	0.200000
the key	0.000692
key	0.000167
key theorists	0.166667
theorists	0.000028
theorists of	1.000000
subject ,	0.250000
especially of	0.066667
and wrote	0.001445
wrote The	0.166667
The Archaeology	0.005208
Archaeology	0.000028
Archaeology of	1.000000
of Knowledge	0.000891
Knowledge	0.000056
Knowledge on	0.500000
<s> Topics	0.000769
Topics	0.000056
Topics of	1.000000
of interest	0.002674
interest Topics	0.090909
analysis include	0.015385
The various	0.005208
various levels	0.055556
levels	0.000614
levels or	0.045455
or dimensions	0.004505
as sounds	0.003484
sounds -LRB-	0.133333
-LRB- intonation	0.002710
intonation	0.000028
intonation ,	1.000000
, gestures	0.000561
gestures	0.000056
gestures ,	0.500000
the lexicon	0.000692
lexicon ,	0.111111
, style	0.000561
style	0.000056
style ,	0.500000
, rhetoric	0.000561
rhetoric	0.000028
rhetoric ,	1.000000
, meanings	0.000561
meanings ,	0.250000
acts ,	0.333333
, moves	0.000561
moves	0.000028
moves ,	1.000000
, strategies	0.000561
strategies	0.000056
strategies ,	0.500000
, turns	0.000561
turns	0.000084
turns and	0.333333
and other	0.013006
other aspects	0.014286
of interaction	0.000891
interaction Genres	0.125000
Genres	0.000028
Genres of	1.000000
discourse -LRB-	0.055556
-LRB- various	0.002710
various types	0.111111
discourse in	0.055556
in politics	0.001873
politics	0.000028
politics ,	1.000000
the media	0.000692
, education	0.000561
education	0.000028
education ,	1.000000
, science	0.000561
, business	0.000561
business	0.000112
business ,	0.250000
The relations	0.026042
relations between	0.416667
discourse and	0.111111
the emergence	0.000692
emergence	0.000028
emergence of	1.000000
of syntactic	0.001783
syntactic structure	0.076923
structure The	0.083333
between text	0.025641
-LRB- discourse	0.002710
discourse -RRB-	0.027778
and context	0.004335
context The	0.060606
and power	0.001445
power The	0.250000
and interaction	0.001445
interaction The	0.125000
and cognition	0.001445
cognition	0.000028
cognition and	1.000000
and memory	0.001445
memory	0.000056
memory Political	0.500000
Political	0.000084
Political discourse	1.000000
discourse Political	0.027778
analysis is	0.015385
analysis which	0.015385
which focuses	0.007246
focuses	0.000056
focuses on	1.000000
on discourse	0.004717
in political	0.001873
political	0.000084
political forums	0.333333
forums	0.000028
forums -LRB-	1.000000
as debates	0.003484
debates	0.000056
debates ,	1.000000
, speeches	0.000561
speeches	0.000028
speeches ,	1.000000
and hearings	0.001445
hearings	0.000028
hearings -RRB-	1.000000
the phenomenon	0.001384
phenomenon	0.000140
phenomenon of	0.600000
interest .	0.090909
<s> Political	0.000769
discourse is	0.083333
the informal	0.000692
informal	0.000056
informal exchange	0.500000
exchange	0.000028
exchange of	1.000000
of reasoned	0.000891
reasoned	0.000028
reasoned views	1.000000
views	0.000028
views as	1.000000
as to	0.013937
to which	0.006640
which of	0.007246
of several	0.002674
several alternative	0.045455
alternative	0.000084
alternative courses	0.333333
courses	0.000028
courses of	1.000000
of action	0.000891
action	0.000140
action should	0.200000
be taken	0.004219
taken to	0.333333
solve a	0.250000
a societal	0.001227
societal	0.000028
societal problem	1.000000
science that	0.100000
been used	0.073529
used through	0.008850
the history	0.000692
States .	0.285714
the essence	0.001384
essence	0.000056
essence of	1.000000
of democracy	0.000891
democracy	0.000028
democracy .	1.000000
<s> Full	0.000769
Full	0.000028
Full of	1.000000
problems and	0.117647
and persuasion	0.001445
persuasion	0.000028
persuasion ,	1.000000
, political	0.000561
political discourse	0.333333
in many	0.014981
many debates	0.019231
, candidacies	0.000561
candidacies	0.000028
candidacies and	1.000000
in our	0.001873
our	0.000140
our everyday	0.200000
everyday	0.000028
everyday life	1.000000
life .	0.250000
<s> Perspectives	0.000769
Perspectives	0.000028
Perspectives The	1.000000
following are	0.066667
are some	0.004149
specific theoretical	0.047619
theoretical perspectives	0.333333
perspectives	0.000028
perspectives and	1.000000
and analytical	0.001445
analytical	0.000056
analytical approaches	0.500000
approaches used	0.035714
linguistic discourse	0.062500
: Emergent	0.009804
Emergent	0.000028
Emergent grammar	1.000000
grammar Text	0.027027
Text grammar	0.166667
grammar -LRB-	0.027027
or `	0.004505
` discourse	0.062500
discourse grammar	0.027778
grammar '	0.027027
' -RRB-	0.052632
-RRB- Cohesion	0.002710
Cohesion	0.000028
Cohesion and	1.000000
and relevance	0.001445
relevance	0.000084
relevance theory	0.333333
theory Functional	0.076923
Functional	0.000028
Functional grammar	1.000000
grammar Rhetoric	0.027027
Rhetoric	0.000028
Rhetoric Stylistics	1.000000
Stylistics	0.000028
Stylistics -LRB-	1.000000
-LRB- linguistics	0.005420
linguistics -RRB-	0.100000
-RRB- Interactional	0.002710
Interactional	0.000028
Interactional sociolinguistics	1.000000
sociolinguistics Ethnography	0.500000
Ethnography	0.000028
Ethnography of	1.000000
of communication	0.001783
communication Pragmatics	0.200000
Pragmatics	0.000028
Pragmatics ,	1.000000
, particularly	0.001123
particularly speech	0.200000
speech act	0.006579
act	0.000112
act theory	0.250000
theory Conversation	0.076923
Conversation	0.000028
Conversation analysis	1.000000
analysis Variation	0.015385
Variation	0.000028
Variation analysis	1.000000
analysis Applied	0.015385
Applied	0.000056
Applied linguistics	0.500000
linguistics Cognitive	0.050000
Cognitive	0.000084
Cognitive psychology	0.333333
often under	0.022727
under the	0.200000
the label	0.000692
label	0.000028
label discourse	1.000000
discourse processing	0.027778
, studying	0.000561
studying	0.000028
studying the	1.000000
the production	0.000692
production	0.000084
production and	0.333333
and comprehension	0.001445
comprehension	0.000195
comprehension of	0.285714
discourse .	0.027778
<s> Discursive	0.000769
Discursive	0.000028
Discursive psychology	1.000000
psychology Response	0.250000
Response	0.000028
Response based	1.000000
based therapy	0.018519
therapy	0.000028
therapy -LRB-	1.000000
-LRB- counselling	0.002710
counselling	0.000028
counselling -RRB-	1.000000
-RRB- Critical	0.002710
Critical	0.000056
Critical discourse	0.500000
analysis Sublanguage	0.015385
Sublanguage	0.000028
Sublanguage analysis	1.000000
analysis Genre	0.015385
Genre	0.000056
Genre Analysis	1.000000
Analysis &	0.200000
& Critical	0.125000
Critical Genre	0.500000
Analysis Although	0.200000
Although these	0.125000
approaches emphasize	0.035714
emphasize	0.000028
emphasize different	1.000000
different aspects	0.020408
they all	0.050000
all view	0.023256
view language	0.333333
language as	0.006757
as social	0.003484
social interaction	0.071429
interaction ,	0.125000
and are	0.007225
are concerned	0.004149
social contexts	0.071429
contexts	0.000195
contexts in	0.142857
in which	0.014981
which discourse	0.007246
is embedded	0.002033
embedded .	0.250000
Often a	0.333333
a distinction	0.001227
distinction	0.000140
distinction is	0.400000
is made	0.004065
made between	0.062500
between `	0.025641
` local	0.062500
local	0.000084
local '	0.333333
' structures	0.105263
structures of	0.200000
as relations	0.003484
among sentences	0.125000
and turns	0.001445
turns -RRB-	0.333333
and `	0.001445
` global	0.062500
global	0.000084
global '	0.333333
structures ,	0.200000
as overall	0.003484
overall topics	0.166667
topics	0.000195
topics and	0.142857
the schematic	0.000692
schematic	0.000028
schematic organization	1.000000
organization of	0.400000
of discourses	0.000891
discourses and	0.500000
and conversations	0.001445
conversations	0.000084
conversations .	0.333333
many types	0.019231
discourse begin	0.027778
begin	0.000084
begin with	0.666667
some kind	0.048193
of global	0.000891
global `	0.333333
` summary	0.062500
summary '	0.023810
in titles	0.001873
titles	0.000056
titles ,	0.500000
, headlines	0.000561
headlines	0.000028
headlines ,	1.000000
, leads	0.000561
leads	0.000028
leads ,	1.000000
, abstracts	0.000561
abstracts	0.000056
abstracts ,	0.500000
and so	0.008671
so on	0.166667
on .	0.018868
A problem	0.020000
problem for	0.022727
discourse analyst	0.027778
analyst	0.000028
analyst is	1.000000
to decide	0.002656
decide	0.000112
decide when	0.250000
a particular	0.004908
particular feature	0.076923
feature is	0.076923
is relevant	0.002033
relevant	0.000195
relevant to	0.142857
the specification	0.001384
specification	0.000056
specification is	0.500000
is required	0.004065
required	0.000195
required .	0.142857
<s> Are	0.000769
Are	0.000028
Are there	1.000000
there general	0.025000
general principles	0.045455
principles	0.000028
principles which	1.000000
will determine	0.028571
the relevance	0.000692
relevance or	0.333333
or nature	0.004505
specification .	0.500000
<s> Prominent	0.000769
Prominent	0.000028
Prominent discourse	1.000000
analysts This	0.500000
This article	0.015873
article contains	0.034483
contains embedded	0.100000
embedded lists	0.250000
lists	0.000028
lists that	1.000000
that may	0.007092
be poorly	0.004219
poorly	0.000028
poorly defined	1.000000
defined ,	0.166667
, unverified	0.000561
unverified	0.000028
unverified or	1.000000
or indiscriminate	0.004505
indiscriminate	0.000028
indiscriminate .	1.000000
Please help	0.666667
help	0.000251
help to	0.111111
to clean	0.001328
clean	0.000056
clean it	0.500000
it up	0.008547
up to	0.227273
to meet	0.005312
meet	0.000112
meet Wikipedia	0.250000
Wikipedia	0.000056
Wikipedia 's	0.500000
's quality	0.019608
quality standards	0.100000
standards .	0.400000
-LRB- May	0.005420
May	0.000056
May 2012	0.500000
2012	0.000028
2012 -RRB-	1.000000
-RRB- Marc	0.002710
Marc	0.000028
Marc Angenot	1.000000
Angenot	0.000028
Angenot ,	1.000000
Robert de	0.250000
de Beaugrande	0.500000
Beaugrande	0.000028
Beaugrande ,	1.000000
, Jan	0.000561
Jan	0.000028
Jan Blommaert	1.000000
Blommaert	0.000028
Blommaert ,	1.000000
, Adriana	0.000561
Adriana	0.000028
Adriana Bolivar	1.000000
Bolivar	0.000028
Bolivar ,	1.000000
, Carmen	0.000561
Carmen	0.000028
Carmen Rosa	1.000000
Rosa	0.000028
Rosa Caldas-Coulthard	1.000000
Caldas-Coulthard	0.000028
Caldas-Coulthard ,	1.000000
, Robyn	0.000561
Robyn	0.000028
Robyn Carston	1.000000
Carston	0.000028
Carston ,	1.000000
, Wallace	0.000561
Wallace	0.000028
Wallace Chafe	1.000000
Chafe	0.000028
Chafe ,	1.000000
, Paul	0.001684
Paul	0.000140
Paul Chilton	0.200000
Chilton	0.000028
Chilton ,	1.000000
, Guy	0.000561
Guy	0.000028
Guy Cook	1.000000
Cook	0.000028
Cook ,	1.000000
, Malcolm	0.000561
Malcolm	0.000028
Malcolm Coulthard	1.000000
Coulthard	0.000028
Coulthard ,	1.000000
James Deese	0.250000
Deese	0.000028
Deese ,	1.000000
Paul Drew	0.200000
Drew	0.000028
Drew ,	1.000000
, John	0.002807
John Du	0.125000
Du	0.000028
Du Bois	1.000000
Bois	0.000028
Bois ,	1.000000
, Alessandro	0.000561
Alessandro	0.000028
Alessandro Duranti	1.000000
Duranti	0.000028
Duranti ,	1.000000
, Brenton	0.000561
Brenton	0.000028
Brenton D.	1.000000
D. Faber	0.200000
Faber	0.000028
Faber ,	1.000000
, Norman	0.000561
Norman Fairclough	0.500000
Fairclough	0.000028
Fairclough ,	1.000000
Foucault ,	0.333333
, Roger	0.000561
Roger	0.000112
Roger Fowler	0.250000
Fowler	0.000028
Fowler ,	1.000000
James Paul	0.250000
Paul Gee	0.200000
Gee	0.000028
Gee ,	1.000000
, Talmy	0.000561
Talmy	0.000028
Talmy Givón	1.000000
Givón	0.000028
Givón ,	1.000000
, Charles	0.000561
Charles	0.000028
Charles Goodwin	1.000000
Goodwin	0.000028
Goodwin ,	1.000000
, Art	0.000561
Art	0.000028
Art Graesser	1.000000
Graesser	0.000028
Graesser ,	1.000000
, Michael	0.002246
Michael	0.000112
Michael Halliday	0.250000
Halliday	0.000028
Halliday ,	1.000000
, Zellig	0.001123
Harris ,	0.111111
John Heritage	0.125000
Heritage	0.000028
Heritage ,	1.000000
, Janet	0.000561
Janet	0.000056
Janet Holmes	0.500000
Holmes	0.000028
Holmes ,	1.000000
, David	0.001684
David	0.000112
David R.	0.250000
R. Howarth	0.166667
Howarth	0.000028
Howarth ,	1.000000
Paul Hopper	0.200000
Hopper	0.000028
Hopper ,	1.000000
, Gail	0.000561
Gail	0.000028
Gail Jefferson	1.000000
Jefferson	0.000028
Jefferson ,	1.000000
, Barbara	0.000561
Barbara	0.000028
Barbara Johnstone	1.000000
Johnstone	0.000028
Johnstone ,	1.000000
, Walter	0.000561
Walter	0.000028
Walter Kintsch	1.000000
Kintsch	0.000028
Kintsch ,	1.000000
, Richard	0.000561
Richard	0.000028
Richard Kittredge	1.000000
Kittredge ,	0.500000
, Adam	0.000561
Adam	0.000028
Adam Jaworski	1.000000
Jaworski	0.000028
Jaworski ,	1.000000
, William	0.001123
William	0.000056
William Labov	0.500000
Labov	0.000028
Labov ,	1.000000
, George	0.000561
George	0.000028
George Lakoff	1.000000
Lakoff	0.000028
Lakoff ,	1.000000
, Jay	0.000561
Jay	0.000028
Jay Lemke	1.000000
Lemke	0.000028
Lemke ,	1.000000
, Stephen	0.000561
Stephen	0.000028
Stephen H.	1.000000
H.	0.000056
H. Levinsohn	0.500000
Levinsohn	0.000028
Levinsohn ,	1.000000
, Jim	0.000561
Jim	0.000028
Jim Martin	1.000000
Martin	0.000056
Martin ,	0.500000
, Aletta	0.000561
Aletta	0.000028
Aletta Norval	1.000000
Norval	0.000028
Norval ,	1.000000
David Nunan	0.250000
Nunan	0.000028
Nunan ,	1.000000
, Elinor	0.000561
Elinor	0.000028
Elinor Ochs	1.000000
Ochs	0.000028
Ochs ,	1.000000
, Gina	0.000561
Gina	0.000028
Gina Poncini	1.000000
Poncini	0.000028
Poncini ,	1.000000
, Jonathan	0.000561
Jonathan	0.000028
Jonathan Potter	1.000000
Potter	0.000028
Potter ,	1.000000
, Edward	0.000561
Edward	0.000028
Edward Robinson	1.000000
Robinson	0.000028
Robinson ,	1.000000
, Nikolas	0.000561
Nikolas	0.000028
Nikolas Rose	1.000000
Rose	0.000028
Rose ,	1.000000
, Harvey	0.000561
Harvey	0.000028
Harvey Sacks	1.000000
Sacks	0.000028
Sacks ,	1.000000
, Svenka	0.000561
Svenka	0.000028
Svenka Savic	1.000000
Savic	0.000028
Savic Naomi	1.000000
Sager ,	0.500000
, Emanuel	0.001123
Emanuel	0.000056
Emanuel Schegloff	0.500000
Schegloff	0.000028
Schegloff ,	1.000000
, Deborah	0.001123
Deborah	0.000056
Deborah Schiffrin	0.500000
Schiffrin	0.000028
Schiffrin ,	1.000000
Michael Schober	0.250000
Schober	0.000028
Schober ,	1.000000
, Stef	0.000561
Stef	0.000028
Stef Slembrouck	1.000000
Slembrouck	0.000028
Slembrouck ,	1.000000
Michael Stubbs	0.250000
Stubbs	0.000028
Stubbs ,	1.000000
John Swales	0.250000
Swales	0.000056
Swales ,	1.000000
Deborah Tannen	0.500000
Tannen	0.000028
Tannen ,	1.000000
, Sandra	0.000561
Sandra	0.000028
Sandra Thompson	1.000000
Thompson	0.000028
Thompson ,	1.000000
, Teun	0.000561
Teun	0.000028
Teun A.	1.000000
A. van	0.200000
van	0.000056
van Dijk	0.500000
Dijk	0.000028
Dijk ,	1.000000
, Theo	0.000561
Theo	0.000028
Theo van	1.000000
van Leeuwen	0.500000
Leeuwen	0.000028
Leeuwen ,	1.000000
, Jef	0.000561
Jef	0.000028
Jef Verschueren	1.000000
Verschueren	0.000028
Verschueren ,	1.000000
, Henry	0.000561
Henry	0.000056
Henry Widdowson	0.500000
Widdowson	0.000028
Widdowson ,	1.000000
, Carla	0.000561
Carla	0.000028
Carla Willig	1.000000
Willig	0.000028
Willig ,	1.000000
, Deirdre	0.000561
Deirdre	0.000028
Deirdre Wilson	1.000000
Wilson	0.000028
Wilson ,	1.000000
, Ruth	0.000561
Ruth	0.000028
Ruth Wodak	1.000000
Wodak	0.000028
Wodak ,	1.000000
, Margaret	0.000561
Margaret	0.000028
Margaret Wetherell	1.000000
Wetherell	0.000028
Wetherell ,	1.000000
, Ernesto	0.000561
Ernesto	0.000028
Ernesto Laclau	1.000000
Laclau	0.000028
Laclau ,	1.000000
, Chantal	0.000561
Chantal	0.000028
Chantal Mouffe	1.000000
Mouffe	0.000028
Mouffe ,	1.000000
, Judith	0.000561
Judith	0.000028
Judith M.	1.000000
M. De	0.250000
De	0.000028
De Guzman	1.000000
Guzman	0.000028
Guzman ,	1.000000
, Cynthia	0.000561
Cynthia	0.000028
Cynthia Hardy	1.000000
Hardy	0.000028
Hardy ,	1.000000
, Louise	0.000561
Louise	0.000028
Louise J.	1.000000
J. Phillips	0.333333
Phillips	0.000028
Phillips .	1.000000
-RRB- Bhatia	0.002710
Bhatia	0.000028
Bhatia ,	1.000000
, V.J.	0.000561
V.J.	0.000028
V.J. ,	1.000000
Harris The	0.111111
The phenomenon	0.005208
of information	0.004456
information overload	0.021739
overload	0.000028
overload has	1.000000
has meant	0.011905
meant	0.000056
meant that	0.500000
that access	0.003546
access	0.000084
access to	1.000000
to coherent	0.001328
coherent and	0.200000
and correctly-developed	0.001445
correctly-developed	0.000028
correctly-developed summaries	1.000000
summaries is	0.069767
is vital	0.002033
vital	0.000028
vital .	1.000000
As access	0.055556
to data	0.002656
data has	0.012987
has increased	0.011905
increased so	0.200000
so has	0.033333
has interest	0.011905
interest in	0.636364
automatic summarization	0.086957
summarization .	0.120000
of summarization	0.007130
summarization technology	0.020000
technology is	0.136364
is search	0.002033
search engines	0.181818
engines	0.000084
engines such	0.333333
as Google	0.003484
Google .	0.500000
<s> Technologies	0.000769
Technologies	0.000028
Technologies that	1.000000
make a	0.200000
coherent summary	0.200000
summary ,	0.142857
any kind	0.032258
, need	0.000561
need to	0.476190
take into	0.300000
into account	0.038462
account	0.000084
account several	0.333333
several variables	0.045455
variables	0.000028
variables such	1.000000
as length	0.003484
length	0.000223
length ,	0.250000
writing style	0.111111
style and	0.500000
and syntax	0.001445
syntax to	0.181818
to make	0.005312
a useful	0.001227
useful summary	0.071429
summary .	0.261905
<s> Extractive	0.000769
Extractive	0.000028
Extractive methods	1.000000
methods work	0.045455
work by	0.083333
by selecting	0.011429
selecting	0.000140
selecting a	0.400000
a subset	0.003681
subset	0.000084
subset of	1.000000
existing words	0.200000
, phrases	0.000561
phrases ,	0.125000
or sentences	0.004505
sentences in	0.105263
form the	0.050000
the summary	0.005536
In contrast	0.047619
contrast ,	0.625000
, abstractive	0.000561
abstractive	0.000167
abstractive methods	0.333333
methods build	0.022727
build	0.000084
build an	0.666667
an internal	0.022727
internal	0.000140
internal semantic	0.200000
semantic representation	0.047619
representation and	0.105263
then use	0.028571
use natural	0.013889
generation techniques	0.111111
techniques to	0.173913
to create	0.011952
create a	0.411765
a summary	0.009816
summary that	0.047619
is closer	0.002033
what a	0.093750
human might	0.021739
might generate	0.038462
generate .	0.055556
Such a	0.125000
summary might	0.023810
might contain	0.038462
contain	0.000335
contain words	0.083333
words not	0.009174
not explicitly	0.008929
explicitly	0.000112
explicitly present	0.250000
present	0.000167
present in	0.833333
original .	0.076923
The state-of-the-art	0.005208
state-of-the-art	0.000056
state-of-the-art abstractive	0.500000
methods are	0.045455
still quite	0.066667
quite weak	0.125000
weak	0.000028
weak ,	1.000000
so most	0.033333
most research	0.017241
on extractive	0.004717
extractive	0.000195
extractive methods	0.142857
methods ,	0.090909
and this	0.001445
is what	0.004065
what we	0.093750
we will	0.088889
will cover	0.028571
cover	0.000028
cover .	1.000000
<s> Two	0.005380
Two	0.000195
Two particular	0.142857
particular types	0.076923
summarization often	0.020000
often addressed	0.022727
addressed	0.000056
addressed in	0.500000
the literature	0.000692
literature	0.000028
literature are	1.000000
are keyphrase	0.004149
keyphrase	0.000530
keyphrase extraction	0.631579
the goal	0.002768
goal is	0.428571
select individual	0.166667
individual words	0.083333
or phrases	0.009009
phrases to	0.062500
`` tag	0.010582
tag ''	0.062500
a document	0.008589
document	0.001004
document ,	0.138889
and document	0.002890
document summarization	0.138889
select whole	0.166667
whole sentences	0.222222
sentences to	0.078947
a short	0.006135
short	0.000223
short paragraph	0.125000
paragraph	0.000084
paragraph summary	0.333333
<s> Extraction	0.001537
Extraction	0.000084
Extraction and	0.333333
and abstraction	0.002890
abstraction	0.000112
abstraction Broadly	0.250000
Broadly	0.000028
Broadly ,	1.000000
, one	0.003369
one distinguishes	0.015385
distinguishes two	0.500000
two approaches	0.034483
approaches :	0.142857
: extraction	0.009804
extraction and	0.064516
abstraction .	0.250000
Extraction techniques	0.333333
techniques merely	0.043478
merely	0.000056
merely copy	0.500000
copy	0.000028
copy the	1.000000
the information	0.003460
information deemed	0.021739
deemed	0.000056
deemed most	0.500000
important by	0.062500
system to	0.053763
summary -LRB-	0.023810
, key	0.000561
key clauses	0.166667
clauses	0.000028
clauses ,	1.000000
, sentences	0.001123
sentences or	0.013158
or paragraphs	0.009009
paragraphs	0.000112
paragraphs -RRB-	0.250000
while abstraction	0.050000
abstraction involves	0.250000
involves paraphrasing	0.100000
paraphrasing	0.000028
paraphrasing sections	1.000000
sections	0.000056
sections of	1.000000
source document	0.083333
document .	0.138889
In general	0.028571
general ,	0.272727
, abstraction	0.000561
abstraction can	0.250000
can condense	0.005525
condense	0.000028
condense a	1.000000
text more	0.006289
more strongly	0.010526
strongly	0.000056
strongly than	0.500000
than extraction	0.022222
the programs	0.000692
programs that	0.090909
can do	0.011050
do this	0.076923
this are	0.021978
are harder	0.004149
harder to	0.285714
develop as	0.200000
they require	0.050000
generation technology	0.111111
technology ,	0.136364
which itself	0.007246
itself is	0.200000
a growing	0.001227
field .	0.148148
<s> Types	0.001537
Types	0.000056
Types of	1.000000
of summaries	0.003565
summaries There	0.023256
are different	0.004149
summaries depending	0.046512
depending	0.000112
depending what	0.250000
the summarization	0.002076
summarization program	0.020000
program focuses	0.045455
on to	0.009434
make the	0.050000
example generic	0.012346
generic summaries	0.333333
summaries or	0.023256
or query	0.004505
query	0.000084
query relevant	0.666667
relevant summaries	0.142857
summaries -LRB-	0.046512
-LRB- sometimes	0.002710
sometimes called	0.076923
called query-biased	0.055556
query-biased	0.000028
query-biased summaries	1.000000
summaries -RRB-	0.023256
<s> Summarization	0.001537
Summarization	0.000112
Summarization systems	0.250000
create both	0.058824
both query	0.032258
relevant text	0.142857
text summaries	0.006289
summaries and	0.046512
and generic	0.001445
generic machine-generated	0.333333
machine-generated	0.000028
machine-generated summaries	1.000000
depending on	0.750000
on what	0.009434
user needs	0.071429
needs .	0.100000
Summarization of	0.250000
of multimedia	0.000891
multimedia	0.000056
multimedia documents	0.500000
e.g. pictures	0.017857
pictures	0.000028
pictures or	1.000000
or movies	0.004505
movies	0.000028
movies ,	1.000000
also possible	0.043478
possible .	0.125000
Some systems	0.095238
systems will	0.008929
will generate	0.085714
generate a	0.333333
summary based	0.023810
a single	0.011043
single	0.000391
single source	0.142857
others can	0.083333
use multiple	0.013889
multiple source	0.076923
source documents	0.125000
a cluster	0.002454
cluster	0.000056
cluster of	1.000000
of news	0.001783
news	0.000363
news stories	0.076923
stories	0.000028
stories on	1.000000
same topic	0.040000
topic -RRB-	0.125000
are known	0.012448
as multi-document	0.003484
multi-document	0.000112
multi-document summarization	0.750000
summarization systems	0.100000
<s> Keyphrase	0.002306
Keyphrase	0.000112
Keyphrase extraction	0.500000
extraction Task	0.032258
Task	0.000084
Task description	0.333333
description	0.000028
description and	1.000000
and example	0.001445
example The	0.012346
The task	0.020833
the following	0.004844
following .	0.133333
<s> You	0.000769
You	0.000028
You are	1.000000
are given	0.012448
a piece	0.002454
piece	0.000084
piece of	1.000000
a journal	0.001227
journal article	0.333333
article ,	0.103448
and you	0.004335
you must	0.076923
must produce	0.071429
of keywords	0.000891
keywords	0.000056
keywords or	0.500000
or keyphrases	0.004505
keyphrases	0.000977
keyphrases that	0.085714
that capture	0.003546
capture	0.000056
capture the	0.500000
the primary	0.001384
primary	0.000056
primary topics	0.500000
topics discussed	0.142857
discussed in	0.142857
the case	0.005536
case of	0.352941
research articles	0.023810
articles ,	0.125000
many authors	0.019231
authors provide	0.200000
provide manually	0.166667
manually	0.000112
manually assigned	0.250000
assigned	0.000056
assigned keywords	0.500000
keywords ,	0.500000
most text	0.017241
text lacks	0.006289
lacks	0.000028
lacks pre-existing	1.000000
pre-existing	0.000056
pre-existing keyphrases	0.500000
keyphrases .	0.314286
, news	0.000561
news articles	0.230769
articles rarely	0.125000
rarely	0.000084
rarely have	0.333333
have keyphrases	0.019231
keyphrases attached	0.028571
attached ,	0.500000
it would	0.034188
be useful	0.012658
useful to	0.142857
automatically do	0.047619
do so	0.038462
so for	0.033333
of applications	0.002674
applications discussed	0.040000
discussed below	0.428571
below .	0.400000
example text	0.012346
a recent	0.001227
recent	0.000223
recent news	0.125000
news article	0.153846
: ``	0.019608
`` The	0.015873
The Army	0.005208
Army	0.000112
Army Corps	0.500000
Corps	0.000056
Corps of	1.000000
of Engineers	0.001783
Engineers	0.000056
Engineers ,	0.500000
, rushing	0.000561
rushing	0.000028
rushing to	1.000000
meet President	0.250000
President	0.000112
President Bush	0.500000
Bush	0.000056
Bush 's	0.500000
's promise	0.019608
promise	0.000028
promise to	1.000000
to protect	0.001328
protect	0.000028
protect New	1.000000
New	0.000056
New Orleans	1.000000
Orleans	0.000056
Orleans by	0.500000
the start	0.002768
start	0.000195
start of	0.285714
the 2006	0.000692
2006 hurricane	0.333333
hurricane	0.000028
hurricane season	1.000000
season	0.000028
season ,	1.000000
, installed	0.000561
installed	0.000084
installed defective	0.333333
defective	0.000056
defective flood-control	1.000000
flood-control	0.000056
flood-control pumps	1.000000
pumps	0.000056
pumps last	0.500000
last	0.000140
last year	0.200000
year despite	0.166667
despite	0.000084
despite warnings	0.333333
warnings	0.000028
warnings from	1.000000
from its	0.009615
own expert	0.166667
expert	0.000028
expert that	1.000000
the equipment	0.000692
equipment	0.000084
equipment would	0.333333
would fail	0.018868
fail	0.000084
fail during	0.333333
during a	0.200000
a storm	0.001227
storm	0.000028
storm ,	1.000000
, according	0.000561
according	0.000140
according to	1.000000
to documents	0.001328
documents obtained	0.026316
obtained	0.000195
obtained by	0.571429
by The	0.005714
The Associated	0.005208
Associated	0.000028
Associated Press	1.000000
Press	0.000028
Press ''	1.000000
An extractive	0.062500
extractive keyphrase	0.142857
keyphrase extractor	0.052632
extractor	0.000056
extractor might	0.500000
might select	0.038462
select ``	0.166667
`` Army	0.005291
Engineers ''	0.500000
`` President	0.005291
Bush ''	0.500000
`` New	0.005291
Orleans ''	0.500000
`` defective	0.005291
pumps ''	0.500000
as keyphrases	0.003484
These are	0.117647
are pulled	0.004149
pulled	0.000028
pulled directly	1.000000
directly	0.000140
directly from	0.200000
an abstractive	0.015152
abstractive keyphrase	0.166667
keyphrase system	0.052632
system would	0.021505
would somehow	0.018868
somehow	0.000028
somehow internalize	1.000000
internalize	0.000028
internalize the	1.000000
the content	0.002076
content and	0.166667
and generate	0.001445
generate keyphrases	0.055556
more descriptive	0.010526
descriptive	0.000084
descriptive and	0.333333
more like	0.010526
like what	0.035714
human would	0.021739
would produce	0.037736
produce ,	0.045455
`` political	0.005291
political negligence	0.333333
negligence	0.000028
negligence ''	1.000000
'' or	0.025773
or ``	0.018018
`` inadequate	0.005291
inadequate	0.000028
inadequate protection	1.000000
protection	0.000028
protection from	1.000000
from floods	0.009615
floods	0.000028
floods ''	1.000000
that these	0.010638
these terms	0.023810
terms do	0.076923
not appear	0.008929
text and	0.018868
and require	0.004335
a deep	0.001227
deep understanding	0.285714
understanding ,	0.090909
makes it	0.250000
it difficult	0.017094
difficult for	0.035714
computer to	0.045455
produce such	0.045455
such keyphrases	0.008130
<s> Keyphrases	0.000769
Keyphrases	0.000028
Keyphrases have	1.000000
many applications	0.038462
to improve	0.011952
improve document	0.076923
document browsing	0.027778
browsing	0.000028
browsing by	1.000000
by providing	0.005714
providing a	0.500000
short summary	0.125000
<s> Also	0.002306
Also	0.000084
Also ,	1.000000
, keyphrases	0.000561
keyphrases can	0.057143
can improve	0.005525
improve information	0.076923
information retrieval	0.108696
retrieval --	0.142857
-- if	0.080000
if documents	0.035714
documents have	0.026316
keyphrases assigned	0.028571
assigned ,	0.500000
a user	0.002454
user could	0.071429
could	0.000446
could search	0.062500
search by	0.090909
by keyphrase	0.005714
keyphrase to	0.052632
reliable hits	0.250000
hits	0.000028
hits than	1.000000
than a	0.111111
a full-text	0.001227
full-text	0.000028
full-text search	1.000000
search .	0.090909
automatic keyphrase	0.043478
extraction can	0.032258
useful in	0.142857
in generating	0.001873
generating	0.000140
generating index	0.200000
index	0.000028
index entries	1.000000
entries for	0.500000
large text	0.043478
text corpus	0.012579
corpus .	0.032258
extraction as	0.064516
as supervised	0.003484
learning Beginning	0.023256
Beginning with	0.500000
the Turney	0.000692
Turney	0.000251
Turney paper	0.222222
paper ,	0.090909
many researchers	0.019231
researchers	0.000279
researchers have	0.300000
have approached	0.009615
approached keyphrase	0.500000
a supervised	0.002454
supervised machine	0.062500
learning problem	0.023256
we construct	0.022222
construct	0.000084
construct an	0.333333
example for	0.024691
each unigram	0.044444
unigram	0.000140
unigram ,	0.600000
, bigram	0.001684
bigram	0.000084
bigram ,	1.000000
and trigram	0.001445
trigram	0.000084
trigram found	0.333333
found in	0.214286
-LRB- though	0.002710
though	0.000279
though other	0.100000
other text	0.028571
text units	0.018868
units	0.000195
units are	0.285714
as discussed	0.003484
<s> We	0.005380
We	0.000195
We then	0.142857
then compute	0.028571
compute	0.000056
compute various	0.500000
various features	0.055556
features describing	0.038462
describing	0.000112
describing each	0.250000
each example	0.044444
example -LRB-	0.012346
, does	0.001123
does the	0.100000
the phrase	0.002768
phrase begin	0.100000
with an	0.027322
an upper-case	0.007576
upper-case	0.000028
upper-case letter	1.000000
letter	0.000167
letter ?	0.166667
We assume	0.142857
assume there	0.500000
known keyphrases	0.153846
keyphrases available	0.028571
available for	0.117647
training documents	0.107143
documents .	0.131579
Using the	0.500000
the known	0.003460
keyphrases ,	0.057143
we can	0.066667
can assign	0.005525
assign	0.000140
assign positive	0.200000
positive	0.000195
positive or	0.285714
or negative	0.009009
negative	0.000223
negative labels	0.125000
labels	0.000056
labels to	1.000000
the examples	0.002076
Then we	0.200000
we learn	0.022222
learn a	0.230769
a classifier	0.001227
classifier	0.000195
classifier that	0.142857
can discriminate	0.005525
discriminate	0.000084
discriminate between	0.333333
between positive	0.025641
positive and	0.285714
and negative	0.002890
negative examples	0.125000
examples as	0.041667
a function	0.001227
function of	0.125000
features .	0.076923
Some classifiers	0.047619
classifiers	0.000056
classifiers make	0.500000
a binary	0.002454
binary	0.000112
binary classification	0.500000
classification	0.000474
classification for	0.117647
a test	0.003681
test example	0.100000
others assign	0.083333
assign a	0.400000
a probability	0.001227
probability	0.000195
probability of	0.142857
of being	0.000891
being a	0.111111
a keyphrase	0.002454
keyphrase .	0.052632
the above	0.001384
above text	0.076923
we might	0.022222
might learn	0.038462
a rule	0.001227
rule	0.000084
rule that	0.333333
that says	0.003546
says	0.000028
says phrases	1.000000
phrases with	0.062500
with initial	0.005464
initial	0.000084
initial capital	0.333333
capital letters	0.333333
letters are	0.100000
are likely	0.016598
likely to	0.437500
be keyphrases	0.004219
<s> After	0.002306
After	0.000084
After training	0.333333
training a	0.035714
a learner	0.001227
learner	0.000056
learner ,	0.500000
can select	0.005525
select keyphrases	0.166667
keyphrases for	0.057143
for test	0.003610
test documents	0.200000
documents in	0.026316
following manner	0.066667
manner	0.000112
manner .	0.750000
We apply	0.142857
apply the	0.200000
same example-generation	0.040000
example-generation	0.000028
example-generation strategy	1.000000
strategy	0.000140
strategy to	0.600000
the test	0.001384
then run	0.028571
run each	0.200000
example through	0.012346
the learner	0.000692
learner .	0.500000
We can	0.285714
can determine	0.011050
the keyphrases	0.001384
keyphrases by	0.028571
by looking	0.005714
looking	0.000140
looking at	0.200000
at binary	0.014706
classification decisions	0.058824
decisions or	0.100000
or probabilities	0.004505
probabilities	0.000307
probabilities returned	0.090909
returned	0.000112
returned from	0.500000
from our	0.009615
our learned	0.200000
learned model	0.200000
model .	0.066667
<s> If	0.006149
If	0.000279
If probabilities	0.100000
probabilities are	0.090909
given ,	0.041667
a threshold	0.003681
threshold	0.000112
threshold is	0.250000
Keyphrase extractors	0.250000
extractors	0.000028
extractors are	1.000000
generally evaluated	0.090909
evaluated using	0.142857
using precision	0.016949
precision	0.000140
precision and	0.400000
and recall	0.002890
recall	0.000084
recall .	0.666667
<s> Precision	0.000769
Precision	0.000028
Precision measures	1.000000
measures	0.000167
measures how	0.333333
how many	0.103448
many of	0.038462
the proposed	0.001384
proposed keyphrases	0.222222
keyphrases are	0.057143
are actually	0.004149
actually	0.000084
actually correct	0.333333
correct .	0.200000
<s> Recall	0.002306
Recall	0.000084
Recall measures	0.333333
the true	0.000692
true	0.000056
true keyphrases	0.500000
keyphrases your	0.028571
your system	0.500000
system proposed	0.010753
proposed .	0.111111
The two	0.010417
two measures	0.034483
measures can	0.333333
be combined	0.004219
combined	0.000056
combined in	0.500000
an F-score	0.007576
F-score	0.000028
F-score ,	1.000000
the harmonic	0.000692
harmonic	0.000028
harmonic mean	1.000000
mean	0.000056
mean of	0.500000
two -LRB-	0.034483
-LRB- F	0.002710
F	0.000028
F =	1.000000
=	0.000251
= 2PR	0.111111
2PR	0.000028
2PR \/	1.000000
\/ -LRB-	0.333333
-LRB- P	0.002710
P	0.000056
P +	0.500000
+	0.000167
+ R	0.166667
R	0.000028
R -RRB-	1.000000
<s> Matches	0.000769
Matches	0.000028
Matches between	1.000000
keyphrases and	0.028571
be checked	0.004219
checked	0.000056
checked after	0.500000
after stemming	0.083333
stemming or	0.500000
or applying	0.004505
applying	0.000112
applying some	0.250000
text normalization	0.006289
<s> Design	0.002306
Design	0.000084
Design choices	1.000000
choices	0.000140
choices Designing	0.200000
Designing	0.000028
Designing a	1.000000
supervised keyphrase	0.125000
extraction system	0.064516
system involves	0.010753
involves deciding	0.100000
deciding	0.000167
deciding on	0.166667
on several	0.004717
several choices	0.045455
choices -LRB-	0.200000
-LRB- some	0.002710
these apply	0.023810
apply to	0.400000
to unsupervised	0.002656
unsupervised ,	0.125000
, too	0.000561
too	0.000167
too -RRB-	0.166667
: What	0.009804
What are	0.363636
are the	0.045643
examples ?	0.041667
first choice	0.030303
choice	0.000223
choice is	0.250000
is exactly	0.002033
exactly	0.000084
exactly how	0.333333
generate examples	0.055556
<s> Turney	0.000769
Turney and	0.222222
and others	0.002890
others have	0.083333
have used	0.019231
used all	0.008850
possible unigrams	0.041667
unigrams	0.000335
unigrams ,	0.250000
, bigrams	0.001123
bigrams	0.000056
bigrams ,	1.000000
and trigrams	0.002890
trigrams	0.000056
trigrams without	0.500000
without intervening	0.076923
intervening	0.000028
intervening punctuation	1.000000
punctuation and	0.285714
after removing	0.083333
removing	0.000056
removing stopwords	0.500000
stopwords	0.000028
stopwords .	1.000000
<s> Hulth	0.002306
Hulth	0.000084
Hulth showed	0.333333
showed	0.000112
showed that	0.750000
that you	0.003546
you can	0.153846
can get	0.005525
get some	0.142857
some improvement	0.012048
improvement	0.000112
improvement by	0.250000
selecting examples	0.200000
examples to	0.041667
be sequences	0.004219
of tokens	0.001783
tokens	0.000195
tokens that	0.142857
that match	0.003546
match	0.000167
match certain	0.166667
certain	0.000195
certain patterns	0.142857
patterns	0.000140
patterns of	0.200000
of part-of-speech	0.001783
part-of-speech tags	0.066667
tags	0.000167
tags .	0.333333
<s> Ideally	0.001537
Ideally	0.000056
Ideally ,	1.000000
the mechanism	0.000692
mechanism	0.000028
mechanism for	1.000000
for generating	0.003610
generating examples	0.200000
examples produces	0.041667
produces all	0.250000
known labeled	0.038462
labeled keyphrases	0.333333
keyphrases as	0.028571
as candidates	0.003484
candidates	0.000140
candidates ,	0.200000
, though	0.003369
though this	0.100000
case .	0.176471
, if	0.005615
if we	0.071429
we use	0.022222
use only	0.027778
only unigrams	0.026316
trigrams ,	0.500000
then we	0.057143
will never	0.028571
to extract	0.003984
extract	0.000112
extract a	0.250000
known keyphrase	0.038462
keyphrase containing	0.052632
containing four	0.125000
four words	0.142857
<s> Thus	0.009224
Thus	0.000335
Thus ,	0.916667
, recall	0.000561
recall may	0.333333
may suffer	0.019231
suffer	0.000028
suffer .	1.000000
, generating	0.000561
generating too	0.200000
too many	0.333333
many examples	0.019231
examples can	0.041667
also lead	0.014493
lead	0.000056
lead to	1.000000
to low	0.001328
low	0.000084
low precision	0.333333
precision .	0.200000
features ?	0.038462
We also	0.142857
also need	0.014493
create features	0.058824
features that	0.076923
that describe	0.003546
describe	0.000167
describe the	0.333333
are informative	0.004149
informative enough	0.500000
enough to	0.200000
to allow	0.003984
allow	0.000140
allow a	0.200000
algorithm to	0.071429
to discriminate	0.002656
discriminate keyphrases	0.333333
keyphrases from	0.028571
from non	0.009615
non	0.000028
non -	1.000000
- keyphrases	0.062500
<s> Typically	0.000769
Typically	0.000028
Typically features	1.000000
features involve	0.038462
involve various	0.166667
various term	0.055556
term frequencies	0.055556
frequencies	0.000056
frequencies -LRB-	0.500000
-LRB- how	0.008130
many times	0.019231
times	0.000140
times a	0.200000
a phrase	0.002454
phrase appears	0.100000
appears in	0.200000
the current	0.001384
current text	0.142857
or in	0.004505
larger corpus	0.125000
the length	0.001384
length of	0.250000
, relative	0.000561
relative position	0.333333
position	0.000112
position of	0.250000
first occurrence	0.030303
occurrence	0.000056
occurrence ,	0.500000
, various	0.000561
various boolean	0.055556
boolean	0.000028
boolean syntactic	1.000000
syntactic features	0.076923
features -LRB-	0.038462
, contains	0.000561
contains all	0.100000
all caps	0.023256
caps	0.000028
caps -RRB-	1.000000
The Turney	0.005208
paper used	0.090909
used about	0.008850
about 12	0.025000
12	0.000140
12 such	0.200000
such features	0.008130
Hulth uses	0.333333
uses a	0.285714
a reduced	0.001227
reduced set	0.250000
of features	0.000891
features ,	0.038462
which were	0.014493
were found	0.048780
found most	0.071429
most successful	0.034483
successful in	0.111111
the KEA	0.000692
KEA	0.000028
KEA -LRB-	1.000000
-LRB- Keyphrase	0.002710
Keyphrase Extraction	0.250000
Extraction Algorithm	0.333333
Algorithm	0.000028
Algorithm -RRB-	1.000000
-RRB- work	0.002710
work derived	0.041667
from Turney	0.009615
Turney 's	0.444444
's seminal	0.019608
seminal	0.000028
seminal paper	1.000000
paper .	0.090909
<s> How	0.003075
How	0.000195
How many	0.142857
many keyphrases	0.019231
keyphrases to	0.028571
to return	0.002656
return	0.000056
return ?	0.500000
the end	0.001384
end ,	0.125000
system will	0.010753
will need	0.028571
return a	0.500000
of keyphrases	0.002674
test document	0.100000
so we	0.033333
we need	0.133333
a way	0.006135
to limit	0.002656
limit the	0.500000
number .	0.046512
<s> Ensemble	0.000769
Ensemble	0.000028
Ensemble methods	1.000000
i.e. ,	0.368421
, using	0.005615
using votes	0.016949
votes	0.000028
votes from	1.000000
from several	0.009615
several classifiers	0.045455
classifiers -RRB-	0.500000
-RRB- have	0.005420
produce numeric	0.045455
numeric	0.000028
numeric scores	1.000000
scores	0.000140
scores that	0.200000
be thresholded	0.004219
thresholded	0.000028
thresholded to	1.000000
a user-provided	0.001227
user-provided	0.000028
user-provided number	1.000000
the technique	0.000692
technique used	0.142857
used by	0.079646
by Turney	0.005714
Turney with	0.111111
with C4	0.005464
C4	0.000028
C4 .5	1.000000
.5	0.000028
.5 decision	1.000000
trees .	0.333333
Hulth used	0.333333
used a	0.026549
single binary	0.071429
binary classifier	0.250000
classifier so	0.142857
the learning	0.000692
algorithm implicitly	0.035714
implicitly	0.000028
implicitly determines	1.000000
determines	0.000084
determines the	0.666667
the appropriate	0.002076
appropriate	0.000112
appropriate number	0.250000
What learning	0.090909
algorithm ?	0.035714
<s> Once	0.003843
Once	0.000140
Once examples	0.200000
and features	0.001445
features are	0.115385
are created	0.012448
created ,	0.285714
learn to	0.076923
predict keyphrases	0.166667
<s> Virtually	0.000769
Virtually	0.000028
Virtually any	1.000000
any supervised	0.032258
algorithm could	0.035714
could be	0.250000
used ,	0.070796
, Naive	0.000561
Naive	0.000028
Naive Bayes	1.000000
Bayes	0.000084
Bayes ,	0.333333
and rule	0.001445
rule induction	0.333333
induction	0.000056
induction .	1.000000
of Turney	0.001783
's GenEx	0.019608
GenEx	0.000028
GenEx algorithm	1.000000
algorithm ,	0.107143
a genetic	0.001227
genetic	0.000056
genetic algorithm	1.000000
learn parameters	0.076923
parameters for	0.500000
a domain-specific	0.001227
domain-specific	0.000056
domain-specific keyphrase	0.500000
extraction algorithm	0.032258
algorithm .	0.142857
The extractor	0.005208
extractor follows	0.500000
follows a	0.500000
of heuristics	0.000891
heuristics	0.000056
heuristics to	0.500000
to identify	0.006640
identify keyphrases	0.083333
The genetic	0.005208
algorithm optimizes	0.035714
optimizes	0.000028
optimizes parameters	1.000000
for these	0.003610
these heuristics	0.023810
heuristics with	0.500000
to performance	0.001328
performance on	0.055556
on training	0.004717
documents with	0.052632
with known	0.010929
known key	0.038462
key phrases	0.166667
phrases .	0.312500
<s> Unsupervised	0.003843
Unsupervised keyphrase	0.333333
: TextRank	0.019608
TextRank	0.000391
TextRank While	0.071429
While	0.000140
While supervised	0.200000
supervised methods	0.125000
methods have	0.045455
have some	0.009615
some nice	0.012048
nice	0.000112
nice properties	0.250000
properties	0.000112
properties ,	0.250000
, like	0.001684
like being	0.035714
being able	0.055556
produce interpretable	0.045455
interpretable	0.000028
interpretable rules	1.000000
for what	0.007220
what features	0.031250
features characterize	0.038462
characterize	0.000056
characterize a	0.500000
keyphrase ,	0.052632
they also	0.025000
also require	0.014493
large amount	0.043478
Many documents	0.083333
are needed	0.004149
, training	0.000561
training on	0.035714
specific domain	0.142857
domain tends	0.050000
tends	0.000028
tends to	1.000000
to customize	0.002656
customize	0.000056
customize the	0.500000
extraction process	0.032258
to that	0.002656
that domain	0.003546
domain ,	0.150000
the resulting	0.001384
resulting classifier	0.250000
classifier is	0.142857
not necessarily	0.017857
necessarily	0.000056
necessarily portable	0.500000
portable	0.000084
portable ,	0.333333
as some	0.003484
's results	0.019608
results demonstrate	0.047619
demonstrate	0.000028
demonstrate .	1.000000
extraction removes	0.032258
removes	0.000028
removes the	1.000000
the need	0.002076
need for	0.142857
It approaches	0.026316
approaches the	0.035714
problem from	0.022727
a different	0.003681
different angle	0.020408
angle	0.000028
angle .	1.000000
<s> Instead	0.001537
Instead	0.000084
Instead of	1.000000
of trying	0.000891
trying	0.000140
trying to	1.000000
learn explicit	0.076923
explicit features	0.200000
that characterize	0.003546
characterize keyphrases	0.500000
the TextRank	0.001384
TextRank algorithm	0.071429
algorithm exploits	0.035714
exploits	0.000028
exploits the	1.000000
text itself	0.006289
itself to	0.200000
determine keyphrases	0.043478
that appear	0.014184
appear ``	0.062500
`` central	0.010582
central	0.000084
central ''	0.666667
'' to	0.015464
same way	0.040000
way that	0.125000
that PageRank	0.003546
PageRank	0.000167
PageRank selects	0.166667
selects	0.000056
selects important	0.500000
important Web	0.062500
Web pages	0.222222
pages .	0.285714
Recall this	0.333333
the notion	0.001384
notion	0.000112
notion of	0.750000
`` prestige	0.005291
prestige	0.000028
prestige ''	1.000000
`` recommendation	0.010582
recommendation	0.000056
recommendation ''	1.000000
'' from	0.005155
from social	0.009615
social networks	0.214286
networks	0.000391
networks .	0.214286
this way	0.021978
way ,	0.041667
, TextRank	0.001123
TextRank does	0.071429
does not	0.500000
not rely	0.017857
rely on	0.857143
on any	0.018868
any previous	0.032258
previous	0.000084
previous training	0.333333
data at	0.012987
but rather	0.029412
rather can	0.062500
be run	0.004219
run on	0.200000
any arbitrary	0.064516
arbitrary	0.000084
arbitrary piece	0.333333
and it	0.007225
it can	0.051282
can produce	0.005525
output simply	0.038462
simply based	0.083333
text 's	0.006289
's intrinsic	0.019608
intrinsic properties	0.250000
properties .	0.250000
Thus the	0.083333
the algorithm	0.000692
is easily	0.002033
easily	0.000251
easily portable	0.222222
portable to	0.333333
new domains	0.041667
domains and	0.250000
and languages	0.001445
<s> TextRank	0.002306
TextRank is	0.071429
general purpose	0.090909
purpose graph-based	0.400000
graph-based	0.000056
graph-based ranking	1.000000
ranking	0.000195
ranking algorithm	0.285714
algorithm for	0.071429
for NLP	0.003610
<s> Essentially	0.000769
Essentially	0.000028
Essentially ,	1.000000
it runs	0.008547
runs	0.000028
runs PageRank	1.000000
PageRank on	0.166667
a graph	0.003681
graph	0.000363
graph specially	0.076923
specially	0.000028
specially designed	1.000000
designed for	0.142857
particular NLP	0.076923
NLP task	0.021277
For keyphrase	0.016393
it builds	0.008547
builds	0.000056
builds a	0.500000
graph using	0.076923
using some	0.033898
some set	0.012048
units as	0.142857
as vertices	0.003484
vertices	0.000251
vertices .	0.222222
<s> Edges	0.001537
Edges	0.000056
Edges are	1.000000
are based	0.020747
some measure	0.012048
measure of	0.181818
semantic or	0.047619
or lexical	0.009009
lexical similarity	0.076923
similarity	0.000279
similarity between	0.200000
text unit	0.006289
unit	0.000084
unit vertices	0.333333
<s> Unlike	0.000769
Unlike	0.000028
Unlike PageRank	1.000000
PageRank ,	0.166667
the edges	0.001384
edges	0.000195
edges are	0.142857
typically undirected	0.055556
undirected	0.000028
undirected and	1.000000
be weighted	0.004219
weighted	0.000084
weighted to	0.333333
to reflect	0.001328
reflect	0.000028
reflect a	1.000000
a degree	0.001227
of similarity	0.000891
similarity .	0.100000
Once the	0.200000
the graph	0.004152
graph is	0.230769
is constructed	0.004065
constructed	0.000056
constructed ,	0.500000
form a	0.150000
a stochastic	0.001227
stochastic matrix	0.125000
matrix	0.000028
matrix ,	1.000000
, combined	0.000561
combined with	0.500000
a damping	0.001227
damping	0.000028
damping factor	1.000000
factor	0.000056
factor -LRB-	0.500000
as in	0.027875
`` random	0.005291
random	0.000195
random surfer	0.142857
surfer	0.000028
surfer model	1.000000
model ''	0.033333
the ranking	0.001384
ranking over	0.142857
over vertices	0.083333
vertices is	0.111111
is obtained	0.002033
by finding	0.005714
finding the	0.400000
the eigenvector	0.000692
eigenvector	0.000056
eigenvector corresponding	0.500000
corresponding to	0.333333
to eigenvalue	0.001328
eigenvalue	0.000028
eigenvalue 1	1.000000
1	0.000112
1 -LRB-	0.250000
the stationary	0.000692
stationary	0.000195
stationary distribution	0.285714
distribution	0.000112
distribution of	0.250000
the random	0.000692
random walk	0.571429
walk	0.000140
walk on	0.400000
graph -RRB-	0.076923
choices What	0.400000
What should	0.090909
should vertices	0.052632
vertices be	0.111111
be ?	0.004219
The vertices	0.005208
vertices should	0.111111
should correspond	0.052632
correspond	0.000056
correspond to	1.000000
we want	0.044444
want	0.000167
want to	0.833333
to rank	0.003984
rank	0.000167
rank .	0.166667
<s> Potentially	0.000769
Potentially	0.000028
Potentially ,	1.000000
we could	0.022222
could do	0.062500
do something	0.038462
something	0.000028
something similar	1.000000
the supervised	0.000692
methods and	0.022727
and create	0.002890
a vertex	0.002454
vertex	0.000084
vertex for	0.666667
, trigram	0.001123
trigram ,	0.666667
to keep	0.002656
keep	0.000084
keep the	0.333333
graph small	0.076923
small ,	0.222222
the authors	0.000692
authors decide	0.200000
decide to	0.250000
rank individual	0.166667
individual unigrams	0.083333
unigrams in	0.250000
a first	0.002454
step ,	0.133333
then include	0.028571
include a	0.111111
a second	0.003681
step that	0.133333
that merges	0.003546
merges	0.000028
merges highly	1.000000
highly ranked	0.111111
ranked	0.000140
ranked adjacent	0.200000
adjacent	0.000167
adjacent unigrams	0.166667
unigrams to	0.083333
form multi-word	0.050000
multi-word	0.000028
multi-word phrases	1.000000
This has	0.015873
a nice	0.002454
nice side	0.250000
side	0.000028
side effect	1.000000
effect	0.000056
effect of	0.500000
of allowing	0.000891
allowing	0.000084
allowing us	0.333333
us	0.000056
us to	0.500000
produce keyphrases	0.045455
keyphrases of	0.028571
of arbitrary	0.000891
arbitrary length	0.333333
length .	0.125000
we rank	0.022222
rank unigrams	0.166667
unigrams and	0.083333
and find	0.001445
find that	0.076923
`` advanced	0.005291
advanced	0.000140
advanced ''	0.200000
`` natural	0.015873
natural ''	0.026667
`` language	0.010582
language ''	0.013514
`` processing	0.010582
processing ''	0.037037
'' all	0.005155
all get	0.023256
get high	0.142857
high ranks	0.055556
ranks	0.000056
ranks ,	0.500000
we would	0.066667
would look	0.037736
look	0.000140
look at	0.400000
and see	0.001445
see that	0.050000
these words	0.047619
words appear	0.009174
appear consecutively	0.062500
consecutively	0.000028
consecutively and	1.000000
a final	0.001227
final	0.000251
final keyphrase	0.111111
keyphrase using	0.052632
using all	0.016949
all four	0.023256
four together	0.142857
together	0.000223
together .	0.125000
the unigrams	0.001384
unigrams placed	0.083333
placed	0.000084
placed in	1.000000
graph can	0.076923
be filtered	0.012658
filtered	0.000084
filtered by	0.333333
by part	0.005714
authors found	0.200000
that adjectives	0.003546
adjectives and	0.333333
and nouns	0.001445
nouns were	0.111111
were the	0.024390
best to	0.111111
to include	0.009296
include .	0.037037
some linguistic	0.012048
linguistic knowledge	0.062500
knowledge comes	0.037037
comes into	0.200000
into play	0.012821
play	0.000028
play in	1.000000
this step	0.010989
How should	0.142857
should we	0.052632
we create	0.044444
create edges	0.058824
edges ?	0.285714
created based	0.142857
on word	0.004717
word co-occurrence	0.016667
co-occurrence	0.000084
co-occurrence in	0.333333
this application	0.010989
application of	0.285714
of TextRank	0.000891
TextRank .	0.071429
Two vertices	0.142857
vertices are	0.111111
are connected	0.004149
connected by	0.200000
by an	0.011429
an edge	0.015152
edge	0.000084
edge if	0.333333
unigrams appear	0.083333
appear within	0.062500
within a	0.277778
a window	0.001227
window	0.000056
window of	1.000000
of size	0.000891
size N	0.166667
N	0.000084
N in	0.333333
<s> N	0.000769
N is	0.333333
typically around	0.055556
around	0.000223
around 2	0.125000
2	0.000140
2 --	0.200000
-- 10	0.040000
10 .	0.125000
'' and	0.067010
'' might	0.010309
be linked	0.008439
linked	0.000084
linked in	0.333333
text about	0.012579
about NLP	0.025000
<s> ``	0.003843
`` Natural	0.005291
Natural ''	0.076923
would also	0.018868
linked because	0.333333
because they	0.133333
they would	0.025000
would both	0.018868
both appear	0.032258
same string	0.040000
string	0.000112
string of	1.000000
of N	0.000891
N words	0.333333
These edges	0.058824
edges build	0.142857
build on	0.333333
`` text	0.005291
text cohesion	0.006289
cohesion	0.000028
cohesion ''	1.000000
the idea	0.001384
idea that	0.285714
words that	0.009174
appear near	0.062500
near	0.000028
near each	1.000000
other are	0.014286
likely related	0.062500
related in	0.066667
a meaningful	0.002454
meaningful way	0.125000
way and	0.041667
`` recommend	0.010582
recommend	0.000056
recommend ''	1.000000
'' each	0.005155
other to	0.014286
the reader	0.002768
reader	0.000279
reader .	0.200000
How are	0.285714
the final	0.002768
final keyphrases	0.222222
keyphrases formed	0.028571
formed	0.000140
formed ?	0.400000
Since this	0.200000
this method	0.010989
method simply	0.062500
simply ranks	0.083333
ranks the	0.500000
the individual	0.001384
individual vertices	0.083333
vertices ,	0.111111
to threshold	0.001328
threshold or	0.500000
or produce	0.004505
a limited	0.002454
The technique	0.005208
technique chosen	0.142857
chosen	0.000140
chosen is	0.200000
to set	0.003984
set a	0.051282
a count	0.001227
count	0.000140
count T	0.200000
T	0.000167
T to	0.166667
a user-specified	0.001227
user-specified	0.000056
user-specified fraction	0.500000
fraction	0.000028
fraction of	1.000000
the total	0.000692
total	0.000056
total number	0.500000
of vertices	0.000891
vertices in	0.111111
graph .	0.153846
Then the	0.400000
the top	0.002768
top	0.000140
top T	0.400000
T vertices\/unigrams	0.166667
vertices\/unigrams	0.000028
vertices\/unigrams are	1.000000
are selected	0.004149
selected	0.000056
selected based	0.500000
on their	0.009434
their stationary	0.029412
stationary probabilities	0.142857
probabilities .	0.181818
A post	0.020000
post	0.000028
post -	1.000000
- processing	0.062500
processing step	0.018519
step is	0.066667
to merge	0.001328
merge	0.000028
merge adjacent	1.000000
adjacent instances	0.166667
instances	0.000084
instances of	0.666667
these T	0.023810
T unigrams	0.333333
unigrams .	0.166667
, potentially	0.000561
potentially	0.000084
potentially more	0.333333
more or	0.031579
less than	0.250000
than T	0.022222
T final	0.166667
keyphrases will	0.028571
be produced	0.004219
produced ,	0.111111
number should	0.023256
be roughly	0.004219
roughly	0.000084
roughly proportional	0.333333
proportional	0.000028
proportional to	1.000000
<s> Why	0.001537
Why it	0.142857
it works	0.008547
works	0.000056
works It	0.500000
not initially	0.008929
initially	0.000028
initially clear	1.000000
clear	0.000112
clear why	0.250000
why applying	0.142857
applying PageRank	0.500000
PageRank to	0.333333
a co-occurrence	0.001227
co-occurrence graph	0.666667
graph would	0.076923
produce useful	0.045455
useful keyphrases	0.071429
One way	0.076923
to think	0.001328
think	0.000084
think about	0.333333
about it	0.025000
A word	0.020000
word that	0.033333
that appears	0.003546
appears multiple	0.200000
multiple times	0.076923
times throughout	0.200000
throughout	0.000028
throughout a	1.000000
text may	0.006289
may have	0.038462
different co-occurring	0.020408
co-occurring	0.000028
co-occurring neighbors	1.000000
neighbors .	0.333333
about machine	0.025000
the unigram	0.000692
unigram ``	0.200000
`` learning	0.021164
learning ''	0.116279
might co-occur	0.038462
co-occur	0.000056
co-occur with	0.500000
`` machine	0.005291
machine ''	0.012658
, supervised	0.000561
supervised ''	0.187500
`` un-supervised	0.005291
un-supervised	0.000028
un-supervised ''	1.000000
`` semi-supervised	0.005291
semi-supervised ''	0.500000
different sentences	0.061224
'' vertex	0.005155
vertex would	0.333333
a central	0.001227
central ``	0.333333
`` hub	0.005291
hub	0.000028
hub ''	1.000000
that connects	0.003546
connects	0.000028
connects to	1.000000
these other	0.023810
other modifying	0.014286
modifying	0.000028
modifying words	1.000000
<s> Running	0.000769
Running	0.000028
Running PageRank\/TextRank	1.000000
PageRank\/TextRank	0.000028
PageRank\/TextRank on	1.000000
is likely	0.006098
rank ``	0.166667
'' highly	0.005155
highly .	0.111111
<s> Similarly	0.000769
Similarly	0.000028
Similarly ,	1.000000
text contains	0.006289
phrase ``	0.100000
`` supervised	0.026455
supervised classification	0.125000
classification ''	0.294118
then there	0.028571
there would	0.025000
be an	0.004219
edge between	0.333333
between ``	0.025641
`` classification	0.015873
If ``	0.100000
'' appears	0.005155
appears several	0.200000
several other	0.045455
other places	0.014286
places and	0.500000
thus has	0.100000
has many	0.023810
many neighbors	0.019231
neighbors ,	0.333333
is importance	0.002033
importance	0.000167
importance would	0.166667
would contribute	0.018868
contribute	0.000028
contribute to	1.000000
the importance	0.001384
importance of	0.500000
If it	0.100000
it ends	0.017094
ends	0.000056
ends up	0.500000
up with	0.136364
a high	0.003681
high rank	0.055556
rank ,	0.166667
it will	0.017094
be selected	0.004219
selected as	0.500000
as one	0.006969
, along	0.000561
along	0.000056
along with	1.000000
and probably	0.001445
probably	0.000112
probably ``	0.250000
final post-processing	0.111111
post-processing	0.000084
post-processing step	0.666667
would then	0.018868
then end	0.028571
end up	0.250000
with keyphrases	0.005464
keyphrases ``	0.028571
In short	0.009524
short ,	0.125000
the co-occurrence	0.000692
graph will	0.153846
will contain	0.028571
contain densely	0.083333
densely	0.000056
densely connected	1.000000
connected regions	0.200000
regions	0.000056
regions for	0.500000
for terms	0.003610
terms that	0.076923
appear often	0.062500
often and	0.022727
different contexts	0.020408
contexts .	0.285714
A random	0.020000
on this	0.014151
this graph	0.010989
will have	0.057143
a stationary	0.002454
distribution that	0.500000
that assigns	0.003546
assigns	0.000028
assigns large	1.000000
large probabilities	0.043478
probabilities to	0.090909
the terms	0.000692
terms in	0.076923
the centers	0.000692
centers	0.000028
centers of	1.000000
the clusters	0.000692
clusters	0.000028
clusters .	1.000000
is similar	0.004065
to densely	0.001328
connected Web	0.200000
pages getting	0.142857
getting ranked	0.250000
ranked highly	0.400000
highly by	0.111111
by PageRank	0.005714
PageRank .	0.166667
<s> Document	0.001537
Document	0.000112
Document summarization	0.250000
summarization Like	0.020000
Like	0.000056
Like keyphrase	0.500000
, document	0.000561
summarization hopes	0.020000
hopes	0.000028
hopes to	1.000000
The only	0.010417
only real	0.026316
real difference	0.111111
difference is	0.250000
that now	0.003546
now we	0.076923
are dealing	0.004149
dealing	0.000056
dealing with	1.000000
with larger	0.005464
larger text	0.062500
units --	0.142857
-- whole	0.040000
sentences instead	0.013158
instead of	0.571429
and phrases	0.002890
<s> While	0.002306
While some	0.200000
some work	0.012048
work has	0.083333
been done	0.029412
done in	0.454545
in abstractive	0.001873
abstractive summarization	0.333333
summarization -LRB-	0.040000
-LRB- creating	0.002710
an abstract	0.007576
abstract	0.000028
abstract synopsis	1.000000
synopsis	0.000028
synopsis like	1.000000
like that	0.035714
the majority	0.000692
majority	0.000028
majority of	1.000000
are extractive	0.004149
extractive -LRB-	0.142857
-LRB- selecting	0.002710
to place	0.002656
place	0.000112
place in	0.500000
summary -RRB-	0.023810
<s> Before	0.000769
Before	0.000056
Before getting	0.500000
getting into	0.250000
into the	0.102564
the details	0.000692
details	0.000056
details of	0.500000
some summarization	0.012048
summarization methods	0.020000
will mention	0.028571
mention	0.000084
mention how	0.333333
how summarization	0.034483
typically evaluated	0.055556
evaluated .	0.142857
The most	0.026042
common way	0.080000
way is	0.041667
is using	0.004065
using the	0.101695
the so-called	0.000692
so-called	0.000084
so-called ROUGE	0.333333
ROUGE	0.000140
ROUGE -LRB-	0.200000
-LRB- Recall-Oriented	0.005420
Recall-Oriented	0.000056
Recall-Oriented Understudy	1.000000
Understudy	0.000056
Understudy for	1.000000
for Gisting	0.007220
Gisting	0.000056
Gisting Evaluation	1.000000
Evaluation -RRB-	0.222222
-RRB- measure	0.002710
measure -LRB-	0.090909
-LRB- http:\/\/haydn.isi.edu\/ROUGE\/	0.002710
http:\/\/haydn.isi.edu\/ROUGE\/	0.000028
http:\/\/haydn.isi.edu\/ROUGE\/ -RRB-	1.000000
a recall-based	0.001227
recall-based	0.000056
recall-based measure	0.500000
measure that	0.090909
that determines	0.007092
determines how	0.333333
how well	0.206897
well a	0.035714
a system-generated	0.001227
system-generated	0.000056
system-generated summary	0.500000
summary covers	0.023810
covers the	0.500000
content present	0.083333
more human-generated	0.010526
human-generated	0.000056
human-generated model	0.500000
model summaries	0.066667
summaries known	0.023256
as references	0.003484
references .	0.250000
is recall-based	0.002033
recall-based to	0.500000
to encourage	0.001328
encourage	0.000028
encourage systems	1.000000
systems to	0.008929
include all	0.037037
the important	0.000692
important topics	0.062500
topics in	0.142857
Recall can	0.333333
be computed	0.004219
computed	0.000056
computed with	0.500000
to unigram	0.001328
or 4-gram	0.004505
4-gram	0.000028
4-gram matching	1.000000
matching ,	0.200000
though ROUGE-1	0.100000
ROUGE-1	0.000140
ROUGE-1 -LRB-	0.200000
-LRB- unigram	0.002710
unigram matching	0.200000
matching -RRB-	0.200000
-RRB- has	0.008130
been shown	0.014706
shown to	0.400000
to correlate	0.001328
correlate	0.000084
correlate best	0.333333
best with	0.055556
with human	0.010929
human assessments	0.021739
assessments	0.000028
assessments of	1.000000
of system-generated	0.000891
system-generated summaries	0.500000
the summaries	0.002768
summaries with	0.046512
with highest	0.010929
highest	0.000084
highest ROUGE-1	0.333333
ROUGE-1 values	0.200000
values correlate	0.125000
correlate with	0.666667
summaries humans	0.023256
humans deemed	0.083333
deemed the	0.500000
best -RRB-	0.055556
<s> ROUGE-1	0.000769
ROUGE-1 is	0.200000
is computed	0.002033
computed as	0.500000
as division	0.003484
division	0.000056
division of	0.500000
of count	0.000891
count of	0.400000
of unigrams	0.001783
in reference	0.003745
reference that	0.125000
in system	0.001873
and count	0.001445
reference summary	0.375000
If there	0.100000
are multiple	0.004149
multiple references	0.076923
references ,	0.250000
the ROUGE-1	0.000692
ROUGE-1 scores	0.200000
scores are	0.200000
are averaged	0.004149
averaged	0.000028
averaged .	1.000000
<s> Because	0.001537
Because	0.000056
Because ROUGE	0.500000
ROUGE is	0.400000
based only	0.018519
on content	0.004717
content overlap	0.166667
overlap	0.000112
overlap ,	0.250000
determine if	0.217391
same general	0.040000
general concepts	0.045455
concepts are	0.400000
are discussed	0.004149
discussed between	0.142857
between an	0.025641
an automatic	0.015152
automatic summary	0.086957
summary and	0.047619
a reference	0.002454
not determine	0.008929
result is	0.181818
is coherent	0.002033
coherent or	0.200000
sentences flow	0.013158
flow	0.000028
flow together	1.000000
together in	0.125000
a sensible	0.001227
sensible	0.000028
sensible manner	1.000000
<s> High-order	0.000769
High-order	0.000028
High-order n-gram	1.000000
n-gram	0.000056
n-gram ROUGE	0.500000
ROUGE measures	0.200000
measures try	0.166667
try	0.000084
try to	1.000000
to judge	0.002656
judge fluency	0.250000
fluency	0.000028
fluency to	1.000000
some degree	0.024096
degree .	0.166667
that ROUGE	0.003546
the BLEU	0.000692
BLEU	0.000084
BLEU measure	0.333333
measure for	0.090909
but BLEU	0.014706
BLEU is	0.333333
is precision	0.002033
precision -	0.200000
- based	0.187500
because translation	0.033333
systems favor	0.008929
favor accuracy	0.500000
A promising	0.020000
promising	0.000028
promising line	1.000000
line	0.000084
line in	0.333333
in document	0.003745
is adaptive	0.002033
adaptive	0.000084
adaptive document\/text	0.333333
document\/text	0.000056
document\/text summarization	0.500000
of adaptive	0.000891
adaptive summarization	0.666667
summarization involves	0.020000
involves preliminary	0.100000
preliminary	0.000084
preliminary recognition	0.333333
of document\/text	0.000891
document\/text genre	0.500000
genre	0.000056
genre and	0.500000
and subsequent	0.001445
subsequent	0.000056
subsequent application	0.500000
summarization algorithms	0.020000
algorithms optimized	0.028571
optimized	0.000028
optimized for	1.000000
this genre	0.010989
genre .	0.500000
<s> First	0.000769
First	0.000028
First summarizes	1.000000
summarizes	0.000028
summarizes that	1.000000
that perform	0.003546
perform adaptive	0.090909
summarization have	0.020000
been created	0.029412
created .	0.142857
<s> Overview	0.001537
Overview	0.000056
Overview of	1.000000
of supervised	0.000891
learning approaches	0.023256
approaches Supervised	0.035714
Supervised	0.000028
Supervised text	1.000000
text summarization	0.006289
very much	0.024390
much like	0.045455
like supervised	0.035714
and we	0.001445
will not	0.114286
not spend	0.008929
spend	0.000028
spend much	1.000000
much time	0.045455
time on	0.030303
on it	0.004717
<s> Basically	0.000769
Basically	0.000028
Basically ,	1.000000
if you	0.071429
you have	0.153846
a collection	0.002454
collection	0.000140
collection of	0.400000
documents and	0.026316
and human-generated	0.001445
human-generated summaries	0.500000
summaries for	0.023256
for them	0.007220
them ,	0.210526
, you	0.000561
can learn	0.005525
learn features	0.076923
make them	0.050000
them good	0.052632
good candidates	0.230769
candidates for	0.200000
for inclusion	0.003610
inclusion	0.000028
inclusion in	1.000000
<s> Features	0.000769
Features	0.000028
Features might	1.000000
might include	0.038462
include the	0.185185
the position	0.000692
position in	0.500000
the document	0.004152
document -LRB-	0.055556
first few	0.030303
few	0.000028
few sentences	1.000000
are probably	0.004149
probably important	0.250000
important -RRB-	0.062500
The main	0.015625
main difficulty	0.250000
difficulty in	0.285714
in supervised	0.001873
supervised extractive	0.062500
extractive summarization	0.428571
known summaries	0.038462
summaries must	0.023256
must be	0.428571
be manually	0.004219
manually created	0.250000
created by	0.285714
by extracting	0.005714
extracting sentences	0.200000
sentences so	0.013158
an original	0.007576
original training	0.076923
training document	0.035714
document can	0.027778
be labeled	0.004219
labeled as	0.333333
`` in	0.010582
in summary	0.003745
summary ''	0.047619
`` not	0.005291
not in	0.008929
not typically	0.017857
typically how	0.055556
how people	0.034483
people create	0.062500
create summaries	0.058824
summaries ,	0.069767
so simply	0.033333
simply using	0.083333
using journal	0.016949
journal abstracts	0.333333
abstracts or	0.500000
or existing	0.004505
existing summaries	0.200000
is usually	0.016260
usually not	0.031250
not sufficient	0.008929
sufficient	0.000140
sufficient .	0.600000
The sentences	0.005208
in these	0.005618
these summaries	0.047619
summaries do	0.023256
necessarily match	0.500000
match up	0.166667
with sentences	0.005464
so it	0.066667
would difficult	0.018868
to assign	0.003984
assign labels	0.200000
to examples	0.001328
examples for	0.041667
training .	0.035714
Note ,	0.111111
these natural	0.023810
natural summaries	0.013333
summaries can	0.046512
can still	0.005525
still be	0.066667
for evaluation	0.003610
evaluation purposes	0.018519
purposes ,	0.250000
since ROUGE-1	0.100000
ROUGE-1 only	0.200000
only cares	0.026316
cares	0.000028
cares about	1.000000
about unigrams	0.025000
Unsupervised approaches	0.166667
TextRank and	0.142857
and LexRank	0.004335
LexRank	0.000335
LexRank The	0.083333
The unsupervised	0.005208
unsupervised approach	0.125000
to summarization	0.002656
also quite	0.014493
quite similar	0.125000
similar in	0.037037
in spirit	0.001873
spirit	0.000028
spirit to	1.000000
unsupervised keyphrase	0.125000
and gets	0.001445
gets	0.000056
gets around	0.500000
around the	0.375000
issue of	0.125000
of costly	0.000891
costly	0.000028
costly training	1.000000
Some unsupervised	0.047619
unsupervised summarization	0.250000
summarization approaches	0.020000
approaches are	0.035714
on finding	0.004717
`` centroid	0.005291
centroid	0.000056
centroid ''	0.500000
'' sentence	0.005155
the mean	0.000692
mean word	0.500000
word vector	0.016667
vector	0.000084
vector of	0.333333
sentences can	0.039474
be ranked	0.004219
ranked with	0.200000
with regard	0.021858
regard	0.000140
regard to	0.800000
their similarity	0.029412
similarity to	0.100000
this centroid	0.010989
centroid sentence	0.500000
A more	0.020000
more principled	0.010526
principled	0.000028
principled way	1.000000
estimate sentence	0.250000
sentence importance	0.020833
importance is	0.166667
using random	0.016949
random walks	0.285714
walks	0.000056
walks and	0.500000
and eigenvector	0.001445
eigenvector centrality	0.500000
centrality	0.000056
centrality .	0.500000
<s> LexRank	0.001537
LexRank is	0.083333
algorithm essentially	0.035714
essentially identical	0.125000
to TextRank	0.001328
TextRank ,	0.142857
and both	0.001445
both use	0.032258
use this	0.027778
approach for	0.028571
for document	0.003610
two methods	0.034483
methods were	0.045455
developed by	0.038462
by different	0.005714
different groups	0.020408
groups	0.000140
groups at	0.200000
same time	0.120000
LexRank simply	0.083333
simply focused	0.083333
on summarization	0.004717
but could	0.014706
could just	0.062500
just	0.000251
just as	0.222222
as easily	0.006969
easily be	0.111111
for keyphrase	0.003610
extraction or	0.032258
any other	0.064516
other NLP	0.014286
NLP ranking	0.021277
ranking task	0.142857
the vertices	0.000692
vertices ?	0.111111
In both	0.019048
both LexRank	0.032258
LexRank and	0.083333
and TextRank	0.001445
constructed by	0.500000
by creating	0.005714
creating a	0.285714
each sentence	0.022222
sentence in	0.041667
The edges	0.005208
edges between	0.142857
some form	0.048193
semantic similarity	0.047619
similarity or	0.100000
or content	0.004505
overlap .	0.250000
While LexRank	0.200000
LexRank uses	0.083333
uses cosine	0.071429
cosine	0.000084
cosine similarity	0.333333
similarity of	0.100000
of TF-IDF	0.000891
TF-IDF	0.000028
TF-IDF vectors	1.000000
vectors	0.000084
vectors ,	0.333333
TextRank uses	0.142857
very similar	0.097561
similar measure	0.037037
measure based	0.090909
words two	0.009174
two sentences	0.068966
have in	0.019231
common -LRB-	0.040000
-LRB- normalized	0.002710
normalized	0.000028
normalized by	1.000000
sentences '	0.013158
' lengths	0.052632
lengths	0.000028
lengths -RRB-	1.000000
The LexRank	0.005208
LexRank paper	0.083333
paper explored	0.090909
explored	0.000056
explored using	0.500000
using unweighted	0.016949
unweighted	0.000028
unweighted edges	1.000000
edges after	0.142857
after applying	0.083333
applying a	0.250000
threshold to	0.250000
the cosine	0.000692
cosine values	0.333333
values ,	0.125000
also experimented	0.014493
experimented	0.000028
experimented with	1.000000
with using	0.005464
using edges	0.016949
edges with	0.142857
with weights	0.005464
weights equal	0.200000
equal	0.000028
equal to	1.000000
the similarity	0.000692
similarity score	0.100000
score	0.000167
score .	0.500000
uses continuous	0.071429
continuous similarity	0.166667
similarity scores	0.100000
scores as	0.200000
as weights	0.003484
weights .	0.400000
are summaries	0.004149
summaries formed	0.023256
both algorithms	0.032258
are ranked	0.004149
ranked by	0.200000
by applying	0.005714
resulting graph	0.250000
A summary	0.020000
summary is	0.047619
is formed	0.002033
formed by	0.200000
by combining	0.005714
combining	0.000112
combining the	0.250000
top ranking	0.200000
ranking sentences	0.142857
or length	0.004505
length cutoff	0.125000
cutoff	0.000028
cutoff to	1.000000
the size	0.001384
size of	0.166667
LexRank differences	0.083333
differences It	0.333333
is worth	0.004065
worth	0.000056
worth noting	0.500000
noting	0.000028
noting that	1.000000
that TextRank	0.007092
TextRank was	0.142857
was applied	0.012987
summarization exactly	0.020000
exactly as	0.333333
as described	0.006969
described here	0.166667
here	0.000056
here ,	0.500000
while LexRank	0.100000
LexRank was	0.083333
was used	0.051948
as part	0.006969
larger summarization	0.062500
summarization system	0.060000
system -LRB-	0.021505
-LRB- MEAD	0.002710
MEAD	0.000028
MEAD -RRB-	1.000000
that combines	0.003546
combines	0.000028
combines the	1.000000
the LexRank	0.000692
LexRank score	0.083333
score -LRB-	0.166667
-LRB- stationary	0.002710
stationary probability	0.142857
probability -RRB-	0.285714
-RRB- with	0.008130
with other	0.005464
other features	0.014286
features like	0.038462
like sentence	0.035714
sentence position	0.041667
position and	0.250000
and length	0.001445
length using	0.125000
a linear	0.002454
linear combination	0.142857
combination with	0.400000
with either	0.005464
either user-specified	0.100000
user-specified or	0.500000
or automatically	0.009009
automatically tuned	0.047619
tuned	0.000028
tuned weights	1.000000
this case	0.010989
case ,	0.176471
some training	0.012048
documents might	0.026316
be needed	0.004219
needed ,	0.047619
though the	0.100000
TextRank results	0.071429
results show	0.047619
show	0.000028
show the	1.000000
the additional	0.000692
additional features	0.166667
are not	0.020747
not absolutely	0.008929
absolutely	0.000028
absolutely necessary	1.000000
necessary .	0.100000
Another important	0.076923
important distinction	0.125000
for single	0.003610
single document	0.071429
LexRank has	0.083333
to multi-document	0.001328
task remains	0.023810
remains	0.000112
remains the	0.250000
same in	0.040000
in both	0.001873
both cases	0.032258
cases --	0.055556
-- only	0.040000
to choose	0.001328
choose	0.000056
choose from	0.500000
from has	0.009615
has grown	0.011905
grown	0.000028
grown .	1.000000
when summarizing	0.028571
summarizing	0.000028
summarizing multiple	1.000000
multiple documents	0.153846
a greater	0.001227
greater	0.000084
greater risk	0.333333
risk	0.000056
risk of	0.500000
of selecting	0.000891
selecting duplicate	0.200000
duplicate	0.000056
duplicate or	0.500000
or highly	0.004505
highly redundant	0.111111
redundant	0.000028
redundant sentences	1.000000
same summary	0.040000
<s> Imagine	0.000769
Imagine	0.000028
Imagine you	1.000000
articles on	0.125000
particular event	0.076923
you want	0.076923
produce one	0.045455
one summary	0.015385
<s> Each	0.003843
Each	0.000167
Each article	0.166667
article is	0.034483
many similar	0.019231
similar sentences	0.111111
you would	0.076923
would only	0.018868
only want	0.026316
include distinct	0.037037
distinct ideas	0.142857
To address	0.111111
address	0.000112
address this	0.250000
this issue	0.010989
issue ,	0.125000
, LexRank	0.000561
LexRank applies	0.083333
applies	0.000195
applies a	0.285714
a heuristic	0.002454
heuristic	0.000084
heuristic post-processing	0.333333
that builds	0.003546
builds up	0.500000
up a	0.090909
summary by	0.023810
by adding	0.011429
adding	0.000056
adding sentences	0.500000
in rank	0.001873
rank order	0.166667
order ,	0.142857
but discards	0.014706
discards	0.000028
discards any	1.000000
any sentences	0.032258
are too	0.004149
too similar	0.166667
to ones	0.001328
ones already	0.100000
already placed	0.200000
The method	0.005208
method used	0.062500
used is	0.008850
called Cross-Sentence	0.055556
Cross-Sentence	0.000028
Cross-Sentence Information	1.000000
Information Subsumption	0.200000
Subsumption	0.000028
Subsumption -LRB-	1.000000
-LRB- CSIS	0.002710
CSIS	0.000056
CSIS -RRB-	0.500000
Why unsupervised	0.142857
summarization works	0.020000
works These	0.500000
work based	0.041667
that sentences	0.003546
sentences ``	0.026316
'' other	0.005155
other similar	0.014286
if one	0.035714
one sentence	0.030769
to many	0.005312
many others	0.019231
others ,	0.083333
will likely	0.057143
likely be	0.062500
sentence of	0.020833
of great	0.000891
great importance	0.333333
importance .	0.166667
The importance	0.005208
this sentence	0.010989
sentence also	0.020833
also stems	0.014493
stems	0.000056
stems from	0.500000
`` recommending	0.005291
recommending	0.000028
recommending ''	1.000000
'' it	0.005155
get ranked	0.142857
highly and	0.111111
and placed	0.001445
sentence must	0.020833
be similar	0.004219
many sentences	0.019231
are in	0.012448
in turn	0.009363
turn	0.000167
turn also	0.166667
also similar	0.014493
other sentences	0.014286
This makes	0.015873
makes intuitive	0.125000
intuitive	0.000028
intuitive sense	1.000000
sense and	0.125000
and allows	0.002890
allows the	0.375000
be applied	0.012658
to any	0.003984
arbitrary new	0.333333
The methods	0.010417
are domain-independent	0.004149
domain-independent	0.000028
domain-independent and	1.000000
and easily	0.001445
portable .	0.333333
One could	0.076923
could imagine	0.062500
imagine	0.000028
imagine the	1.000000
features indicating	0.038462
indicating	0.000028
indicating important	1.000000
important sentences	0.125000
the news	0.001384
news domain	0.230769
domain might	0.050000
might vary	0.038462
vary	0.000167
vary considerably	0.166667
considerably	0.000028
considerably from	1.000000
the biomedical	0.000692
biomedical	0.000028
biomedical domain	1.000000
domain .	0.300000
the unsupervised	0.000692
unsupervised ``	0.125000
'' -	0.010309
based approach	0.018519
approach applies	0.028571
applies to	0.142857
any domain	0.032258
<s> Incorporating	0.000769
Incorporating	0.000028
Incorporating diversity	1.000000
diversity	0.000112
diversity :	0.250000
: GRASSHOPPER	0.009804
GRASSHOPPER	0.000084
GRASSHOPPER algorithm	0.333333
algorithm As	0.035714
As mentioned	0.166667
mentioned above	0.166667
, multi-document	0.000561
multi-document extractive	0.250000
summarization faces	0.020000
faces	0.000028
faces a	1.000000
a problem	0.003681
potential redundancy	0.142857
redundancy	0.000084
redundancy .	0.333333
would like	0.037736
like to	0.071429
extract sentences	0.250000
are both	0.008299
both ``	0.064516
, contain	0.000561
contain the	0.166667
the main	0.001384
main ideas	0.125000
ideas -RRB-	0.250000
`` diverse	0.005291
diverse	0.000056
diverse ''	0.500000
they differ	0.025000
differ from	0.333333
one another	0.015385
another -RRB-	0.076923
LexRank deals	0.083333
deals	0.000112
deals with	1.000000
with diversity	0.005464
diversity as	0.250000
heuristic final	0.333333
final stage	0.111111
stage	0.000140
stage using	0.200000
using CSIS	0.016949
CSIS ,	0.500000
systems have	0.044643
used similar	0.008850
similar methods	0.037037
as Maximal	0.003484
Maximal	0.000028
Maximal Marginal	1.000000
Marginal	0.000028
Marginal Relevance	1.000000
Relevance	0.000028
Relevance -LRB-	1.000000
-LRB- MMR	0.002710
MMR	0.000028
MMR -RRB-	1.000000
in trying	0.001873
to eliminate	0.002656
eliminate	0.000056
eliminate redundancy	0.500000
redundancy in	0.666667
in information	0.001873
retrieval results	0.142857
We have	0.142857
have developed	0.009615
developed a	0.115385
algorithm like	0.035714
like Page\/Lex\/TextRank	0.035714
Page\/Lex\/TextRank	0.000028
Page\/Lex\/TextRank that	1.000000
that handles	0.003546
handles	0.000028
handles both	1.000000
`` centrality	0.005291
centrality ''	0.500000
`` diversity	0.005291
diversity ''	0.250000
a unified	0.001227
unified	0.000028
unified mathematical	1.000000
mathematical framework	0.500000
framework	0.000112
framework based	0.250000
on absorbing	0.004717
absorbing	0.000084
absorbing Markov	0.333333
Markov chain	0.055556
chain	0.000028
chain random	1.000000
walks .	0.500000
An absorbing	0.062500
absorbing random	0.333333
walk is	0.200000
is like	0.004065
like a	0.071429
standard random	0.071429
walk ,	0.200000
, except	0.000561
except	0.000028
except some	1.000000
some states	0.012048
states	0.000112
states are	0.250000
are now	0.016598
now absorbing	0.076923
absorbing states	0.333333
states that	0.250000
that act	0.003546
act as	0.750000
`` black	0.005291
black	0.000028
black holes	1.000000
holes	0.000028
holes ''	1.000000
that cause	0.003546
cause	0.000056
cause the	0.500000
the walk	0.000692
walk to	0.200000
to end	0.002656
end abruptly	0.125000
abruptly	0.000028
abruptly at	1.000000
at that	0.014706
that state	0.003546
state	0.000391
state .	0.071429
The algorithm	0.005208
called GRASSHOPPER	0.055556
GRASSHOPPER for	0.333333
for reasons	0.003610
reasons	0.000056
reasons that	0.500000
that should	0.007092
should soon	0.052632
soon	0.000084
soon become	0.333333
become clear	0.250000
clear .	0.250000
addition to	0.500000
to explicitly	0.002656
explicitly promoting	0.250000
promoting	0.000028
promoting diversity	1.000000
diversity during	0.250000
ranking process	0.142857
process ,	0.027778
, GRASSHOPPER	0.000561
GRASSHOPPER incorporates	0.333333
incorporates	0.000028
incorporates a	1.000000
a prior	0.001227
prior ranking	0.333333
ranking -LRB-	0.142857
on sentence	0.004717
summarization -RRB-	0.020000
<s> Maximum	0.001537
Maximum	0.000084
Maximum entropy-based	0.333333
entropy-based	0.000028
entropy-based summarization	1.000000
summarization It	0.020000
abstractive method	0.166667
<s> Even	0.000769
Even	0.000028
Even though	1.000000
though automating	0.100000
automating	0.000028
automating abstractive	1.000000
summarization research	0.020000
most practical	0.017241
practical	0.000056
practical systems	0.500000
of extractive	0.000891
<s> Extracted	0.000769
Extracted	0.000028
Extracted sentences	1.000000
can form	0.005525
a valid	0.001227
valid	0.000028
valid summary	1.000000
summary in	0.047619
in itself	0.001873
itself or	0.200000
or form	0.004505
basis for	0.333333
for further	0.010830
further condensation	0.125000
condensation	0.000028
condensation operations	1.000000
operations	0.000028
operations .	1.000000
, evaluation	0.000561
of extracted	0.000891
extracted	0.000028
extracted summaries	1.000000
be automated	0.004219
automated ,	0.142857
since it	0.200000
a classification	0.001227
classification task	0.058824
the DUC	0.000692
DUC	0.000028
DUC 2001	1.000000
2001	0.000056
2001 and	0.500000
and 2002	0.001445
2002	0.000056
2002 evaluation	0.500000
evaluation workshops	0.018519
workshops	0.000056
workshops ,	0.500000
, TNO	0.000561
TNO	0.000028
TNO developed	1.000000
sentence extraction	0.020833
for multi-document	0.003610
summarization in	0.040000
The system	0.031250
system was	0.053763
was based	0.012987
hybrid system	0.500000
system using	0.010753
a naive	0.001227
naive	0.000056
naive Bayes	0.500000
Bayes classifier	0.333333
classifier and	0.142857
statistical language	0.030303
for modeling	0.003610
modeling salience	0.142857
salience	0.000028
salience .	1.000000
system exhibited	0.010753
exhibited	0.000028
exhibited good	1.000000
good results	0.076923
results ,	0.095238
we wanted	0.022222
wanted	0.000028
wanted to	1.000000
to explore	0.002656
explore	0.000112
explore the	0.250000
the effectiveness	0.000692
effectiveness	0.000084
effectiveness of	0.333333
a maximum	0.002454
maximum	0.000167
maximum entropy	0.500000
entropy	0.000140
entropy -LRB-	0.200000
-LRB- ME	0.002710
ME	0.000056
ME -RRB-	0.500000
-RRB- classifier	0.002710
classifier for	0.142857
the meeting	0.000692
meeting	0.000028
meeting summarization	1.000000
summarization task	0.020000
as ME	0.003484
ME is	0.500000
is known	0.006098
known to	0.076923
be robust	0.004219
robust against	0.250000
against	0.000140
against feature	0.200000
feature dependencies	0.076923
dependencies	0.000056
dependencies .	1.000000
Maximum entropy	0.666667
entropy has	0.200000
has also	0.035714
also been	0.057971
applied successfully	0.066667
successfully	0.000084
successfully for	0.333333
for summarization	0.003610
the broadcast	0.000692
broadcast	0.000028
broadcast news	1.000000
<s> Aided	0.000769
Aided	0.000084
Aided summarization	0.333333
summarization Machine	0.020000
Machine learning	0.111111
learning techniques	0.023256
techniques from	0.043478
from closely	0.009615
related fields	0.066667
fields such	0.166667
as information	0.003484
retrieval or	0.142857
or text	0.009009
text mining	0.012579
mining have	0.200000
been successfully	0.014706
successfully adapted	0.333333
adapted	0.000028
adapted to	1.000000
to help	0.002656
help automatic	0.111111
<s> Apart	0.000769
Apart	0.000028
Apart from	1.000000
from Fully	0.009615
Fully	0.000028
Fully Automated	1.000000
Automated Summarizers	0.500000
Summarizers	0.000028
Summarizers -LRB-	1.000000
-LRB- FAS	0.002710
FAS	0.000028
FAS -RRB-	1.000000
are systems	0.012448
systems that	0.062500
that aid	0.003546
aid users	0.250000
users with	0.111111
-LRB- MAHS	0.002710
MAHS	0.000028
MAHS =	1.000000
= Machine	0.111111
Machine Aided	0.111111
Aided Human	0.333333
Human	0.000140
Human Summarization	0.200000
Summarization -RRB-	0.500000
example by	0.024691
by highlighting	0.005714
highlighting	0.000028
highlighting candidate	1.000000
candidate	0.000084
candidate passages	0.333333
passages	0.000056
passages to	0.500000
be included	0.004219
included in	0.125000
and there	0.005780
that depend	0.003546
depend	0.000084
depend on	1.000000
on post-processing	0.004717
post-processing by	0.333333
-LRB- HAMS	0.002710
HAMS	0.000028
HAMS =	1.000000
= Human	0.111111
Human Aided	0.200000
Aided Machine	0.333333
Machine Summarization	0.111111
Evaluation An	0.111111
An ongoing	0.062500
ongoing	0.000056
ongoing issue	0.500000
issue in	0.125000
this field	0.021978
field is	0.037037
Evaluation techniques	0.111111
techniques fall	0.043478
fall	0.000112
fall into	0.500000
into intrinsic	0.012821
intrinsic and	0.250000
and extrinsic	0.001445
extrinsic ,	0.166667
, inter-texual	0.000561
inter-texual	0.000056
inter-texual and	0.500000
and intra-texual	0.001445
intra-texual	0.000028
intra-texual .	1.000000
evaluation tests	0.037037
tests the	0.500000
in of	0.001873
of itself	0.000891
itself while	0.200000
while an	0.050000
an extrinsic	0.007576
summarization based	0.020000
on how	0.009434
how it	0.068966
it affects	0.008547
affects	0.000028
affects the	1.000000
the completion	0.000692
completion	0.000028
completion of	1.000000
other task	0.014286
Intrinsic evaluations	0.333333
evaluations	0.000167
evaluations have	0.166667
have assessed	0.009615
assessed	0.000028
assessed mainly	1.000000
mainly the	0.166667
the coherence	0.000692
coherence	0.000084
coherence and	0.666667
and informativeness	0.001445
informativeness	0.000084
informativeness of	0.666667
summaries .	0.139535
Extrinsic evaluations	0.500000
evaluations ,	0.166667
, have	0.001123
have tested	0.009615
tested	0.000056
tested the	0.500000
the impact	0.001384
impact	0.000056
impact of	0.500000
summarization on	0.020000
like relevance	0.035714
relevance assessment	0.333333
assessment	0.000028
assessment ,	1.000000
, reading	0.000561
reading comprehension	0.250000
comprehension ,	0.142857
<s> Intra-texual	0.000769
Intra-texual	0.000028
Intra-texual methods	1.000000
methods assess	0.022727
assess	0.000084
assess the	0.333333
specific summarization	0.047619
the inter-texual	0.000692
inter-texual ones	0.500000
ones focus	0.100000
on contrastive	0.004717
contrastive	0.000028
contrastive analysis	1.000000
of outputs	0.000891
outputs	0.000028
outputs of	1.000000
several summarization	0.045455
<s> Human	0.002306
Human judgement	0.200000
judgement	0.000084
judgement often	0.333333
often has	0.045455
has wide	0.011905
wide	0.000112
wide variance	0.250000
variance	0.000028
variance on	1.000000
is considered	0.004065
considered a	0.111111
`` good	0.005291
good ''	0.076923
'' summary	0.005155
that making	0.003546
making the	0.285714
evaluation process	0.018519
process automatic	0.027778
automatic is	0.043478
particularly difficult	0.200000
difficult .	0.071429
is both	0.002033
both time	0.032258
time and	0.090909
and labor	0.001445
labor	0.000056
labor intensive	0.500000
intensive	0.000028
intensive as	1.000000
as it	0.003484
it requires	0.017094
requires humans	0.062500
humans to	0.083333
to read	0.001328
read	0.000195
read not	0.142857
summaries but	0.023256
also the	0.028986
Other issues	0.142857
issues are	0.200000
are those	0.004149
those concerning	0.045455
concerning	0.000028
concerning coherence	1.000000
and coverage	0.001445
coverage .	0.333333
One of	0.153846
the metrics	0.000692
metrics used	0.111111
in NIST	0.001873
NIST 's	0.500000
's annual	0.019608
annual	0.000056
annual Document	0.500000
Document Understanding	0.250000
Understanding Conferences	0.500000
Conferences	0.000056
Conferences ,	0.500000
which research	0.007246
research groups	0.023810
groups submit	0.200000
submit	0.000056
submit their	0.500000
their systems	0.058824
systems for	0.017857
for both	0.003610
both summarization	0.032258
summarization and	0.020000
translation tasks	0.013514
the ROUGE	0.000692
ROUGE metric	0.200000
metric -LRB-	0.333333
It essentially	0.026316
essentially calculates	0.125000
calculates	0.000028
calculates n-gram	1.000000
n-gram overlaps	0.500000
overlaps	0.000056
overlaps between	0.500000
between automatically	0.025641
automatically generated	0.142857
generated summaries	0.066667
and previously-written	0.001445
previously-written	0.000028
previously-written human	1.000000
human summaries	0.021739
A high	0.020000
high level	0.166667
level of	0.350000
of overlap	0.000891
overlap should	0.250000
should indicate	0.052632
indicate	0.000084
indicate a	0.333333
of shared	0.000891
shared concepts	0.500000
concepts between	0.200000
two summaries	0.034483
that overlap	0.003546
overlap metrics	0.250000
metrics like	0.111111
like this	0.035714
are unable	0.004149
provide any	0.166667
any feedback	0.032258
feedback	0.000056
feedback on	0.500000
summary 's	0.047619
's coherence	0.019608
coherence .	0.333333
<s> Anaphor	0.000769
Anaphor	0.000028
Anaphor resolution	1.000000
resolution remains	0.250000
remains another	0.250000
another problem	0.153846
problem yet	0.022727
yet to	0.500000
be fully	0.008439
fully solved	0.166667
solved .	0.200000
<s> Evaluating	0.001537
Evaluating	0.000056
Evaluating summaries	1.000000
either manually	0.100000
manually or	0.250000
automatically ,	0.047619
a hard	0.002454
hard task	0.166667
in evaluation	0.001873
evaluation comes	0.018519
comes from	0.400000
the impossibility	0.000692
impossibility	0.000028
impossibility of	1.000000
of building	0.000891
building	0.000028
building a	1.000000
a fair	0.001227
fair	0.000028
fair gold-standard	1.000000
gold-standard	0.000028
gold-standard against	1.000000
against which	0.200000
the results	0.002768
results of	0.047619
systems can	0.026786
be compared	0.004219
also very	0.028986
very hard	0.024390
hard to	0.333333
determine what	0.043478
a correct	0.002454
correct summary	0.066667
is ,	0.018293
because there	0.066667
is always	0.004065
always the	0.333333
good summary	0.153846
is quite	0.002033
quite different	0.375000
from any	0.009615
any human	0.032258
human summary	0.021739
summary used	0.023810
an approximation	0.015152
approximation	0.000167
approximation to	0.333333
correct output	0.066667
<s> Current	0.002306
Current difficulties	0.200000
difficulties in	0.500000
in evaluating	0.001873
evaluating summaries	0.400000
summaries automatically	0.023256
automatically The	0.047619
evaluate the	0.500000
the informativeness	0.000692
of automatic	0.001783
automatic summaries	0.130435
compare them	0.142857
them with	0.052632
with human-made	0.005464
human-made	0.000056
human-made model	0.500000
as content	0.003484
content selection	0.083333
selection	0.000028
selection is	1.000000
not a	0.008929
a deterministic	0.002454
deterministic	0.000112
deterministic problem	0.250000
, different	0.001684
different people	0.020408
people would	0.062500
would choose	0.018868
choose different	0.500000
even ,	0.037037
same person	0.040000
person may	0.052632
may chose	0.019231
chose	0.000028
chose different	1.000000
sentences at	0.013158
at different	0.014706
different times	0.020408
times ,	0.200000
, showing	0.000561
showing evidence	0.500000
evidence	0.000056
evidence of	0.500000
of low	0.000891
low agreement	0.333333
agreement among	0.333333
among humans	0.125000
humans as	0.083333
which sentences	0.007246
are good	0.004149
summary sentences	0.023810
<s> Besides	0.000769
Besides	0.000028
Besides the	1.000000
human variability	0.021739
variability	0.000028
variability ,	1.000000
the semantic	0.001384
semantic equivalence	0.047619
equivalence is	0.500000
is another	0.004065
because two	0.033333
two distinct	0.034483
distinct sentences	0.142857
same meaning	0.040000
meaning but	0.043478
but not	0.058824
not using	0.008929
same words	0.040000
This phenomenon	0.031746
phenomenon is	0.200000
as paraphrase	0.003484
paraphrase	0.000028
paraphrase .	1.000000
can find	0.005525
find an	0.153846
automatically evaluating	0.047619
summaries using	0.023256
using paraphrases	0.016949
paraphrases	0.000028
paraphrases -LRB-	1.000000
-LRB- ParaEval	0.002710
ParaEval	0.000028
ParaEval -RRB-	1.000000
<s> Moreover	0.003075
Moreover	0.000112
Moreover ,	1.000000
most summarization	0.017241
systems perform	0.008929
perform an	0.090909
an extractive	0.007576
extractive approach	0.142857
, selecting	0.000561
selecting and	0.200000
and copying	0.001445
copying	0.000028
copying important	1.000000
sentences from	0.026316
Although humans	0.125000
humans can	0.083333
also cut	0.014493
cut	0.000028
cut and	1.000000
and paste	0.001445
paste	0.000028
paste relevant	1.000000
relevant information	0.142857
information of	0.021739
the times	0.000692
times they	0.200000
they rephrase	0.025000
rephrase	0.000028
rephrase sentences	1.000000
sentences when	0.013158
when necessary	0.028571
necessary ,	0.100000
or they	0.004505
they join	0.025000
join	0.000028
join different	1.000000
different related	0.020408
related information	0.066667
into one	0.025641
summaries qualitatively	0.023256
qualitatively	0.000028
qualitatively The	1.000000
main drawback	0.125000
drawback	0.000028
drawback of	1.000000
evaluation systems	0.018519
systems existing	0.008929
existing so	0.200000
so far	0.033333
far is	0.125000
that we	0.010638
need at	0.047619
least one	0.200000
one reference	0.015385
and for	0.001445
for some	0.018051
some methods	0.024096
methods more	0.022727
compare automatic	0.142857
with models	0.005464
hard and	0.166667
and expensive	0.002890
expensive task	0.142857
<s> Much	0.002306
Much	0.000084
Much effort	0.333333
effort has	0.250000
have corpus	0.009615
of texts	0.001783
texts and	0.058824
their corresponding	0.029412
corresponding summaries	0.166667
methods presented	0.022727
presented	0.000167
presented in	0.500000
the previous	0.001384
previous Section	0.333333
Section	0.000028
Section ,	1.000000
, not	0.003930
only do	0.026316
do we	0.038462
have human-made	0.009615
human-made summaries	0.500000
summaries available	0.023256
for comparison	0.003610
comparison	0.000084
comparison ,	0.333333
also manual	0.014493
manual annotation	0.500000
annotation has	0.250000
be performed	0.008439
in some	0.007491
them -LRB-	0.052632
e.g. SCU	0.017857
SCU	0.000028
SCU in	1.000000
the Pyramid	0.000692
Pyramid	0.000028
Pyramid Method	1.000000
Method	0.000028
Method -RRB-	1.000000
In any	0.019048
, what	0.001123
evaluation methods	0.018519
need as	0.047619
an input	0.022727
summaries to	0.046512
to serve	0.001328
as gold	0.003484
gold standards	0.166667
standards and	0.200000
all perform	0.023256
perform a	0.272727
a quantitative	0.002454
quantitative evaluation	0.500000
evaluation with	0.018519
to different	0.001328
different similarity	0.020408
similarity metrics	0.100000
metrics .	0.111111
To overcome	0.111111
overcome these	0.500000
these problems	0.023810
we think	0.022222
think that	0.333333
the quantitative	0.000692
evaluation might	0.018519
might not	0.076923
not be	0.107143
the only	0.000692
only way	0.026316
evaluate summaries	0.250000
a qualitative	0.002454
qualitative	0.000056
qualitative automatic	0.500000
be also	0.004219
also important	0.014493
important .	0.062500
<s> Therefore	0.001537
Therefore	0.000056
Therefore ,	1.000000
second aim	0.100000
aim	0.000056
aim of	0.500000
this paper	0.010989
paper is	0.090909
to suggest	0.001328
suggest	0.000084
suggest a	0.333333
a novel	0.001227
novel	0.000028
novel proposal	1.000000
proposal	0.000028
proposal for	1.000000
evaluating automatically	0.200000
automatically the	0.047619
qualitative manner	0.500000
manner rather	0.250000
than in	0.022222
quantitative one	0.250000
<s> Our	0.002306
Our	0.000084
Our evaluation	0.333333
evaluation approach	0.018519
a preliminary	0.001227
preliminary approach	0.333333
be studied	0.004219
studied	0.000028
studied more	1.000000
more deeply	0.010526
deeply	0.000028
deeply ,	1.000000
and developed	0.002890
the future	0.001384
future .	0.333333
<s> Its	0.001537
Its	0.000056
Its main	0.500000
main underlying	0.125000
underlying	0.000084
underlying idea	0.333333
idea is	0.142857
to define	0.002656
define	0.000056
define several	0.500000
several quality	0.045455
quality criteria	0.100000
criteria and	0.250000
and check	0.001445
check	0.000056
check how	0.500000
how a	0.068966
a generated	0.001227
generated summary	0.066667
summary tackles	0.023810
tackles	0.000028
tackles each	1.000000
these ,	0.047619
that a	0.010638
reference model	0.125000
model would	0.100000
would not	0.018868
be necessary	0.008439
necessary anymore	0.100000
anymore	0.000028
anymore ,	1.000000
, taking	0.000561
taking	0.000140
taking only	0.200000
only into	0.026316
into consideration	0.012821
consideration	0.000084
consideration the	0.333333
the automatic	0.001384
original source	0.076923
source .	0.041667
Once performed	0.400000
performed ,	0.200000
it could	0.008547
used together	0.008850
together with	0.125000
with any	0.005464
other automatic	0.014286
automatic methodology	0.043478
methodology to	0.500000
measure summary	0.090909
's informativeness	0.019608
informativeness .	0.333333
Language Generation	0.083333
Generation	0.000056
Generation -LRB-	0.500000
-LRB- NLG	0.008130
NLG	0.000586
NLG -RRB-	0.047619
the natural	0.001384
processing task	0.018519
of generating	0.000891
generating natural	0.200000
language from	0.006757
machine representation	0.025316
representation system	0.052632
system such	0.010753
a knowledge	0.001227
base or	0.250000
a logical	0.001227
logical form	0.166667
<s> Psycholinguists	0.000769
Psycholinguists	0.000028
Psycholinguists prefer	1.000000
prefer the	0.500000
term language	0.055556
language production	0.006757
production when	0.333333
such formal	0.008130
representations are	0.250000
are interpreted	0.004149
interpreted	0.000028
interpreted as	1.000000
as models	0.003484
for mental	0.003610
mental	0.000084
mental representations	0.333333
representations .	0.250000
In a	0.019048
a sense	0.001227
sense ,	0.125000
one can	0.015385
can say	0.005525
say that	0.142857
that an	0.003546
an NLG	0.007576
NLG system	0.095238
translator that	0.142857
that converts	0.003546
converts	0.000028
converts a	1.000000
computer based	0.022727
based representation	0.018519
representation into	0.052632
language representation	0.006757
the methods	0.000692
produce the	0.136364
final language	0.111111
language are	0.006757
are very	0.016598
very different	0.073171
from those	0.019231
those of	0.045455
a compiler	0.003681
compiler	0.000084
compiler due	0.333333
due to	0.400000
the inherent	0.000692
inherent	0.000028
inherent expressivity	1.000000
expressivity	0.000028
expressivity of	1.000000
<s> NLG	0.001537
NLG may	0.047619
understanding .	0.060606
The difference	0.005208
difference can	0.250000
be put	0.004219
put this	0.250000
way :	0.041667
: whereas	0.009804
whereas in	0.333333
in natural	0.013109
understanding the	0.121212
system needs	0.043011
needs to	0.400000
to disambiguate	0.003984
disambiguate	0.000084
disambiguate the	0.333333
input sentence	0.024390
sentence to	0.020833
the machine	0.000692
representation language	0.105263
in NLG	0.005618
NLG the	0.047619
make decisions	0.050000
decisions about	0.200000
about how	0.025000
put a	0.250000
a concept	0.001227
concept	0.000112
concept into	0.250000
The simplest	0.005208
simplest	0.000028
simplest -LRB-	1.000000
and perhaps	0.001445
perhaps trivial	0.166667
trivial -RRB-	0.250000
-RRB- examples	0.002710
examples are	0.041667
that generate	0.003546
generate form	0.055556
form letters	0.050000
letters .	0.400000
Such systems	0.125000
systems do	0.008929
typically involve	0.055556
involve grammar	0.166667
grammar rules	0.135135
but may	0.029412
may generate	0.019231
a letter	0.001227
letter to	0.166667
a consumer	0.001227
consumer	0.000028
consumer ,	1.000000
e.g. stating	0.017857
stating	0.000028
stating that	1.000000
a credit	0.002454
credit	0.000084
credit card	1.000000
card	0.000112
card spending	0.250000
spending	0.000028
spending limit	1.000000
limit is	0.250000
is about	0.002033
about to	0.025000
be reached	0.004219
reached	0.000056
reached .	0.500000
<s> More	0.006149
More	0.000251
More complex	0.111111
complex NLG	0.041667
NLG systems	0.238095
systems dynamically	0.008929
dynamically	0.000056
dynamically create	0.500000
create texts	0.058824
texts to	0.117647
meet a	0.250000
a communicative	0.001227
communicative goal	0.666667
goal .	0.142857
As in	0.222222
in other	0.009363
other areas	0.028571
areas	0.000167
areas of	0.333333
this can	0.010989
done using	0.090909
using either	0.016949
either explicit	0.100000
explicit models	0.200000
models of	0.038462
language -LRB-	0.013514
, grammars	0.000561
grammars -RRB-	0.142857
the domain	0.001384
models derived	0.038462
derived by	0.333333
by analyzing	0.011429
analyzing human-written	0.200000
human-written	0.000056
human-written texts	0.500000
texts .	0.176471
NLG is	0.095238
a fast-evolving	0.001227
fast-evolving	0.000028
fast-evolving field	1.000000
The best	0.005208
best single	0.055556
source for	0.041667
for up-to-date	0.003610
up-to-date	0.000028
up-to-date research	1.000000
the area	0.001384
area is	0.181818
the SIGGEN	0.000692
SIGGEN	0.000028
SIGGEN portion	1.000000
portion	0.000056
portion of	1.000000
the ACL	0.000692
ACL	0.000056
ACL Anthology	0.500000
Anthology	0.000028
Anthology .	1.000000
<s> Perhaps	0.000769
Perhaps	0.000028
Perhaps the	1.000000
the closest	0.000692
closest the	0.500000
field comes	0.037037
comes to	0.200000
a specialist	0.001227
specialist	0.000028
specialist textbook	1.000000
textbook	0.000056
textbook is	0.500000
is Reiter	0.002033
Reiter	0.000028
Reiter and	1.000000
and Dale	0.001445
Dale	0.000028
Dale -LRB-	1.000000
-LRB- 2000	0.002710
2000	0.000084
2000 -RRB-	0.333333
this book	0.010989
book does	0.125000
not describe	0.008929
describe developments	0.166667
developments in	0.666667
field since	0.037037
since 2000	0.100000
2000 .	0.333333
This system	0.015873
system takes	0.010753
takes as	0.333333
input six	0.024390
six numbers	0.500000
numbers	0.000195
numbers ,	0.285714
which give	0.007246
give	0.000112
give predicted	0.250000
predicted	0.000056
predicted pollen	0.500000
pollen	0.000363
pollen levels	0.692308
levels in	0.045455
of Scotland	0.001783
Scotland	0.000140
Scotland .	0.400000
<s> From	0.000769
From	0.000028
From these	1.000000
these numbers	0.047619
system generates	0.010753
generates	0.000084
generates a	0.666667
short textual	0.125000
textual summary	0.200000
of pollen	0.001783
levels as	0.045455
the historical	0.000692
historical	0.000028
historical data	1.000000
data for	0.012987
for 1-July-2005	0.003610
1-July-2005	0.000028
1-July-2005 ,	1.000000
the software	0.002076
software produces	0.037037
produces Grass	0.250000
Grass	0.000112
Grass pollen	1.000000
levels for	0.136364
for Friday	0.010830
Friday	0.000084
Friday have	1.000000
have increased	0.028846
increased from	0.600000
the moderate	0.002076
moderate	0.000140
moderate to	0.600000
to high	0.003984
high levels	0.166667
levels of	0.318182
of yesterday	0.002674
yesterday	0.000084
yesterday with	0.666667
with values	0.016393
values of	0.500000
of around	0.001783
around 6	0.375000
6	0.000112
6 to	0.750000
to 7	0.003984
7	0.000195
7 across	0.428571
across most	0.600000
most parts	0.051724
the country	0.002076
country	0.000112
country .	0.500000
in Northern	0.001873
Northern	0.000084
Northern areas	0.333333
areas ,	0.166667
, pollen	0.000561
levels will	0.090909
be moderate	0.004219
moderate with	0.200000
of 4	0.001783
4	0.000140
4 .	0.400000
the actual	0.002076
actual forecast	0.200000
forecast	0.000028
forecast -LRB-	1.000000
-LRB- written	0.002710
human meteorologist	0.021739
meteorologist	0.000028
meteorologist -RRB-	1.000000
-RRB- from	0.002710
from this	0.009615
this data	0.010989
data was	0.012987
was Pollen	0.012987
Pollen	0.000028
Pollen counts	1.000000
counts	0.000028
counts are	1.000000
are expected	0.004149
expected to	0.285714
to remain	0.001328
remain	0.000028
remain high	1.000000
high at	0.055556
at level	0.014706
level 6	0.050000
6 over	0.250000
over most	0.083333
Scotland ,	0.200000
even level	0.037037
level 7	0.050000
7 in	0.285714
the south	0.001384
south	0.000056
south east	1.000000
east	0.000056
east .	1.000000
only relief	0.026316
relief	0.000028
relief is	1.000000
the Northern	0.001384
Northern Isles	0.666667
Isles	0.000056
Isles and	1.000000
and far	0.002890
far northeast	0.250000
northeast	0.000056
northeast of	1.000000
of mainland	0.001783
mainland	0.000056
mainland Scotland	1.000000
Scotland with	0.200000
with medium	0.005464
medium	0.000084
medium levels	0.333333
pollen count	0.076923
count .	0.200000
<s> Comparing	0.000769
Comparing	0.000028
Comparing these	1.000000
these two	0.023810
two illustrates	0.034483
illustrates	0.000056
illustrates some	0.500000
the choices	0.000692
choices that	0.200000
that NLG	0.007092
systems must	0.008929
must make	0.071429
make ;	0.050000
; these	0.042553
these are	0.047619
are further	0.004149
further discussed	0.125000
<s> Stages	0.000769
Stages	0.000028
Stages The	1.000000
The process	0.015625
generate text	0.055556
text can	0.006289
as keeping	0.003484
keeping	0.000056
keeping a	0.500000
of canned	0.000891
canned	0.000056
canned text	0.500000
text that	0.025157
is copied	0.002033
copied	0.000056
copied and	0.500000
and pasted	0.001445
pasted	0.000028
pasted ,	1.000000
, possibly	0.000561
possibly linked	0.500000
linked with	0.333333
some glue	0.012048
glue	0.000028
glue text	1.000000
The results	0.005208
results may	0.047619
be satisfactory	0.004219
satisfactory	0.000028
satisfactory in	1.000000
in simple	0.001873
simple domains	0.038462
domains such	0.125000
as horoscope	0.003484
horoscope	0.000028
horoscope machines	1.000000
machines	0.000112
machines or	0.250000
or generators	0.004505
generators	0.000056
generators of	0.500000
of personalised	0.000891
personalised	0.000028
personalised business	1.000000
business letters	0.250000
a sophisticated	0.001227
sophisticated	0.000195
sophisticated NLG	0.142857
include stages	0.037037
stages	0.000056
stages of	0.500000
of planning	0.000891
planning	0.000056
planning and	0.500000
and merging	0.001445
merging	0.000056
merging of	0.500000
information to	0.086957
to enable	0.001328
enable	0.000056
enable the	1.000000
the generation	0.000692
generation of	0.111111
that looks	0.003546
looks natural	0.250000
natural and	0.026667
and does	0.001445
not become	0.008929
become repetitive	0.250000
repetitive	0.000056
repetitive .	0.500000
Typical stages	0.500000
stages are	0.500000
are :	0.008299
: Content	0.009804
Content	0.000028
Content determination	1.000000
determination	0.000028
determination :	1.000000
: Deciding	0.009804
Deciding	0.000028
Deciding what	1.000000
what information	0.031250
to mention	0.001328
mention in	0.333333
the pollen	0.000692
pollen example	0.076923
example above	0.012346
, deciding	0.002246
deciding whether	0.333333
whether to	0.076923
explicitly mention	0.250000
mention that	0.333333
that pollen	0.003546
pollen level	0.153846
level is	0.050000
is 7	0.002033
Document structuring	0.250000
structuring	0.000028
structuring :	1.000000
: Overall	0.009804
Overall	0.000028
Overall organization	1.000000
convey .	0.333333
deciding to	0.333333
to describe	0.002656
the areas	0.001384
areas with	0.333333
with high	0.010929
high pollen	0.055556
levels first	0.045455
first ,	0.030303
, instead	0.000561
with low	0.005464
low pollen	0.333333
levels .	0.045455
<s> Aggregation	0.000769
Aggregation	0.000028
Aggregation :	1.000000
: Merging	0.009804
Merging	0.000028
Merging of	1.000000
of similar	0.001783
improve readability	0.076923
readability	0.000028
readability and	1.000000
and naturalness	0.001445
naturalness	0.000028
naturalness .	1.000000
, merging	0.000561
merging the	0.500000
sentences Grass	0.013158
yesterday and	0.333333
and Grass	0.001445
be around	0.004219
country into	0.250000
the single	0.000692
single sentence	0.071429
sentence Grass	0.020833
<s> Lexical	0.001537
Lexical	0.000056
Lexical choice	0.500000
choice :	0.125000
: Putting	0.009804
Putting	0.000028
Putting words	1.000000
words to	0.009174
the concepts	0.000692
whether medium	0.076923
medium or	0.333333
or moderate	0.004505
moderate should	0.200000
used when	0.017699
when describing	0.057143
describing a	0.250000
a pollen	0.001227
<s> Referring	0.000769
Referring	0.000028
Referring expression	1.000000
expression generation	0.100000
: Creating	0.019608
Creating	0.000056
Creating referring	0.500000
referring expressions	0.500000
expressions	0.000084
expressions that	0.333333
that identify	0.003546
identify objects	0.083333
objects and	0.200000
and regions	0.001445
regions .	0.500000
to use	0.013280
use in	0.013889
Scotland to	0.200000
to refer	0.001328
a certain	0.001227
certain region	0.142857
region	0.000028
region in	1.000000
in Scotland	0.001873
This task	0.031746
task also	0.023810
also includes	0.014493
includes making	0.142857
making decisions	0.142857
about pronouns	0.025000
pronouns and	0.500000
other types	0.014286
of anaphora	0.000891
anaphora	0.000028
anaphora .	1.000000
<s> Realisation	0.000769
Realisation	0.000028
Realisation :	1.000000
Creating the	0.500000
actual text	0.200000
which should	0.007246
be correct	0.004219
correct according	0.066667
rules of	0.093023
of syntax	0.001783
, morphology	0.001123
and orthography	0.001445
orthography .	0.500000
using will	0.016949
be for	0.004219
future tense	0.333333
tense	0.000056
tense of	0.500000
of to	0.000891
be .	0.004219
<s> Applications	0.001537
Applications	0.000056
Applications The	0.500000
The popular	0.005208
popular media	0.111111
media has	0.166667
been especially	0.014706
especially interested	0.066667
interested	0.000028
interested in	1.000000
systems which	0.017857
which generate	0.021739
generate jokes	0.055556
jokes	0.000028
jokes -LRB-	1.000000
see computational	0.050000
computational humor	0.100000
humor	0.000028
humor -RRB-	1.000000
But from	0.166667
a commercial	0.002454
commercial	0.000307
commercial perspective	0.090909
perspective ,	0.250000
successful NLG	0.111111
NLG applications	0.047619
applications have	0.080000
been data-to-text	0.014706
data-to-text	0.000028
data-to-text systems	1.000000
generate textual	0.055556
textual summaries	0.200000
of databases	0.000891
databases and	0.125000
data sets	0.038961
sets ;	0.090909
systems usually	0.017857
usually perform	0.031250
perform data	0.090909
data analysis	0.012987
analysis as	0.015385
as text	0.003484
text generation	0.006289
, several	0.000561
several systems	0.045455
been built	0.014706
built	0.000084
built that	0.333333
that produce	0.003546
produce textual	0.045455
textual weather	0.200000
weather forecasts	0.571429
forecasts	0.000140
forecasts from	0.200000
from weather	0.009615
weather data	0.142857
The earliest	0.005208
earliest such	0.500000
such system	0.008130
be deployed	0.004219
deployed	0.000056
deployed was	0.500000
was FoG	0.012987
FoG	0.000056
FoG ,	0.500000
by Environment	0.005714
Environment	0.000028
Environment Canada	1.000000
Canada to	0.166667
generate weather	0.055556
forecasts in	0.200000
in French	0.001873
and English	0.001445
English in	0.027027
the early	0.002076
early 1990s	0.100000
1990s	0.000084
1990s .	0.333333
The success	0.005208
of FoG	0.000891
FoG triggered	0.500000
triggered	0.000028
triggered other	1.000000
other work	0.014286
both research	0.032258
and commercial	0.001445
commercial .	0.090909
this area	0.032967
area include	0.090909
include an	0.074074
an experiment	0.007576
experiment which	0.200000
which showed	0.007246
that users	0.003546
users sometimes	0.111111
sometimes preferred	0.076923
preferred	0.000028
preferred computer-generated	1.000000
computer-generated	0.000028
computer-generated weather	1.000000
forecasts to	0.200000
to human-written	0.001328
human-written ones	0.500000
in part	0.001873
part because	0.037037
the computer	0.001384
computer forecasts	0.022727
forecasts used	0.200000
used more	0.008850
more consistent	0.010526
consistent	0.000028
consistent terminology	1.000000
terminology	0.000028
terminology ,	1.000000
demonstration that	0.200000
that statistical	0.003546
techniques could	0.043478
generate high-quality	0.055556
high-quality	0.000028
high-quality weather	1.000000
forecasts .	0.200000
Recent applications	0.333333
applications include	0.080000
the ARNS	0.000692
ARNS	0.000028
ARNS system	1.000000
system used	0.010753
to summarise	0.003984
summarise	0.000084
summarise conditions	0.333333
conditions in	0.200000
in US	0.001873
US	0.000195
US ports	0.142857
ports	0.000028
ports .	1.000000
the 1990s	0.001384
1990s there	0.333333
was considerable	0.012987
considerable interest	0.200000
in using	0.005618
using NLG	0.050847
NLG to	0.142857
summarise financial	0.333333
financial and	0.250000
and business	0.001445
business data	0.250000
example the	0.012346
the SPOTLIGHT	0.000692
SPOTLIGHT	0.000028
SPOTLIGHT system	1.000000
system developed	0.010753
developed at	0.076923
at A.C.	0.014706
A.C.	0.000028
A.C. Nielsen	1.000000
Nielsen	0.000028
Nielsen automatically	1.000000
generated readable	0.066667
readable English	0.333333
English text	0.027027
text based	0.006289
large amounts	0.043478
of retail	0.000891
retail	0.000028
retail sales	1.000000
sales	0.000084
sales data	0.333333
More recently	0.111111
recently	0.000084
recently there	0.333333
is growing	0.002033
growing interest	0.500000
summarise electronic	0.333333
electronic	0.000056
electronic medical	0.500000
medical records	0.333333
records	0.000112
records .	0.500000
<s> Commercial	0.001537
Commercial	0.000056
Commercial applications	0.500000
applications in	0.080000
area are	0.090909
are starting	0.004149
starting	0.000028
starting to	1.000000
to appear	0.002656
appear ,	0.062500
and researchers	0.001445
have shown	0.009615
shown that	0.200000
NLG summaries	0.047619
of medical	0.001783
medical data	0.333333
data can	0.025974
be effective	0.008439
effective decision-support	0.166667
decision-support	0.000028
decision-support aids	1.000000
aids	0.000028
aids for	1.000000
for medical	0.003610
medical professionals	0.166667
professionals	0.000028
professionals .	1.000000
There is	0.272727
also growing	0.014493
to enhance	0.001328
enhance	0.000028
enhance accessibility	1.000000
accessibility	0.000028
accessibility ,	1.000000
by describing	0.005714
describing graphs	0.250000
graphs	0.000028
graphs and	1.000000
sets to	0.090909
to blind	0.001328
blind	0.000112
blind people	0.750000
people .	0.125000
a highly	0.001227
highly interactive	0.111111
interactive use	0.250000
of NLG	0.000891
the WYSIWYM	0.000692
WYSIWYM	0.000028
WYSIWYM framework	1.000000
framework .	0.750000
It stands	0.026316
stands	0.000028
stands for	1.000000
for What	0.003610
What you	0.090909
you see	0.076923
see is	0.050000
what you	0.031250
you meant	0.076923
meant and	0.500000
allows users	0.250000
users to	0.222222
to see	0.002656
see and	0.050000
and manipulate	0.001445
manipulate the	0.333333
the continuously	0.000692
continuously	0.000028
continuously rendered	1.000000
rendered	0.000028
rendered view	1.000000
view -LRB-	0.333333
NLG output	0.047619
output -RRB-	0.038462
an underlying	0.007576
underlying formal	0.333333
formal language	0.222222
language document	0.006757
NLG input	0.047619
input -RRB-	0.024390
, thereby	0.000561
thereby	0.000028
thereby editing	1.000000
editing	0.000056
editing the	0.500000
the formal	0.001384
language without	0.006757
without having	0.076923
having	0.000140
having to	0.200000
learn it	0.076923
Evaluation As	0.111111
other scientific	0.014286
scientific	0.000056
scientific fields	0.500000
, NLG	0.000561
NLG researchers	0.047619
researchers need	0.100000
to test	0.002656
test how	0.100000
well their	0.035714
, modules	0.000561
modules	0.000056
modules ,	0.500000
and algorithms	0.001445
algorithms work	0.028571
are three	0.004149
three basic	0.333333
basic techniques	0.076923
techniques for	0.086957
evaluating NLG	0.200000
systems :	0.008929
: task-based	0.009804
task-based	0.000112
task-based -LRB-	0.250000
-LRB- extrinsic	0.002710
extrinsic -RRB-	0.166667
-RRB- evaluation	0.002710
: give	0.019608
give the	0.500000
the generated	0.001384
generated text	0.133333
and assess	0.001445
assess how	0.666667
well it	0.071429
it helps	0.008547
helps	0.000056
helps him	0.500000
him	0.000056
him perform	0.500000
a task	0.003681
task -LRB-	0.023810
or otherwise	0.004505
otherwise achieves	0.500000
achieves	0.000056
achieves its	0.500000
its communicative	0.028571
goal -RRB-	0.142857
system which	0.010753
which generates	0.007246
generates summaries	0.333333
evaluated by	0.142857
by giving	0.005714
giving	0.000056
giving these	0.500000
to doctors	0.001328
doctors	0.000084
doctors ,	0.333333
and assessing	0.001445
assessing	0.000028
assessing whether	1.000000
summaries helps	0.023256
helps doctors	0.500000
doctors make	0.333333
make better	0.050000
better decisions	0.111111
decisions .	0.100000
<s> human	0.000769
human ratings	0.086957
ratings :	0.111111
and ask	0.001445
ask him	0.250000
him or	0.500000
or her	0.009009
her	0.000056
her to	0.500000
to rate	0.002656
rate	0.000307
rate the	0.090909
quality and	0.100000
and usefulness	0.001445
usefulness	0.000028
usefulness of	1.000000
<s> metrics	0.000769
metrics :	0.111111
: compare	0.009804
compare generated	0.142857
generated texts	0.066667
to texts	0.001328
texts written	0.058824
by people	0.011429
people from	0.062500
using an	0.033898
automatic metric	0.043478
metric such	0.333333
as BLEU	0.003484
BLEU .	0.333333
Generally speaking	0.400000
we ultimately	0.022222
ultimately	0.000028
ultimately want	1.000000
to know	0.001328
know	0.000056
know is	0.500000
is how	0.004065
how useful	0.034483
useful NLG	0.071429
are at	0.008299
at helping	0.014706
helping	0.000028
helping people	1.000000
people ,	0.062500
first of	0.030303
above techniques	0.076923
techniques .	0.043478
, task-based	0.000561
task-based evaluations	0.750000
evaluations are	0.333333
are time-consuming	0.004149
time-consuming and	0.333333
be difficult	0.004219
to carry	0.001328
carry	0.000028
carry out	1.000000
out -LRB-	0.071429
-LRB- especially	0.005420
especially if	0.066667
if they	0.035714
require subjects	0.045455
subjects	0.000028
subjects with	1.000000
with specialised	0.005464
specialised	0.000056
specialised expertise	0.500000
expertise	0.000028
expertise ,	1.000000
as doctors	0.003484
doctors -RRB-	0.333333
<s> Hence	0.001537
Hence	0.000056
Hence -LRB-	0.500000
-RRB- task-based	0.005420
the exception	0.000692
exception	0.000028
exception ,	1.000000
the norm	0.000692
norm	0.000028
norm .	1.000000
In recent	0.019048
recent years	0.500000
years researchers	0.047619
have started	0.009615
started	0.000112
started trying	0.250000
to assess	0.001328
well human-ratings	0.035714
human-ratings	0.000028
human-ratings and	1.000000
and metrics	0.001445
metrics correlate	0.111111
with -LRB-	0.005464
-LRB- predict	0.002710
predict -RRB-	0.166667
evaluations .	0.166667
Much of	0.333333
this work	0.010989
is being	0.006098
being conducted	0.055556
of Generation	0.000891
Generation Challenges	0.500000
Challenges	0.000028
Challenges shared-task	1.000000
shared-task	0.000028
shared-task events	1.000000
events	0.000028
events .	1.000000
<s> Initial	0.000769
Initial	0.000028
Initial results	1.000000
results suggest	0.047619
suggest that	0.333333
that human	0.007092
ratings are	0.222222
are much	0.008299
much better	0.045455
better than	0.111111
than metrics	0.022222
metrics in	0.111111
this regard	0.010989
regard .	0.200000
In other	0.019048
other words	0.028571
ratings usually	0.111111
usually do	0.031250
do predict	0.038462
predict task-effectiveness	0.333333
task-effectiveness	0.000056
task-effectiveness at	0.500000
least to	0.200000
degree -LRB-	0.166667
-LRB- although	0.005420
although there	0.166667
are exceptions	0.004149
exceptions	0.000028
exceptions -RRB-	1.000000
while ratings	0.050000
ratings produced	0.111111
by metrics	0.005714
metrics often	0.111111
often do	0.022727
not predict	0.008929
task-effectiveness well	0.500000
well .	0.071429
These results	0.058824
results are	0.190476
very preliminary	0.024390
preliminary ,	0.333333
, hopefully	0.000561
hopefully	0.000028
hopefully better	1.000000
better data	0.111111
data will	0.012987
be available	0.004219
available soon	0.058824
soon .	0.333333
are currently	0.008299
currently the	0.142857
most popular	0.051724
popular evaluation	0.111111
evaluation technique	0.018519
technique in	0.142857
NLG ;	0.047619
; this	0.063830
is contrast	0.002033
contrast to	0.250000
to machine	0.003984
where metrics	0.028571
metrics are	0.111111
very widely	0.024390
widely	0.000223
widely used	0.875000
a subtopic	0.001227
subtopic	0.000028
subtopic of	1.000000
processing in	0.037037
in artificial	0.001873
that deals	0.003546
with machine	0.005464
machine reading	0.012658
comprehension .	0.428571
of disassembling	0.000891
disassembling	0.000028
disassembling and	1.000000
and parsing	0.001445
parsing input	0.035714
input is	0.024390
complex than	0.083333
than the	0.088889
the reverse	0.000692
reverse	0.000056
reverse process	0.500000
of assembling	0.000891
assembling	0.000028
assembling output	1.000000
output in	0.038462
generation because	0.111111
because of	0.200000
the occurrence	0.000692
occurrence of	0.500000
of unknown	0.000891
unknown	0.000028
unknown and	1.000000
and unexpected	0.001445
unexpected	0.000028
unexpected features	1.000000
features in	0.038462
input and	0.048780
appropriate syntactic	0.250000
syntactic and	0.153846
semantic schemes	0.047619
schemes	0.000056
schemes to	0.500000
to apply	0.001328
to it	0.001328
, factors	0.000561
factors	0.000084
factors which	0.333333
are pre-determined	0.004149
pre-determined	0.000028
pre-determined when	1.000000
when outputting	0.028571
outputting	0.000056
outputting language	0.500000
is considerable	0.002033
considerable commercial	0.200000
commercial interest	0.090909
field because	0.037037
its application	0.028571
application to	0.071429
to news-gathering	0.001328
news-gathering	0.000028
news-gathering ,	1.000000
, text	0.001684
text categorization	0.006289
categorization	0.000028
categorization ,	1.000000
, voice-activation	0.000561
voice-activation	0.000028
voice-activation ,	1.000000
, archiving	0.000561
archiving	0.000028
archiving and	1.000000
and large-scale	0.001445
large-scale	0.000028
large-scale content-analysis	1.000000
content-analysis	0.000028
content-analysis .	1.000000
<s> Eight	0.000769
Eight	0.000028
Eight years	1.000000
years after	0.047619
after John	0.083333
John McCarthy	0.125000
McCarthy	0.000028
McCarthy coined	1.000000
coined	0.000028
coined the	1.000000
term artificial	0.055556
intelligence ,	0.125000
, Bobrow	0.000561
Bobrow	0.000028
Bobrow 's	1.000000
's dissertation	0.019608
dissertation -LRB-	0.333333
-LRB- titled	0.002710
titled	0.000028
titled Natural	1.000000
Language Input	0.083333
Input	0.000056
Input for	0.500000
a Computer	0.001227
Computer	0.000167
Computer Problem	0.166667
Problem	0.000028
Problem Solving	1.000000
Solving System	0.500000
System	0.000028
System -RRB-	1.000000
-RRB- showed	0.002710
showed how	0.250000
computer can	0.022727
can understand	0.005525
understand simple	0.285714
simple natural	0.038462
solve algebra	0.250000
algebra word	0.500000
word problems	0.016667
problems .	0.176471
A year	0.020000
year later	0.166667
later	0.000279
later ,	0.500000
in 1965	0.001873
1965	0.000112
1965 ,	0.500000
, Joseph	0.000561
Weizenbaum at	0.333333
at MIT	0.029412
MIT	0.000056
MIT wrote	0.500000
wrote ELIZA	0.166667
an interactive	0.007576
interactive program	0.250000
program that	0.090909
that carried	0.003546
carried	0.000056
carried on	0.500000
a dialogue	0.002454
dialogue	0.000056
dialogue in	0.500000
in English	0.009363
English on	0.027027
any topic	0.064516
popular being	0.111111
being psychotherapy	0.055556
psychotherapy	0.000028
psychotherapy .	1.000000
<s> ELIZA	0.002306
ELIZA worked	0.111111
worked by	0.200000
by simple	0.005714
simple parsing	0.038462
parsing and	0.035714
and substitution	0.001445
of key	0.000891
key words	0.166667
into canned	0.012821
canned phrases	0.500000
and Weizenbaum	0.001445
Weizenbaum sidestepped	0.333333
sidestepped	0.000028
sidestepped the	1.000000
of giving	0.000891
giving the	0.500000
a database	0.003681
database	0.000279
database of	0.200000
of real-world	0.000891
real-world knowledge	0.166667
knowledge or	0.037037
a rich	0.003681
rich	0.000140
rich lexicon	0.600000
lexicon .	0.111111
<s> Yet	0.000769
Yet	0.000028
Yet ELIZA	1.000000
ELIZA gained	0.111111
gained	0.000056
gained surprising	0.500000
surprising	0.000028
surprising popularity	1.000000
popularity	0.000028
popularity as	1.000000
a toy	0.002454
toy	0.000056
toy project	0.500000
project and	0.076923
be seen	0.012658
seen as	0.300000
very early	0.024390
early precursor	0.100000
precursor	0.000028
precursor to	1.000000
to current	0.001328
current commercial	0.142857
commercial systems	0.090909
systems such	0.017857
as those	0.017422
those used	0.136364
by Ask.com	0.005714
Ask.com	0.000028
Ask.com .	1.000000
In 1969	0.019048
1969	0.000056
1969 Roger	0.500000
Roger Schank	0.750000
Schank at	0.200000
at Stanford	0.014706
Stanford	0.000056
Stanford University	0.500000
University introduced	0.111111
introduced	0.000056
introduced the	1.000000
the conceptual	0.000692
conceptual dependency	0.500000
dependency theory	0.200000
theory for	0.076923
This model	0.031746
, partially	0.000561
partially	0.000028
partially influenced	1.000000
the work	0.001384
work of	0.083333
of Sydney	0.000891
Sydney	0.000028
Sydney Lamb	1.000000
Lamb	0.000028
Lamb ,	1.000000
was extensively	0.012987
extensively	0.000028
extensively used	1.000000
by Schank	0.005714
Schank 's	0.200000
's students	0.019608
students at	0.666667
at Yale	0.029412
Yale	0.000056
Yale University	0.500000
University ,	0.111111
as Robert	0.003484
Robert Wilensky	0.250000
, Wendy	0.000561
Wendy	0.000028
Wendy Lehnert	1.000000
and Janet	0.001445
Janet Kolodner	0.500000
Kolodner	0.000028
Kolodner .	1.000000
In 1970	0.009524
1970 ,	0.333333
William A.	0.500000
A. Woods	0.200000
Woods	0.000028
Woods introduced	1.000000
the augmented	0.000692
augmented	0.000028
augmented transition	1.000000
transition	0.000028
transition network	1.000000
network	0.000167
network -LRB-	0.166667
-LRB- ATN	0.002710
ATN	0.000028
ATN -RRB-	1.000000
-RRB- to	0.010840
to represent	0.001328
represent natural	0.111111
input .	0.048780
of phrase	0.000891
phrase structure	0.200000
structure rules	0.083333
rules ATNs	0.023256
ATNs	0.000084
ATNs used	0.333333
used an	0.008850
an equivalent	0.007576
equivalent set	0.200000
of finite	0.000891
finite	0.000140
finite state	0.800000
state automata	0.071429
automata	0.000028
automata that	1.000000
were called	0.024390
called recursively	0.055556
recursively	0.000056
recursively .	0.500000
<s> ATNs	0.000769
ATNs and	0.333333
their more	0.029412
more general	0.031579
general format	0.045455
format	0.000056
format called	0.500000
called ``	0.277778
`` generalized	0.005291
generalized	0.000028
generalized ATNs	1.000000
ATNs ''	0.333333
'' continued	0.005155
continued	0.000251
continued to	0.333333
of years	0.000891
years .	0.190476
In 1971	0.009524
1971	0.000084
1971 Terry	0.333333
Terry	0.000028
Terry Winograd	1.000000
Winograd	0.000084
Winograd finished	0.333333
finished	0.000056
finished writing	0.500000
writing SHRDLU	0.111111
SHRDLU for	0.166667
for his	0.007220
his PhD	0.083333
PhD	0.000028
PhD thesis	1.000000
thesis	0.000028
thesis at	1.000000
MIT .	0.500000
<s> SHRDLU	0.001537
SHRDLU could	0.166667
could understand	0.062500
simple English	0.038462
English sentences	0.027027
a restricted	0.001227
restricted world	0.250000
world of	0.066667
of children	0.000891
children	0.000056
children 's	0.500000
's blocks	0.019608
blocks to	0.250000
to direct	0.001328
direct a	0.166667
a robotic	0.001227
robotic	0.000028
robotic arm	1.000000
arm	0.000028
arm to	1.000000
to move	0.001328
move	0.000028
move items	1.000000
items .	0.500000
The successful	0.005208
successful demonstration	0.111111
of SHRDLU	0.000891
SHRDLU provided	0.166667
provided significant	0.200000
significant momentum	0.111111
momentum	0.000028
momentum for	1.000000
for continued	0.003610
continued research	0.222222
<s> Winograd	0.000769
Winograd continued	0.333333
major influence	0.083333
influence	0.000028
influence in	1.000000
field with	0.037037
his book	0.083333
book Language	0.125000
Language as	0.083333
a Cognitive	0.001227
Cognitive Process	0.333333
Process	0.000028
Process .	1.000000
<s> At	0.002306
At	0.000084
At Stanford	0.333333
Stanford ,	0.500000
, Winograd	0.000561
Winograd was	0.333333
was later	0.012987
later the	0.100000
the adviser	0.000692
adviser	0.000028
adviser for	1.000000
for Larry	0.003610
Larry	0.000056
Larry Page	0.500000
Page	0.000028
Page ,	1.000000
, who	0.000561
who co-founded	0.100000
co-founded	0.000028
co-founded Google	1.000000
the 1970s	0.000692
1970s and	0.666667
and 1980s	0.002890
1980s the	0.111111
processing group	0.018519
group	0.000112
group at	0.250000
at SRI	0.014706
SRI	0.000028
SRI International	1.000000
International	0.000028
International continued	1.000000
and development	0.002890
development in	0.083333
A number	0.060000
of commercial	0.000891
commercial efforts	0.181818
efforts	0.000195
efforts based	0.142857
research were	0.023810
were undertaken	0.024390
undertaken	0.000056
undertaken ,	0.500000
in 1982	0.001873
1982 Gary	0.333333
Gary	0.000028
Gary Hendrix	1.000000
Hendrix	0.000028
Hendrix formed	1.000000
formed Symantec	0.200000
Symantec	0.000056
Symantec Corporation	0.500000
Corporation	0.000112
Corporation originally	0.250000
originally	0.000056
originally as	0.500000
a company	0.001227
company	0.000084
company for	0.333333
for developing	0.007220
developing	0.000112
developing a	0.250000
language interface	0.006757
interface	0.000112
interface for	0.250000
for database	0.003610
database queries	0.100000
queries	0.000084
queries on	0.333333
on personal	0.004717
personal	0.000112
personal computers	0.250000
computers .	0.222222
the advent	0.000692
advent	0.000028
advent of	1.000000
of mouse	0.000891
mouse	0.000028
mouse driven	1.000000
driven	0.000028
driven ,	1.000000
, graphic	0.000561
graphic	0.000028
graphic user	1.000000
user interfaces	0.142857
interfaces	0.000056
interfaces Symantec	0.500000
Symantec changed	0.500000
changed	0.000056
changed direction	0.500000
direction	0.000084
direction .	0.333333
other commercial	0.014286
efforts were	0.142857
were started	0.024390
started around	0.250000
, Larry	0.000561
Larry R.	0.500000
R. Harris	0.166667
Harris at	0.111111
the Artificial	0.000692
Artificial	0.000056
Artificial Intelligence	0.500000
Intelligence Corporation	0.333333
Corporation and	0.250000
and Roger	0.001445
Schank and	0.400000
and his	0.001445
at Cognitive	0.014706
Cognitive Systems	0.333333
Systems corp.	0.083333
corp.	0.000028
corp. .	1.000000
In 1983	0.009524
1983	0.000028
1983 ,	1.000000
Michael Dyer	0.250000
Dyer	0.000028
Dyer developed	1.000000
developed the	0.153846
the BORIS	0.000692
BORIS	0.000028
BORIS system	1.000000
system at	0.010753
Yale which	0.500000
which bore	0.007246
bore	0.000028
bore similarities	1.000000
similarities	0.000056
similarities to	0.500000
of Roger	0.000891
and W.	0.001445
W.	0.000056
W. G.	0.500000
G. Lehnart	0.500000
Lehnart	0.000028
Lehnart .	1.000000
<s> Scope	0.000769
Scope	0.000028
Scope and	1.000000
The umbrella	0.005208
umbrella	0.000028
umbrella term	1.000000
understanding ''	0.060606
a diverse	0.001227
diverse set	0.500000
computer applications	0.022727
, ranging	0.001123
ranging	0.000056
ranging from	1.000000
from small	0.019231
, relatively	0.000561
relatively	0.000028
relatively simple	1.000000
simple tasks	0.038462
as short	0.003484
short commands	0.125000
commands	0.000140
commands issued	0.200000
issued	0.000028
issued to	1.000000
to robots	0.001328
robots	0.000028
robots ,	1.000000
to highly	0.001328
highly complex	0.111111
complex endeavors	0.041667
endeavors	0.000028
endeavors such	1.000000
the full	0.002076
full comprehension	0.200000
of newspaper	0.000891
newspaper articles	0.333333
articles or	0.250000
or poetry	0.004505
poetry	0.000028
poetry passages	1.000000
passages .	0.500000
Many real	0.083333
world applications	0.066667
applications fall	0.040000
fall between	0.250000
two extremes	0.034483
extremes	0.000028
extremes ,	1.000000
for instance	0.018051
instance text	0.071429
text classification	0.006289
automatic analysis	0.043478
of emails	0.000891
emails	0.000056
emails and	0.500000
their routing	0.029412
routing	0.000084
routing to	0.333333
suitable department	0.250000
department	0.000056
department in	0.500000
a corporation	0.001227
corporation	0.000028
corporation does	1.000000
not require	0.008929
require in	0.045455
in depth	0.001873
depth	0.000084
depth understanding	0.333333
but is	0.029412
is far	0.004065
far more	0.250000
the management	0.000692
management	0.000195
management of	0.285714
of simple	0.001783
simple queries	0.038462
queries to	0.333333
to database	0.001328
database tables	0.100000
tables	0.000084
tables with	0.333333
with fixed	0.005464
fixed	0.000056
fixed schemata	0.500000
schemata	0.000028
schemata .	1.000000
<s> Throughout	0.000769
Throughout	0.000028
Throughout the	1.000000
the years	0.000692
years various	0.047619
various attempts	0.055556
at processing	0.014706
processing natural	0.018519
language or	0.013514
or English-like	0.004505
English-like	0.000084
English-like sentences	0.333333
sentences presented	0.013158
presented to	0.166667
to computers	0.001328
computers have	0.111111
have taken	0.009615
taken place	0.333333
place at	0.250000
at varying	0.014706
varying	0.000028
varying degrees	1.000000
degrees	0.000056
degrees of	0.500000
of complexity	0.000891
complexity .	0.083333
Some attempts	0.047619
attempts have	0.166667
not resulted	0.008929
resulted	0.000056
resulted in	1.000000
in systems	0.003745
systems with	0.017857
with deep	0.005464
but have	0.029412
have helped	0.009615
helped	0.000084
helped overall	0.333333
overall system	0.166667
system usability	0.010753
usability	0.000028
usability .	1.000000
, Wayne	0.000561
Wayne	0.000028
Wayne Ratliff	1.000000
Ratliff	0.000028
Ratliff originally	1.000000
originally developed	0.500000
the Vulcan	0.000692
Vulcan	0.000056
Vulcan program	0.500000
program with	0.045455
an English-like	0.007576
English-like syntax	0.333333
to mimic	0.001328
mimic	0.000028
mimic the	1.000000
the English	0.002076
English speaking	0.027027
speaking computer	0.125000
computer in	0.045455
in Star	0.001873
Star	0.000028
Star Trek	1.000000
Trek	0.000028
Trek .	1.000000
<s> Vulcan	0.000769
Vulcan later	0.500000
later became	0.100000
became the	0.200000
the dBase	0.000692
dBase	0.000028
dBase system	1.000000
system whose	0.010753
whose easy-to-use	0.333333
easy-to-use	0.000028
easy-to-use syntax	1.000000
syntax effectively	0.090909
effectively launched	0.333333
launched	0.000028
launched the	1.000000
the personal	0.000692
personal computer	0.250000
computer database	0.022727
database industry	0.100000
industry	0.000084
industry .	0.333333
Systems with	0.083333
an easy	0.007576
easy	0.000084
easy to	1.000000
or English	0.004505
English like	0.027027
like syntax	0.035714
syntax are	0.090909
are ,	0.004149
, quite	0.000561
quite distinct	0.125000
from systems	0.009615
that use	0.007092
lexicon and	0.111111
and include	0.002890
internal representation	0.600000
-LRB- often	0.005420
as first	0.003484
first order	0.030303
order logic	0.071429
logic -RRB-	0.250000
the semantics	0.001384
semantics of	0.071429
language sentences	0.006757
Hence the	0.500000
the breadth	0.000692
breadth	0.000056
breadth and	0.500000
and depth	0.001445
depth of	0.333333
`` understanding	0.005291
'' aimed	0.005155
aimed	0.000056
aimed at	1.000000
at by	0.014706
system determine	0.010753
determine both	0.043478
both the	0.064516
the implied	0.000692
implied	0.000028
implied challenges	1.000000
challenges	0.000056
challenges -RRB-	0.500000
applications it	0.040000
can deal	0.005525
deal with	0.500000
with .	0.005464
The ``	0.010417
`` breadth	0.005291
breadth ''	0.500000
'' of	0.005155
is measured	0.006098
measured	0.000167
measured by	0.500000
the sizes	0.000692
sizes	0.000084
sizes of	0.666667
its vocabulary	0.028571
and grammar	0.002890
grammar .	0.108108
`` depth	0.005291
depth ''	0.333333
the degree	0.000692
degree to	0.166667
which its	0.007246
its understanding	0.028571
understanding approximates	0.030303
approximates	0.000056
approximates that	0.500000
a fluent	0.001227
fluent	0.000028
fluent native	1.000000
At the	0.333333
the narrowest	0.000692
narrowest	0.000028
narrowest and	1.000000
and shallowest	0.001445
shallowest	0.000028
shallowest ,	1.000000
, English-like	0.000561
English-like command	0.333333
command	0.000056
command interpreters	0.500000
interpreters	0.000028
interpreters require	1.000000
require minimal	0.045455
minimal	0.000028
minimal complexity	1.000000
complexity ,	0.166667
a small	0.002454
small range	0.111111
range	0.000195
range of	0.571429
applications .	0.160000
<s> Narrow	0.000769
Narrow	0.000028
Narrow but	1.000000
but deep	0.014706
deep systems	0.142857
systems explore	0.008929
explore and	0.250000
and model	0.001445
model mechanisms	0.033333
mechanisms	0.000056
mechanisms of	0.500000
of understanding	0.000891
but they	0.044118
they still	0.025000
still have	0.066667
have limited	0.009615
limited application	0.100000
application .	0.071429
Systems that	0.333333
that attempt	0.003546
to understand	0.003984
understand the	0.285714
the contents	0.000692
contents	0.000028
contents of	1.000000
document such	0.027778
a news	0.002454
news release	0.076923
release	0.000084
release beyond	0.333333
beyond simple	0.166667
simple keyword	0.038462
keyword	0.000028
keyword matching	1.000000
matching and	0.200000
judge its	0.250000
its suitability	0.028571
suitability	0.000056
suitability for	0.500000
user are	0.071429
are broader	0.004149
broader	0.000028
broader and	1.000000
require significant	0.045455
significant complexity	0.111111
still somewhat	0.066667
somewhat	0.000056
somewhat shallow	0.500000
shallow .	0.166667
both very	0.032258
very broad	0.048780
broad	0.000112
broad and	0.250000
and very	0.001445
very deep	0.024390
deep are	0.142857
are beyond	0.004149
current state	0.142857
state of	0.357143
the art	0.001384
art	0.000056
art .	0.500000
<s> Components	0.000769
Components	0.000028
Components and	1.000000
and architecture	0.001445
architecture	0.000056
architecture Regardless	0.500000
Regardless	0.000028
Regardless of	1.000000
the approach	0.000692
approach used	0.028571
some common	0.012048
common components	0.040000
components	0.000140
components can	0.200000
identified in	0.200000
most natural	0.034483
understanding systems	0.030303
needs a	0.200000
a lexicon	0.002454
lexicon of	0.222222
language and	0.013514
a parser	0.003681
parser and	0.062500
rules to	0.069767
to break	0.001328
break	0.000056
break sentences	0.500000
The construction	0.005208
lexicon with	0.111111
suitable ontology	0.250000
ontology requires	0.500000
requires significant	0.062500
significant effort	0.111111
effort ,	0.250000
the Wordnet	0.000692
Wordnet	0.000028
Wordnet lexicon	1.000000
lexicon required	0.111111
required many	0.142857
many person-years	0.019231
person-years	0.000028
person-years of	1.000000
of effort	0.000891
effort .	0.250000
system also	0.010753
also needs	0.014493
a semantic	0.001227
semantic theory	0.095238
theory to	0.076923
to guide	0.001328
the comprehension	0.000692
The interpretation	0.005208
interpretation	0.000056
interpretation capabilities	0.500000
capabilities of	0.200000
understanding system	0.030303
system depend	0.010753
theory it	0.076923
uses .	0.071429
<s> Competing	0.000769
Competing	0.000028
Competing semantic	1.000000
semantic theories	0.047619
language have	0.006757
have specific	0.009615
specific trade	0.047619
trade	0.000056
trade offs	0.500000
offs	0.000028
offs in	1.000000
in their	0.007491
their suitability	0.029412
suitability as	0.500000
computer automated	0.022727
automated semantic	0.142857
semantic interpretation	0.047619
interpretation .	0.500000
These range	0.058824
range from	0.285714
from naive	0.009615
naive semantics	0.500000
semantics or	0.142857
or stochastic	0.004505
stochastic semantic	0.125000
semantic analysis	0.095238
analysis to	0.015385
of pragmatics	0.000891
pragmatics to	0.333333
to derive	0.001328
derive	0.000056
derive meaning	0.500000
meaning from	0.043478
from context	0.009615
<s> Advanced	0.002306
Advanced	0.000140
Advanced applications	0.200000
applications of	0.040000
understanding also	0.030303
also attempt	0.014493
to incorporate	0.001328
incorporate	0.000028
incorporate logical	1.000000
logical inference	0.166667
inference within	0.250000
within their	0.055556
their framework	0.029412
is generally	0.002033
generally achieved	0.090909
by mapping	0.005714
mapping	0.000056
mapping the	0.500000
the derived	0.000692
derived meaning	0.166667
meaning into	0.043478
of assertions	0.000891
assertions	0.000056
assertions in	0.500000
in predicate	0.001873
predicate	0.000028
predicate logic	1.000000
logic ,	0.250000
then using	0.028571
using logical	0.016949
logical deduction	0.166667
deduction	0.000028
deduction to	1.000000
to arrive	0.001328
arrive	0.000028
arrive at	1.000000
at conclusions	0.014706
conclusions	0.000028
conclusions .	1.000000
on functional	0.004717
functional	0.000056
functional languages	0.500000
as Lisp	0.003484
Lisp	0.000028
Lisp hence	1.000000
hence	0.000056
hence need	0.500000
a subsystem	0.001227
subsystem	0.000028
subsystem for	1.000000
the representation	0.000692
of logical	0.000891
logical assertions	0.166667
assertions ,	0.500000
while logic	0.050000
logic oriented	0.250000
oriented	0.000028
oriented systems	1.000000
those using	0.045455
language Prolog	0.006757
Prolog	0.000028
Prolog generally	1.000000
generally rely	0.090909
on an	0.014151
an extension	0.007576
extension	0.000028
extension of	1.000000
the built	0.000692
built in	0.333333
in logical	0.001873
logical representation	0.166667
representation framework	0.052632
The management	0.005208
of context	0.001783
context in	0.030303
understanding can	0.030303
can present	0.005525
present special	0.166667
special	0.000140
special challenges	0.200000
challenges .	0.500000
A large	0.020000
large variety	0.043478
of examples	0.000891
and counter	0.001445
counter	0.000028
counter examples	1.000000
examples have	0.041667
have resulted	0.009615
in multiple	0.001873
multiple approaches	0.076923
formal modeling	0.111111
modeling of	0.142857
context ,	0.121212
each with	0.022222
with specific	0.005464
specific strengths	0.047619
strengths and	0.500000
and weaknesses	0.001445
weaknesses	0.000028
weaknesses .	1.000000
usually abbreviated	0.031250
abbreviated	0.000028
abbreviated to	1.000000
to OCR	0.001328
OCR ,	0.142857
the mechanical	0.000692
mechanical	0.000028
mechanical or	1.000000
or electronic	0.004505
electronic conversion	0.500000
of scanned	0.000891
scanned	0.000084
scanned images	0.333333
images	0.000167
images of	0.333333
of handwritten	0.000891
handwritten	0.000056
handwritten ,	0.500000
, typewritten	0.001123
typewritten	0.000140
typewritten or	0.200000
or printed	0.004505
into machine-encoded	0.012821
machine-encoded	0.000028
machine-encoded text	1.000000
is widely	0.004065
a form	0.001227
data entry	0.038961
entry from	0.250000
from some	0.009615
some sort	0.012048
of original	0.000891
original paper	0.076923
paper data	0.090909
data source	0.012987
source ,	0.041667
, whether	0.000561
whether documents	0.076923
, sales	0.000561
sales receipts	0.333333
receipts	0.000028
receipts ,	1.000000
, mail	0.000561
mail	0.000056
mail ,	0.500000
any number	0.032258
of printed	0.001783
printed records	0.083333
is crucial	0.002033
crucial	0.000028
crucial to	1.000000
the computerization	0.000692
computerization	0.000028
computerization of	1.000000
printed texts	0.083333
texts so	0.058824
be electronically	0.004219
electronically	0.000028
electronically searched	1.000000
searched	0.000056
searched ,	1.000000
, stored	0.000561
stored	0.000028
stored more	1.000000
more compactly	0.010526
compactly	0.000028
compactly ,	1.000000
, displayed	0.000561
displayed	0.000056
displayed on-line	0.500000
on-line	0.000084
on-line ,	0.333333
and used	0.001445
machine processes	0.012658
processes	0.000140
processes such	0.200000
as machine	0.003484
mining .	0.200000
<s> OCR	0.003075
OCR is	0.061224
in pattern	0.001873
intelligence and	0.125000
and computer	0.001445
computer vision	0.022727
vision	0.000028
vision .	1.000000
<s> Early	0.001537
Early	0.000056
Early versions	0.500000
versions	0.000084
versions needed	0.333333
be programmed	0.008439
programmed	0.000056
programmed with	0.500000
with images	0.005464
each character	0.022222
character ,	0.090909
and worked	0.001445
worked on	0.400000
on one	0.009434
one font	0.015385
font	0.000084
font at	0.333333
at a	0.058824
a time	0.002454
time .	0.121212
`` Intelligent	0.005291
Intelligent	0.000084
Intelligent ''	0.333333
'' systems	0.015464
high degree	0.055556
of recognition	0.001783
recognition accuracy	0.057851
accuracy for	0.064516
for most	0.007220
most fonts	0.017241
fonts	0.000084
fonts are	0.333333
now common	0.076923
are capable	0.008299
capable	0.000056
capable of	1.000000
of reproducing	0.000891
reproducing	0.000028
reproducing formatted	1.000000
formatted	0.000028
formatted output	1.000000
that closely	0.003546
closely approximates	0.200000
approximates the	0.500000
original scanned	0.076923
scanned page	0.333333
page including	0.142857
including images	0.071429
images ,	0.333333
, columns	0.000561
columns	0.000028
columns and	1.000000
other non-textual	0.014286
non-textual	0.000028
non-textual components	1.000000
components .	0.200000
In 1914	0.009524
1914	0.000028
1914 ,	1.000000
Emanuel Goldberg	0.500000
Goldberg	0.000056
Goldberg developed	0.500000
machine that	0.012658
that read	0.003546
read characters	0.142857
characters and	0.062500
and converted	0.001445
converted	0.000084
converted them	0.333333
them into	0.052632
into standard	0.012821
standard telegraph	0.071429
telegraph	0.000028
telegraph code	1.000000
code	0.000195
code .	0.428571
-RRB- Around	0.002710
Around	0.000028
Around the	1.000000
, Edmund	0.000561
Edmund	0.000028
Edmund Fournier	1.000000
Fournier	0.000028
Fournier d'Albe	1.000000
d'Albe	0.000028
d'Albe developed	1.000000
the Optophone	0.000692
Optophone	0.000028
Optophone ,	1.000000
a handheld	0.001227
handheld	0.000028
handheld scanner	1.000000
scanner	0.000084
scanner that	0.333333
that when	0.003546
when moved	0.028571
moved	0.000028
moved across	1.000000
across a	0.200000
a printed	0.001227
printed page	0.083333
produced tones	0.111111
tones	0.000028
tones that	1.000000
that corresponded	0.003546
corresponded	0.000028
corresponded to	1.000000
to specific	0.001328
specific letters	0.047619
letters or	0.100000
or characters	0.004505
characters .	0.125000
<s> Goldberg	0.000769
Goldberg continued	0.500000
develop OCR	0.200000
OCR technology	0.183673
for data	0.007220
entry .	0.250000
<s> Later	0.000769
Later	0.000028
Later ,	1.000000
he proposed	0.142857
proposed photographing	0.111111
photographing	0.000028
photographing data	1.000000
data records	0.012987
records and	0.250000
using photocells	0.016949
photocells	0.000028
photocells ,	1.000000
, matching	0.000561
matching the	0.200000
the photos	0.000692
photos	0.000028
photos against	1.000000
against a	0.200000
a template	0.002454
template	0.000112
template containing	0.250000
containing the	0.375000
desired identification	0.200000
identification pattern	0.200000
pattern .	0.166667
In 1929	0.009524
1929	0.000028
1929 Gustav	1.000000
Gustav	0.000028
Gustav Tauschek	1.000000
Tauschek	0.000056
Tauschek had	0.500000
had similar	0.071429
similar ideas	0.037037
ideas ,	0.250000
and obtained	0.001445
obtained a	0.285714
a patent	0.002454
patent	0.000112
patent on	0.750000
on OCR	0.004717
OCR in	0.040816
Germany .	0.500000
<s> Paul	0.000769
Paul W.	0.200000
W. Handel	0.500000
Handel	0.000028
Handel also	1.000000
also obtained	0.014493
a US	0.002454
US patent	0.285714
on such	0.004717
such template-matching	0.008130
template-matching	0.000028
template-matching OCR	1.000000
technology in	0.045455
in USA	0.001873
USA	0.000028
USA in	1.000000
in 1933	0.001873
1933	0.000028
1933 -LRB-	1.000000
-LRB- U.S.	0.005420
U.S.	0.000195
U.S. Patent	0.428571
Patent	0.000084
Patent 1,915,993	0.333333
1,915,993	0.000028
1,915,993 -RRB-	1.000000
In 1935	0.009524
1935	0.000028
1935 Tauschek	1.000000
Tauschek was	0.500000
was also	0.025974
also granted	0.014493
granted	0.000028
granted a	1.000000
on his	0.004717
his method	0.083333
method -LRB-	0.062500
Patent 2,026,329	0.333333
2,026,329	0.000028
2,026,329 -RRB-	1.000000
In 1949	0.009524
1949 RCA	0.500000
RCA	0.000140
RCA engineers	0.200000
engineers	0.000028
engineers worked	1.000000
first primitive	0.030303
primitive	0.000028
primitive computer-type	1.000000
computer-type	0.000028
computer-type OCR	1.000000
OCR to	0.061224
help blind	0.111111
people for	0.062500
the US	0.001384
US Veterans	0.142857
Veterans	0.000028
Veterans Administration	1.000000
Administration	0.000028
Administration ,	1.000000
but instead	0.014706
of converting	0.001783
converting	0.000056
converting the	0.500000
the printed	0.000692
printed characters	0.083333
characters to	0.062500
machine language	0.037975
, their	0.000561
their device	0.029412
device	0.000056
device converted	0.500000
converted it	0.333333
then spoke	0.028571
spoke	0.000028
spoke the	1.000000
the letters	0.000692
letters :	0.100000
: an	0.009804
an early	0.007576
early text-to-speech	0.100000
text-to-speech technology	0.250000
technology .	0.090909
It proved	0.026316
proved	0.000084
proved far	0.333333
far too	0.125000
too expensive	0.333333
expensive and	0.142857
and was	0.001445
was not	0.025974
not pursued	0.008929
pursued	0.000028
pursued after	1.000000
after testing	0.083333
testing .	0.200000
David H.	0.250000
H. Shepard	0.500000
Shepard	0.000084
Shepard ,	0.333333
a cryptanalyst	0.001227
cryptanalyst	0.000028
cryptanalyst at	1.000000
the Armed	0.000692
Armed	0.000028
Armed Forces	1.000000
Forces	0.000028
Forces Security	1.000000
Security	0.000028
Security Agency	1.000000
Agency	0.000056
Agency in	0.500000
States ,	0.142857
, addressed	0.000561
addressed the	0.500000
converting printed	0.500000
printed messages	0.083333
messages	0.000056
messages into	0.500000
into machine	0.012821
computer processing	0.022727
processing and	0.018519
and built	0.001445
built a	0.333333
machine to	0.012658
this ,	0.043956
, called	0.000561
`` Gismo	0.010582
Gismo	0.000056
Gismo .	0.500000
. ''	0.005460
He received	0.125000
received a	0.500000
patent for	0.250000
his ``	0.083333
`` Apparatus	0.005291
Apparatus	0.000028
Apparatus for	1.000000
for Reading	0.003610
Reading	0.000056
Reading ''	0.500000
in 1953	0.001873
1953	0.000028
1953 U.S.	1.000000
Patent 2,663,758	0.333333
2,663,758	0.000028
2,663,758 .	1.000000
Gismo ''	0.500000
'' could	0.005155
could read	0.062500
read 23	0.142857
23	0.000028
23 letters	1.000000
letters of	0.200000
English alphabet	0.054054
alphabet	0.000084
alphabet ,	0.666667
, comprehend	0.000561
comprehend	0.000028
comprehend Morse	1.000000
Morse	0.000028
Morse Code	1.000000
Code	0.000028
Code ,	1.000000
, read	0.001123
read musical	0.142857
musical	0.000028
musical notations	1.000000
notations ,	0.500000
read aloud	0.142857
aloud	0.000028
aloud from	1.000000
from printed	0.009615
printed pages	0.083333
and duplicate	0.001445
duplicate typewritten	0.500000
typewritten pages	0.200000
<s> Shepard	0.000769
Shepard went	0.333333
went	0.000140
went on	0.200000
to found	0.001328
found Intelligent	0.071429
Intelligent Machines	0.333333
Machines	0.000028
Machines Research	1.000000
Research Corporation	0.125000
Corporation -LRB-	0.500000
-LRB- IMR	0.002710
IMR	0.000056
IMR -RRB-	0.500000
which soon	0.007246
soon developed	0.333333
the world	0.002076
world 's	0.066667
's first	0.019608
first commercial	0.060606
commercial OCR	0.181818
OCR systems	0.081633
In 1955	0.009524
commercial system	0.090909
was installed	0.012987
installed at	0.666667
the Reader	0.000692
Reader	0.000084
Reader 's	1.000000
's Digest	0.058824
Digest	0.000084
Digest ,	0.333333
which used	0.007246
used OCR	0.008850
input sales	0.024390
sales reports	0.333333
reports into	0.400000
It converted	0.026316
converted the	0.333333
the typewritten	0.000692
typewritten reports	0.200000
into punched	0.012821
punched	0.000028
punched cards	1.000000
cards	0.000028
cards for	1.000000
for input	0.003610
input into	0.024390
the magazine	0.000692
magazine	0.000028
magazine 's	1.000000
's subscription	0.019608
subscription	0.000028
subscription department	1.000000
department ,	0.500000
for help	0.003610
help in	0.111111
in processing	0.001873
processing the	0.018519
the shipment	0.000692
shipment	0.000028
shipment of	1.000000
of 15-20	0.000891
15-20	0.000028
15-20 million	1.000000
million	0.000084
million books	0.333333
books	0.000028
books a	1.000000
a year	0.001227
The second	0.005208
second system	0.100000
was sold	0.012987
sold	0.000084
sold to	0.333333
the Standard	0.000692
Standard	0.000056
Standard Oil	0.500000
Oil	0.000028
Oil Company	1.000000
Company	0.000056
Company for	0.500000
for reading	0.007220
reading credit	0.125000
card imprints	0.250000
imprints	0.000028
imprints for	1.000000
for billing	0.003610
billing	0.000028
billing purposes	1.000000
purposes .	0.500000
Other systems	0.142857
systems sold	0.008929
sold by	0.333333
by IMR	0.005714
IMR during	0.500000
late 1950s	0.111111
1950s included	0.250000
included a	0.125000
a bill	0.001227
bill	0.000056
bill stub	0.500000
stub	0.000028
stub reader	1.000000
reader to	0.100000
the Ohio	0.000692
Ohio	0.000028
Ohio Bell	1.000000
Bell	0.000028
Bell Telephone	1.000000
Telephone	0.000028
Telephone Company	1.000000
Company and	0.500000
a page	0.001227
page scanner	0.142857
scanner to	0.333333
States Air	0.142857
Air	0.000084
Air Force	0.666667
Force	0.000056
Force for	0.500000
and transmitting	0.001445
transmitting	0.000028
transmitting by	1.000000
by teletype	0.005714
teletype	0.000028
teletype typewritten	1.000000
typewritten messages	0.200000
messages .	0.500000
<s> IBM	0.000769
IBM and	0.333333
others were	0.083333
were later	0.024390
later licensed	0.100000
licensed	0.000028
licensed on	1.000000
on Shepard	0.004717
Shepard 's	0.333333
's OCR	0.019608
OCR patents	0.020408
patents	0.000028
patents .	1.000000
In about	0.009524
about 1965	0.025000
, Reader	0.000561
Digest and	0.333333
and RCA	0.001445
RCA collaborated	0.200000
collaborated	0.000028
collaborated to	1.000000
to build	0.001328
an OCR	0.007576
OCR Document	0.020408
Document reader	0.250000
reader designed	0.200000
to digitize	0.001328
digitize	0.000028
digitize the	1.000000
the serial	0.000692
serial	0.000028
serial numbers	1.000000
numbers on	0.142857
on Reader	0.004717
Digest coupons	0.333333
coupons	0.000028
coupons returned	1.000000
from advertisements	0.009615
advertisements	0.000056
advertisements .	1.000000
The fonts	0.005208
fonts used	0.333333
used on	0.008850
the documents	0.002076
documents were	0.026316
were printed	0.024390
printed by	0.083333
an RCA	0.015152
RCA Drum	0.200000
Drum	0.000028
Drum printer	1.000000
printer	0.000028
printer using	1.000000
the OCR-A	0.000692
OCR-A	0.000028
OCR-A font	1.000000
font .	0.666667
The reader	0.005208
reader was	0.200000
was connected	0.012987
connected directly	0.200000
directly to	0.400000
to an	0.001328
RCA 301	0.200000
301	0.000028
301 computer	1.000000
computer -LRB-	0.022727
-LRB- one	0.002710
first solid	0.030303
solid	0.000028
solid state	1.000000
state computers	0.071429
computers -RRB-	0.111111
This reader	0.015873
was followed	0.012987
a specialised	0.001227
specialised document	0.500000
document reader	0.027778
reader installed	0.100000
at TWA	0.014706
TWA	0.000028
TWA where	1.000000
reader processed	0.100000
processed Airline	0.166667
Airline	0.000028
Airline Ticket	1.000000
Ticket	0.000028
Ticket stock	1.000000
stock	0.000084
stock .	0.666667
The readers	0.005208
readers	0.000056
readers processed	0.500000
processed documents	0.166667
documents at	0.026316
a rate	0.001227
rate of	0.272727
of 1,500	0.000891
1,500	0.000028
1,500 documents	1.000000
documents per	0.026316
per	0.000112
per minute	0.250000
minute	0.000028
minute ,	1.000000
and checked	0.001445
checked each	0.500000
each document	0.022222
, rejecting	0.000561
rejecting	0.000084
rejecting those	0.333333
those it	0.045455
not able	0.008929
process correctly	0.027778
correctly	0.000028
correctly .	1.000000
product became	0.142857
became part	0.200000
the RCA	0.000692
RCA product	0.200000
product line	0.142857
line as	0.333333
a reader	0.001227
process ``	0.027778
`` Turn	0.005291
Turn	0.000028
Turn around	1.000000
around Documents	0.125000
Documents	0.000028
Documents ''	1.000000
'' such	0.005155
those utility	0.045455
utility and	0.500000
and insurance	0.001445
insurance	0.000028
insurance bills	1.000000
bills	0.000028
bills returned	1.000000
returned with	0.250000
with payments	0.005464
payments	0.000028
payments .	1.000000
The United	0.005208
States Postal	0.142857
Postal	0.000028
Postal Service	1.000000
Service	0.000028
Service has	1.000000
been using	0.029412
using OCR	0.050847
OCR machines	0.020408
machines to	0.250000
to sort	0.001328
sort mail	0.333333
mail since	0.500000
since 1965	0.100000
1965 based	0.250000
on technology	0.004717
technology devised	0.045455
devised	0.000056
devised primarily	0.500000
primarily by	0.500000
the prolific	0.000692
prolific	0.000028
prolific inventor	1.000000
inventor	0.000028
inventor Jacob	1.000000
Jacob	0.000028
Jacob Rabinow	1.000000
Rabinow	0.000028
Rabinow .	1.000000
first use	0.030303
of OCR	0.003565
in Europe	0.003745
Europe was	0.200000
the British	0.000692
British	0.000084
British General	0.333333
General	0.000028
General Post	1.000000
Post	0.000056
Post Office	0.500000
Office	0.000028
Office -LRB-	1.000000
-LRB- GPO	0.002710
GPO	0.000028
GPO -RRB-	1.000000
In 1965	0.009524
1965 it	0.250000
it began	0.008547
began planning	0.142857
planning an	0.500000
an entire	0.007576
entire banking	0.333333
banking	0.000028
banking system	1.000000
the National	0.001384
National	0.000084
National Giro	0.333333
Giro	0.000028
Giro ,	1.000000
that revolutionized	0.003546
revolutionized	0.000028
revolutionized bill	1.000000
bill payment	0.500000
payment	0.000028
payment systems	1.000000
systems in	0.008929
the UK	0.002768
UK	0.000112
UK .	0.500000
<s> Canada	0.000769
Canada Post	0.166667
Post has	0.500000
systems since	0.008929
since 1971	0.100000
1971 -LRB-	0.333333
systems read	0.008929
read the	0.142857
the name	0.001384
name and	0.200000
and address	0.001445
address of	0.250000
the addressee	0.000692
addressee	0.000028
addressee at	1.000000
first mechanized	0.030303
mechanized	0.000028
mechanized sorting	1.000000
sorting	0.000028
sorting center	1.000000
center	0.000028
center ,	1.000000
and print	0.001445
print	0.000028
print a	1.000000
a routing	0.001227
routing bar	0.333333
bar	0.000056
bar code	1.000000
code on	0.142857
the envelope	0.000692
envelope	0.000028
envelope based	1.000000
the postal	0.000692
postal	0.000028
postal code	1.000000
To avoid	0.111111
avoid	0.000028
avoid confusion	1.000000
confusion	0.000028
confusion with	1.000000
the human-readable	0.000692
human-readable	0.000028
human-readable address	1.000000
address field	0.250000
field which	0.037037
be located	0.004219
located	0.000028
located anywhere	1.000000
anywhere	0.000028
anywhere on	1.000000
the letter	0.000692
letter ,	0.166667
, special	0.000561
special ink	0.200000
ink	0.000028
ink -LRB-	1.000000
-LRB- orange	0.002710
orange	0.000028
orange in	1.000000
in visible	0.001873
visible	0.000084
visible light	0.333333
light	0.000084
light -RRB-	0.333333
used that	0.008850
is clearly	0.004065
clearly	0.000084
clearly visible	0.333333
visible under	0.333333
under ultraviolet	0.200000
ultraviolet	0.000028
ultraviolet light	1.000000
light .	0.333333
<s> Envelopes	0.000769
Envelopes	0.000028
Envelopes may	1.000000
may then	0.019231
then be	0.028571
be processed	0.004219
with equipment	0.005464
equipment based	0.333333
on simple	0.004717
simple bar	0.038462
code readers	0.142857
readers .	0.500000
<s> Importance	0.000769
Importance	0.000028
Importance of	1.000000
the Blind	0.001384
Blind	0.000056
Blind In	0.500000
In 1974	0.009524
1974	0.000028
1974 Ray	1.000000
Ray	0.000028
Ray Kurzweil	1.000000
Kurzweil	0.000195
Kurzweil started	0.142857
started the	0.250000
the company	0.000692
company Kurzweil	0.333333
Kurzweil Computer	0.285714
Computer Products	0.333333
Products	0.000056
Products ,	0.500000
, Inc.	0.001123
Inc.	0.000056
Inc. and	0.500000
and continued	0.001445
continued development	0.111111
of omni-font	0.000891
omni-font	0.000028
omni-font OCR	1.000000
which could	0.007246
could recognize	0.062500
recognize	0.000251
recognize text	0.111111
text printed	0.006289
printed in	0.083333
in virtually	0.001873
virtually	0.000056
virtually any	0.500000
any font	0.032258
He decided	0.125000
decided that	0.333333
best application	0.055556
this technology	0.010989
technology would	0.045455
be to	0.008439
a reading	0.001227
reading machine	0.125000
machine for	0.012658
the blind	0.000692
blind ,	0.250000
which would	0.014493
would allow	0.018868
allow blind	0.200000
people to	0.125000
computer read	0.022727
read text	0.142857
them out	0.052632
out loud	0.071429
loud	0.000028
loud .	1.000000
This device	0.015873
device required	0.500000
required the	0.142857
the invention	0.000692
invention	0.000028
invention of	1.000000
of two	0.001783
two enabling	0.034483
enabling	0.000028
enabling technologies	1.000000
technologies	0.000112
technologies --	0.250000
the CCD	0.000692
CCD	0.000028
CCD flatbed	1.000000
flatbed	0.000028
flatbed scanner	1.000000
scanner and	0.333333
the text-to-speech	0.000692
text-to-speech synthesizer	0.250000
synthesizer	0.000028
synthesizer .	1.000000
On January	0.166667
January 13	0.250000
13 ,	0.500000
1976 the	0.500000
the successful	0.000692
successful finished	0.111111
finished product	0.500000
product was	0.142857
was unveiled	0.012987
unveiled	0.000028
unveiled during	1.000000
a widely-reported	0.001227
widely-reported	0.000028
widely-reported news	1.000000
news conference	0.076923
conference	0.000056
conference headed	0.500000
headed	0.000028
headed by	1.000000
by Kurzweil	0.005714
Kurzweil and	0.142857
the leaders	0.000692
leaders	0.000028
leaders of	1.000000
National Federation	0.333333
Federation	0.000028
Federation of	1.000000
Blind -LRB-	0.500000
In 1978	0.009524
1978 Kurzweil	0.333333
Products began	0.500000
began selling	0.142857
selling	0.000028
selling a	1.000000
commercial version	0.090909
the optical	0.000692
optical	0.000056
optical character	1.000000
recognition computer	0.016529
<s> LexisNexis	0.000769
LexisNexis	0.000028
LexisNexis was	1.000000
was one	0.012987
first customers	0.030303
customers	0.000056
customers ,	0.500000
and bought	0.001445
bought	0.000028
bought the	1.000000
to upload	0.001328
upload	0.000028
upload paper	1.000000
paper legal	0.090909
legal and	0.333333
and news	0.001445
news documents	0.076923
documents onto	0.026316
onto	0.000028
onto its	1.000000
its nascent	0.028571
nascent	0.000028
nascent online	1.000000
online databases	0.125000
databases .	0.500000
Two years	0.428571
years later	0.142857
, Kurzweil	0.001684
Kurzweil sold	0.142857
sold his	0.333333
his company	0.083333
company to	0.333333
to Xerox	0.001328
Xerox	0.000056
Xerox ,	0.500000
which had	0.007246
had an	0.071429
an interest	0.007576
in further	0.001873
further commercializing	0.125000
commercializing	0.000028
commercializing paper-to-computer	1.000000
paper-to-computer	0.000028
paper-to-computer text	1.000000
text conversion	0.006289
conversion .	0.333333
<s> Xerox	0.000769
Xerox eventually	0.500000
eventually	0.000028
eventually spun	1.000000
spun	0.000028
spun it	1.000000
it off	0.008547
off	0.000056
off as	0.500000
as Scansoft	0.003484
Scansoft	0.000028
Scansoft ,	1.000000
which merged	0.007246
merged	0.000028
merged with	1.000000
with Nuance	0.005464
Nuance	0.000084
Nuance Communications	0.666667
Communications	0.000056
Communications -LRB-	1.000000
OCR software	0.081633
software Desktop	0.037037
Desktop	0.000028
Desktop &	1.000000
& Server	0.125000
Server	0.000028
Server OCR	1.000000
OCR Software	0.020408
Software	0.000056
Software OCR	0.500000
software and	0.037037
and ICR	0.002890
ICR	0.000084
ICR software	0.333333
software technology	0.037037
technology are	0.045455
are analytical	0.004149
analytical artificial	0.500000
intelligence systems	0.125000
that consider	0.003546
consider sequences	0.250000
of characters	0.001783
characters rather	0.062500
than whole	0.022222
whole words	0.111111
<s> Based	0.000769
Based	0.000028
Based on	1.000000
of sequential	0.000891
sequential	0.000028
sequential lines	1.000000
lines	0.000084
lines and	0.333333
and curves	0.001445
curves	0.000028
curves ,	1.000000
, OCR	0.001123
OCR and	0.020408
ICR make	0.333333
make `	0.050000
` best	0.062500
best guesses	0.055556
guesses	0.000028
guesses '	1.000000
' at	0.052632
at characters	0.014706
characters using	0.062500
using database	0.016949
database look-up	0.100000
look-up	0.000028
look-up tables	1.000000
tables to	0.333333
to closely	0.001328
closely associate	0.200000
associate	0.000056
associate or	0.500000
or match	0.004505
match the	0.333333
the strings	0.000692
strings	0.000056
strings of	1.000000
characters that	0.062500
that form	0.003546
form words	0.050000
<s> WebOCR	0.001537
WebOCR	0.000112
WebOCR &	0.750000
& OnlineOCR	0.250000
OnlineOCR	0.000084
OnlineOCR With	0.333333
With IT	0.142857
IT	0.000028
IT technology	1.000000
technology development	0.045455
development ,	0.166667
the platform	0.000692
platform	0.000056
platform for	0.500000
for people	0.007220
use software	0.013889
software has	0.037037
been changed	0.014706
changed from	0.500000
from single	0.009615
single PC	0.071429
PC	0.000112
PC platform	0.250000
platform to	0.500000
to multi-platforms	0.001328
multi-platforms	0.000028
multi-platforms such	1.000000
as PC	0.003484
PC +	0.250000
+ Web-based	0.166667
Web-based	0.000084
Web-based +	0.333333
+ Cloud	0.166667
Cloud	0.000028
Cloud Computing	1.000000
Computing +	0.500000
+ Mobile	0.166667
Mobile	0.000084
Mobile devices	0.333333
devices	0.000112
devices .	0.500000
After 30	0.333333
30	0.000084
30 years	0.666667
years development	0.095238
software started	0.037037
started to	0.250000
to adapt	0.001328
adapt	0.000028
adapt to	1.000000
new application	0.041667
application requirements	0.071429
requirements	0.000056
requirements .	0.500000
WebOCR also	0.250000
as OnlineOCR	0.003484
OnlineOCR or	0.333333
or Web-based	0.004505
Web-based OCR	0.666667
OCR service	0.040816
service ,	0.400000
, has	0.000561
been a	0.029412
new trend	0.041667
trend	0.000056
trend to	1.000000
meet larger	0.250000
larger volume	0.062500
volume and	0.250000
and larger	0.002890
larger group	0.062500
group of	0.500000
of users	0.001783
users after	0.111111
after 30	0.083333
the desktop	0.000692
desktop	0.000028
desktop OCR	1.000000
OCR .	0.020408
<s> Internet	0.000769
Internet	0.000056
Internet and	0.500000
and broadband	0.001445
broadband	0.000028
broadband technologies	1.000000
technologies have	0.250000
have made	0.009615
made WebOCR	0.062500
OnlineOCR practically	0.333333
practically	0.000028
practically available	1.000000
available to	0.058824
to both	0.003984
both individual	0.032258
individual users	0.083333
users and	0.111111
and enterprise	0.001445
enterprise	0.000028
enterprise customers	1.000000
customers .	0.500000
Since 2000	0.200000
2000 ,	0.333333
some major	0.012048
major OCR	0.166667
OCR vendors	0.020408
vendors	0.000112
vendors began	0.250000
began offering	0.142857
offering	0.000028
offering WebOCR	1.000000
& Online	0.125000
Online software	0.500000
software ,	0.037037
of new	0.000891
new entrants	0.041667
entrants	0.000028
entrants companies	1.000000
companies to	0.500000
to seize	0.001328
seize	0.000028
seize the	1.000000
the opportunity	0.000692
opportunity	0.000056
opportunity to	0.500000
develop innovative	0.200000
innovative	0.000028
innovative Web-based	1.000000
are free	0.004149
free	0.000112
free of	0.250000
of charge	0.000891
charge	0.000028
charge services	1.000000
services	0.000084
services .	0.666667
<s> Application-Oriented	0.000769
Application-Oriented	0.000056
Application-Oriented OCR	1.000000
OCR Since	0.020408
Since OCR	0.200000
technology has	0.045455
more widely	0.010526
widely applied	0.125000
to paper-intensive	0.001328
paper-intensive	0.000028
paper-intensive industry	1.000000
industry ,	0.333333
is facing	0.002033
facing	0.000028
facing more	1.000000
complex images	0.041667
images environment	0.166667
environment	0.000167
environment in	0.166667
world .	0.133333
example :	0.024691
: complicated	0.009804
complicated backgrounds	0.333333
backgrounds	0.000028
backgrounds ,	1.000000
, degraded-images	0.000561
degraded-images	0.000028
degraded-images ,	1.000000
, heavy-noise	0.000561
heavy-noise	0.000028
heavy-noise ,	1.000000
, paper	0.000561
paper skew	0.090909
skew	0.000028
skew ,	1.000000
, picture	0.000561
picture	0.000112
picture distortion	0.250000
distortion	0.000028
distortion ,	1.000000
, low-resolution	0.000561
low-resolution	0.000028
low-resolution ,	1.000000
, disturbed	0.000561
disturbed	0.000028
disturbed by	1.000000
by grid	0.005714
grid	0.000028
grid &	1.000000
& lines	0.125000
lines ,	0.333333
text image	0.006289
image consisting	0.333333
of special	0.000891
special fonts	0.200000
fonts ,	0.333333
, symbols	0.000561
symbols	0.000084
symbols ,	0.333333
, glossary	0.001123
glossary	0.000056
glossary words	0.500000
and etc.	0.001445
<s> All	0.000769
All	0.000028
All the	1.000000
the factors	0.000692
factors affect	0.333333
affect	0.000084
affect OCR	0.333333
OCR products	0.020408
products	0.000112
products '	0.250000
' stability	0.052632
stability	0.000028
stability in	1.000000
in recognition	0.003745
the major	0.001384
technology providers	0.045455
providers	0.000028
providers began	1.000000
develop dedicated	0.200000
dedicated OCR	0.333333
each for	0.022222
for special	0.003610
special types	0.200000
of images	0.000891
images .	0.166667
They combine	0.333333
combine	0.000084
combine various	0.666667
various optimization	0.055556
optimization	0.000028
optimization methods	1.000000
methods related	0.022727
the special	0.000692
special image	0.200000
image ,	0.333333
as business	0.003484
business rules	0.250000
standard expression	0.071429
expression ,	0.200000
glossary or	0.500000
or dictionary	0.004505
dictionary and	0.142857
and rich	0.001445
rich information	0.200000
information contained	0.021739
contained	0.000028
contained in	1.000000
in color	0.001873
color	0.000028
color images	1.000000
improve the	0.076923
the recognition	0.001384
Such strategy	0.125000
customize OCR	0.500000
`` Application-Oriented	0.005291
OCR ''	0.040816
`` Customized	0.005291
Customized	0.000028
Customized OCR	1.000000
, widely	0.000561
the fields	0.000692
of Business-card	0.000891
Business-card	0.000028
Business-card OCR	1.000000
, Invoice	0.000561
Invoice	0.000028
Invoice OCR	1.000000
, Screenshot	0.000561
Screenshot	0.000028
Screenshot OCR	1.000000
, ID	0.000561
ID	0.000028
ID card	1.000000
card OCR	0.250000
, Driver-license	0.000561
Driver-license	0.000028
Driver-license OCR	1.000000
OCR or	0.020408
or Auto	0.004505
Auto	0.000028
Auto plant	1.000000
plant	0.000028
plant OCR	1.000000
<s> See	0.000769
See	0.000167
See also	0.500000
also :	0.028986
: List	0.009804
List	0.000056
List of	1.000000
of optical	0.000891
recognition software	0.024793
software Current	0.037037
Current state	0.200000
technology This	0.045455
This section	0.031746
section needs	0.166667
needs additional	0.100000
additional citations	0.166667
citations	0.000084
citations for	0.333333
for verification	0.003610
verification	0.000028
verification .	1.000000
help improve	0.111111
article by	0.034483
adding citations	0.500000
citations to	0.666667
to reliable	0.001328
reliable sources	0.250000
sources	0.000167
sources .	0.333333
<s> Unsourced	0.000769
Unsourced	0.000028
Unsourced material	1.000000
material	0.000056
material may	0.500000
be challenged	0.004219
challenged	0.000028
challenged and	1.000000
and removed	0.001445
removed	0.000028
removed .	1.000000
May 2009	0.500000
2009 -RRB-	0.333333
-RRB- Commissioned	0.002710
Commissioned	0.000028
Commissioned by	1.000000
the U.S.	0.002768
U.S. Department	0.142857
Department	0.000028
Department of	1.000000
of Energy	0.000891
Energy	0.000028
Energy -LRB-	1.000000
-LRB- DOE	0.002710
DOE	0.000028
DOE -RRB-	1.000000
the Information	0.000692
Information Science	0.200000
Science	0.000056
Science Research	0.500000
Research Institute	0.125000
Institute	0.000028
Institute -LRB-	1.000000
-LRB- ISRI	0.002710
ISRI	0.000028
ISRI -RRB-	1.000000
-RRB- had	0.002710
had the	0.071429
the mission	0.000692
mission	0.000028
mission to	1.000000
to foster	0.001328
foster	0.000028
foster the	1.000000
the improvement	0.000692
improvement of	0.500000
of automated	0.000891
automated technologies	0.142857
technologies for	0.250000
for understanding	0.003610
understanding machine	0.030303
machine printed	0.012658
printed documents	0.083333
it conducted	0.008547
conducted the	0.200000
most authoritative	0.017241
authoritative	0.000028
authoritative of	1.000000
the Annual	0.000692
Annual	0.000028
Annual Test	1.000000
Test	0.000028
Test of	1.000000
OCR Accuracy	0.020408
Accuracy for	0.142857
for 5	0.003610
5	0.000056
5 consecutive	0.500000
consecutive	0.000056
consecutive years	0.500000
years in	0.047619
the mid-90s	0.000692
mid-90s	0.000028
mid-90s .	1.000000
<s> Recognition	0.001537
Recognition	0.000223
Recognition of	0.250000
of Latin-script	0.000891
Latin-script	0.000028
Latin-script ,	1.000000
typewritten text	0.200000
text is	0.025157
is still	0.008130
still not	0.066667
not 100	0.008929
100	0.000084
100 %	0.666667
% accurate	0.051282
accurate even	0.142857
even where	0.037037
where clear	0.028571
clear imaging	0.250000
imaging	0.000028
imaging is	1.000000
is available	0.002033
available .	0.176471
One study	0.076923
study based	0.250000
on recognition	0.004717
of 19th	0.000891
19th	0.000028
19th -	1.000000
- and	0.125000
and early	0.001445
early 20th-century	0.100000
20th-century	0.000028
20th-century newspaper	1.000000
newspaper pages	0.333333
pages concluded	0.142857
concluded	0.000056
concluded that	1.000000
that character-by-character	0.003546
character-by-character	0.000028
character-by-character OCR	1.000000
OCR accuracy	0.020408
for commercial	0.003610
software varied	0.037037
varied	0.000028
varied from	1.000000
from 71	0.009615
71	0.000028
71 %	1.000000
% to	0.051282
to 98	0.001328
98	0.000084
98 %	1.000000
% ;	0.025641
; total	0.021277
total accuracy	0.500000
accuracy can	0.032258
achieved only	0.100000
only by	0.052632
human review	0.021739
review	0.000084
review .	0.333333
Other areas	0.142857
areas --	0.166667
-- including	0.040000
including recognition	0.071429
of hand	0.000891
hand printing	0.071429
printing	0.000028
printing ,	1.000000
, cursive	0.000561
cursive	0.000140
cursive handwriting	0.200000
handwriting	0.000056
handwriting ,	0.500000
and printed	0.001445
other scripts	0.014286
those East	0.045455
East	0.000028
East Asian	1.000000
Asian	0.000028
Asian language	1.000000
language characters	0.006757
characters which	0.125000
which have	0.014493
many strokes	0.019231
strokes	0.000028
strokes for	1.000000
single character	0.071429
character -RRB-	0.045455
-RRB- --	0.002710
still the	0.066667
subject of	0.250000
of active	0.000891
active	0.000056
active research	0.500000
Accuracy rates	0.285714
rates	0.000223
rates can	0.125000
be measured	0.004219
measured in	0.166667
in several	0.003745
several ways	0.045455
ways ,	0.250000
and how	0.004335
how they	0.103448
are measured	0.004149
measured can	0.166667
can greatly	0.005525
greatly affect	0.142857
affect the	0.333333
the reported	0.000692
reported	0.000140
reported accuracy	0.200000
accuracy rate	0.064516
rate .	0.090909
if word	0.035714
word context	0.016667
context -LRB-	0.060606
-LRB- basically	0.002710
basically	0.000028
basically a	1.000000
not used	0.017857
to correct	0.001328
correct software	0.066667
software finding	0.037037
finding non-existent	0.200000
non-existent	0.000028
non-existent words	1.000000
a character	0.001227
character error	0.045455
error rate	0.416667
of 1	0.000891
1 %	0.500000
% -LRB-	0.076923
-LRB- 99	0.002710
99	0.000028
99 %	1.000000
% accuracy	0.102564
accuracy -RRB-	0.064516
-RRB- may	0.002710
may result	0.019231
result in	0.090909
an error	0.007576
of 5	0.000891
5 %	0.500000
-LRB- 95	0.002710
95	0.000112
95 %	1.000000
or worse	0.004505
worse	0.000028
worse if	1.000000
the measurement	0.000692
measurement	0.000056
measurement is	0.500000
on whether	0.004717
whether each	0.076923
each whole	0.022222
whole word	0.111111
word was	0.016667
was recognized	0.012987
recognized	0.000167
recognized with	0.166667
with no	0.010929
no incorrect	0.076923
incorrect	0.000084
incorrect letters	0.333333
<s> On-line	0.002306
On-line	0.000084
On-line character	0.666667
recognition is	0.074380
sometimes confused	0.076923
with Optical	0.005464
Optical Character	0.333333
Character	0.000056
Character Recognition	1.000000
Recognition -LRB-	0.125000
see Handwriting	0.050000
Handwriting	0.000028
Handwriting recognition	1.000000
recognition -RRB-	0.008264
an instance	0.007576
of off-line	0.000891
off-line	0.000028
off-line character	1.000000
system recognizes	0.010753
recognizes	0.000084
recognizes the	1.000000
the fixed	0.000692
fixed static	0.500000
static	0.000028
static shape	1.000000
shape	0.000028
shape of	1.000000
the character	0.000692
while on-line	0.050000
on-line character	0.333333
recognition instead	0.008264
instead recognizes	0.142857
the dynamic	0.000692
dynamic motion	0.200000
motion	0.000028
motion during	1.000000
during handwriting	0.100000
handwriting .	0.500000
, on-line	0.000561
on-line recognition	0.333333
as that	0.003484
that used	0.003546
for gestures	0.003610
gestures in	0.500000
the Penpoint	0.000692
Penpoint	0.000028
Penpoint OS	1.000000
OS	0.000056
OS or	0.500000
the Tablet	0.000692
Tablet	0.000056
Tablet PC	1.000000
PC can	0.250000
can tell	0.005525
tell	0.000084
tell whether	0.333333
whether a	0.153846
a horizontal	0.001227
horizontal	0.000028
horizontal mark	1.000000
mark was	0.333333
was drawn	0.012987
drawn	0.000028
drawn right-to-left	1.000000
right-to-left	0.000028
right-to-left ,	1.000000
or left-to-right	0.004505
left-to-right	0.000028
left-to-right .	1.000000
also referred	0.014493
by other	0.005714
other terms	0.014286
terms such	0.076923
as dynamic	0.003484
dynamic character	0.200000
, real-time	0.000561
real-time character	0.500000
and Intelligent	0.001445
Intelligent Character	0.333333
Recognition or	0.125000
or ICR	0.004505
ICR .	0.333333
On-line systems	0.333333
for recognizing	0.003610
recognizing hand-printed	0.200000
hand-printed	0.000112
hand-printed text	0.500000
text on	0.006289
the fly	0.000692
fly	0.000028
fly have	1.000000
have become	0.009615
become well	0.250000
well known	0.035714
as commercial	0.003484
commercial products	0.090909
products in	0.250000
in recent	0.003745
years -LRB-	0.047619
see Tablet	0.050000
PC history	0.250000
history -RRB-	0.250000
<s> Among	0.000769
Among	0.000028
Among these	1.000000
input devices	0.048780
devices for	0.250000
for personal	0.003610
personal digital	0.250000
digital assistants	0.142857
assistants	0.000028
assistants such	1.000000
those running	0.045455
running Palm	0.333333
Palm	0.000028
Palm OS	1.000000
OS .	0.500000
The Apple	0.005208
Apple	0.000028
Apple Newton	1.000000
Newton	0.000028
Newton pioneered	1.000000
pioneered this	0.333333
this product	0.010989
product .	0.142857
algorithms used	0.028571
these devices	0.023810
devices take	0.250000
the order	0.001384
, speed	0.000561
and direction	0.001445
direction of	0.333333
of individual	0.001783
individual lines	0.083333
lines segments	0.333333
segments at	0.200000
at input	0.014706
input are	0.024390
known .	0.038462
user can	0.071429
be retrained	0.004219
retrained	0.000028
retrained to	1.000000
only specific	0.026316
specific letter	0.047619
letter shapes	0.333333
shapes	0.000084
shapes .	0.333333
methods can	0.022727
in software	0.001873
software that	0.037037
that scans	0.003546
scans	0.000028
scans paper	1.000000
paper documents	0.090909
so accurate	0.033333
accurate recognition	0.142857
of hand-printed	0.001783
hand-printed documents	0.250000
documents is	0.026316
still largely	0.066667
largely	0.000140
largely an	0.200000
an open	0.015152
open problem	0.250000
rates of	0.375000
of 80	0.000891
80	0.000028
80 %	1.000000
to 90	0.001328
% on	0.025641
on neat	0.004717
neat	0.000028
neat ,	1.000000
, clean	0.000561
clean hand-printed	0.500000
hand-printed characters	0.250000
achieved ,	0.200000
that accuracy	0.003546
rate still	0.090909
still translates	0.066667
translates	0.000028
translates to	1.000000
to dozens	0.001328
dozens	0.000028
dozens of	1.000000
of errors	0.000891
errors per	0.200000
per page	0.250000
, making	0.000561
the technology	0.000692
technology useful	0.045455
useful only	0.071429
only in	0.052632
in very	0.005618
limited applications	0.100000
of cursive	0.000891
cursive text	0.200000
an active	0.007576
active area	0.500000
with recognition	0.005464
recognition rates	0.016529
rates even	0.125000
even lower	0.037037
lower	0.000140
lower than	0.200000
than that	0.044444
<s> Higher	0.000769
Higher	0.000028
Higher rates	1.000000
of general	0.000891
general cursive	0.045455
cursive script	0.400000
script	0.000112
script will	0.250000
likely not	0.062500
be possible	0.008439
possible without	0.041667
without the	0.076923
of contextual	0.000891
contextual	0.000056
contextual or	0.500000
or grammatical	0.004505
grammatical information	0.090909
, recognizing	0.000561
recognizing entire	0.200000
entire words	0.333333
dictionary is	0.142857
is easier	0.004065
easier than	0.250000
than trying	0.022222
to parse	0.005312
parse individual	0.111111
individual characters	0.083333
characters from	0.062500
from script	0.009615
script .	0.500000
<s> Reading	0.000769
Reading the	0.500000
the Amount	0.000692
Amount	0.000028
Amount line	1.000000
line of	0.333333
a cheque	0.001227
cheque	0.000028
cheque -LRB-	1.000000
always a	0.333333
a written-out	0.001227
written-out	0.000028
written-out number	1.000000
number -RRB-	0.046512
example where	0.012346
where using	0.028571
a smaller	0.001227
smaller	0.000195
smaller dictionary	0.142857
dictionary can	0.142857
can increase	0.005525
increase recognition	0.250000
rates greatly	0.125000
greatly .	0.285714
<s> Knowledge	0.000769
Knowledge of	0.500000
grammar of	0.054054
being scanned	0.055556
scanned can	0.333333
also help	0.014493
help determine	0.111111
if a	0.035714
word is	0.066667
a verb	0.007362
, allowing	0.000561
allowing greater	0.333333
greater accuracy	0.333333
The shapes	0.005208
shapes of	0.666667
individual cursive	0.083333
cursive characters	0.200000
characters themselves	0.062500
themselves	0.000112
themselves simply	0.250000
simply do	0.083333
not contain	0.008929
contain enough	0.083333
enough information	0.200000
to accurately	0.001328
accurately -LRB-	0.500000
-LRB- greater	0.002710
greater than	0.333333
than 98	0.022222
% -RRB-	0.025641
-RRB- recognize	0.002710
recognize all	0.111111
all handwritten	0.023256
handwritten cursive	0.500000
is necessary	0.004065
necessary to	0.200000
understand that	0.142857
that OCR	0.003546
basic technology	0.076923
technology also	0.045455
in advanced	0.001873
advanced scanning	0.400000
scanning	0.000056
scanning applications	0.500000
<s> Due	0.000769
Due	0.000028
Due to	1.000000
an advanced	0.007576
scanning solution	0.500000
solution	0.000028
solution can	1.000000
be unique	0.004219
unique	0.000028
unique and	1.000000
and patented	0.001445
patented	0.000028
patented and	1.000000
not easily	0.026786
easily copied	0.111111
copied despite	0.500000
despite being	0.333333
being based	0.055556
this basic	0.010989
basic OCR	0.076923
For more	0.032787
complex recognition	0.041667
recognition problems	0.008264
, intelligent	0.000561
intelligent	0.000028
intelligent character	1.000000
generally used	0.090909
as artificial	0.003484
artificial neural	0.090909
neural	0.000419
neural networks	0.533333
networks can	0.071429
made indifferent	0.062500
indifferent	0.000028
indifferent to	1.000000
both affine	0.032258
affine	0.000028
affine and	1.000000
and non-linear	0.001445
non-linear	0.000028
non-linear transformations	1.000000
transformations .	0.500000
A technique	0.020000
technique which	0.142857
is having	0.002033
having considerable	0.200000
considerable success	0.200000
success in	0.200000
recognizing difficult	0.200000
difficult words	0.035714
and character	0.001445
character groups	0.045455
groups within	0.200000
within documents	0.055556
documents generally	0.026316
generally amenable	0.090909
amenable	0.000028
amenable to	1.000000
to computer	0.001328
computer OCR	0.022727
to submit	0.001328
submit them	0.500000
them automatically	0.052632
automatically to	0.047619
to humans	0.001328
humans in	0.083333
the reCAPTCHA	0.000692
reCAPTCHA	0.000028
reCAPTCHA system	1.000000
In corpus	0.009524
, part-of-speech	0.001123
part-of-speech tagging	0.466667
tagging -LRB-	0.040000
POS tagging	0.384615
tagging or	0.080000
or POST	0.004505
POST	0.000028
POST -RRB-	1.000000
called grammatical	0.055556
grammatical tagging	0.181818
or word-category	0.004505
word-category	0.000028
word-category disambiguation	1.000000
disambiguation ,	0.100000
of marking	0.000891
marking up	0.500000
-LRB- corpus	0.002710
as corresponding	0.003484
particular part	0.076923
speech ,	0.072368
on both	0.004717
both its	0.032258
its definition	0.028571
definition ,	0.400000
its context	0.028571
context --	0.030303
-- i.e.	0.040000
i.e. relationship	0.052632
relationship with	0.166667
with adjacent	0.005464
adjacent and	0.166667
related words	0.066667
phrase ,	0.100000
, sentence	0.000561
or paragraph	0.004505
paragraph .	0.333333
A simplified	0.020000
simplified	0.000056
simplified form	0.500000
is commonly	0.004065
commonly taught	0.125000
taught to	0.333333
to school-age	0.001328
school-age	0.000028
school-age children	1.000000
children ,	0.500000
words as	0.009174
as nouns	0.003484
, verbs	0.001123
verbs	0.000140
verbs ,	0.600000
, adjectives	0.000561
adjectives ,	0.333333
, adverbs	0.000561
adverbs	0.000028
adverbs ,	1.000000
, POS	0.000561
tagging is	0.080000
now done	0.076923
using algorithms	0.016949
algorithms which	0.028571
which associate	0.007246
associate discrete	0.500000
discrete terms	0.333333
terms ,	0.076923
as hidden	0.003484
hidden	0.000223
hidden parts	0.125000
in accordance	0.001873
accordance	0.000028
accordance with	1.000000
of descriptive	0.000891
descriptive tags	0.333333
<s> POS-tagging	0.000769
POS-tagging	0.000028
POS-tagging algorithms	1.000000
algorithms fall	0.028571
into two	0.012821
two distinctive	0.034483
distinctive	0.000056
distinctive groups	0.500000
groups :	0.200000
: rule-based	0.009804
rule-based and	0.142857
and stochastic	0.001445
stochastic .	0.125000
<s> E.	0.000769
E. Brill	0.250000
Brill	0.000084
Brill 's	0.333333
's tagger	0.019608
first and	0.030303
and widely	0.001445
used English	0.008850
English POS-taggers	0.027027
POS-taggers	0.000028
POS-taggers ,	1.000000
, employs	0.000561
employs	0.000056
employs rule-based	0.500000
rule-based algorithms	0.142857
not rare	0.008929
rare --	0.250000
-- in	0.040000
as opposed	0.003484
opposed	0.000028
opposed to	1.000000
many artificial	0.019231
artificial languages	0.090909
languages -RRB-	0.040000
large percentage	0.043478
percentage	0.000028
percentage of	1.000000
of word-forms	0.000891
word-forms	0.000028
word-forms are	1.000000
are ambiguous	0.004149
ambiguous .	0.250000
, even	0.003930
even ``	0.037037
`` dogs	0.021164
dogs	0.000195
dogs ''	0.571429
usually thought	0.031250
thought of	0.666667
of as	0.001783
as just	0.003484
just a	0.222222
a plural	0.001227
plural noun	0.400000
verb :	0.076923
The sailor	0.005208
sailor	0.000140
sailor dogs	0.200000
dogs the	0.142857
the barmaid	0.000692
barmaid	0.000167
barmaid .	0.166667
<s> Performing	0.000769
Performing	0.000028
Performing grammatical	1.000000
tagging will	0.040000
will indicate	0.028571
indicate that	0.333333
verb ,	0.384615
the more	0.003460
more common	0.010526
common plural	0.040000
since one	0.100000
words must	0.009174
main verb	0.125000
the noun	0.000692
noun reading	0.071429
reading is	0.125000
is less	0.002033
less likely	0.166667
likely following	0.062500
following ``	0.066667
`` sailor	0.010582
sailor ''	0.400000
-LRB- sailor	0.005420
sailor !	0.200000
!	0.000028
! </s>	1.000000
<s> →	0.000769
→	0.000084
→ dogs	0.333333
dogs -RRB-	0.142857
<s> Semantic	0.000769
Semantic analysis	0.333333
analysis can	0.030769
can then	0.005525
then extrapolate	0.028571
extrapolate	0.000028
extrapolate that	1.000000
`` barmaid	0.010582
barmaid ''	0.333333
'' implicate	0.005155
implicate	0.000028
implicate ``	1.000000
as 1	0.003484
1 -RRB-	0.250000
the nautical	0.000692
nautical	0.000056
nautical context	0.500000
sailor →	0.200000
→ <verb>	0.333333
<verb>	0.000028
<verb> ←	1.000000
←	0.000028
← barmaid	1.000000
barmaid -RRB-	0.500000
and 2	0.001445
2 -RRB-	0.200000
-RRB- an	0.002710
an action	0.007576
action applied	0.200000
the object	0.000692
object	0.000056
object ``	0.500000
-LRB- -LRB-	0.002710
-LRB- subject	0.002710
subject -RRB-	0.125000
-RRB- dogs	0.002710
dogs →	0.142857
→ barmaid	0.333333
this context	0.021978
a nautical	0.001227
nautical term	0.500000
term meaning	0.055556
meaning ``	0.043478
`` fastens	0.005291
fastens	0.000028
fastens -LRB-	1.000000
-LRB- a	0.013550
a watertight	0.001227
watertight	0.000028
watertight barmaid	1.000000
-RRB- securely	0.002710
securely	0.000028
securely ;	1.000000
; applies	0.021277
a dog	0.001227
dog	0.000084
dog to	0.333333
to ''	0.002656
`` Dogged	0.005291
Dogged	0.000028
Dogged ''	1.000000
be either	0.004219
either an	0.100000
an adjective	0.037879
adjective or	0.428571
a past-tense	0.001227
past-tense	0.000028
past-tense verb	1.000000
verb .	0.153846
<s> Just	0.000769
Just	0.000028
Just which	1.000000
which parts	0.007246
speech a	0.006579
can represent	0.011050
represent varies	0.111111
varies	0.000056
varies greatly	1.000000
<s> Trained	0.000769
Trained	0.000028
Trained linguists	1.000000
linguists can	0.333333
can identify	0.005525
grammatical parts	0.090909
speech to	0.019737
to various	0.001328
various fine	0.055556
fine	0.000056
fine degrees	0.500000
degrees depending	0.500000
the tagging	0.001384
tagging system	0.040000
<s> Schools	0.000769
Schools	0.000028
Schools commonly	1.000000
commonly teach	0.125000
teach	0.000028
teach that	1.000000
are 9	0.004149
9	0.000028
9 parts	1.000000
speech in	0.013158
English :	0.027027
: noun	0.009804
, article	0.001684
, adjective	0.000561
adjective ,	0.142857
, preposition	0.000561
preposition	0.000084
preposition ,	1.000000
, pronoun	0.000561
pronoun	0.000028
pronoun ,	1.000000
, adverb	0.000561
adverb	0.000028
adverb ,	1.000000
, conjunction	0.000561
conjunction	0.000084
conjunction ,	0.333333
and interjection	0.001445
interjection	0.000028
interjection .	1.000000
are clearly	0.004149
clearly many	0.333333
many more	0.019231
more categories	0.010526
categories	0.000251
categories and	0.222222
and sub-categories	0.001445
sub-categories	0.000028
sub-categories .	1.000000
For nouns	0.016393
, plural	0.000561
, possessive	0.000561
possessive	0.000028
possessive ,	1.000000
and singular	0.001445
singular	0.000112
singular forms	0.250000
forms can	0.166667
be distinguished	0.004219
distinguished	0.000028
distinguished .	1.000000
many languages	0.019231
languages words	0.020000
also marked	0.014493
marked for	0.666667
for their	0.007220
their ``	0.029412
`` case	0.005291
case ''	0.058824
-LRB- role	0.002710
role as	0.250000
as subject	0.003484
, object	0.000561
object ,	0.500000
, grammatical	0.000561
grammatical gender	0.090909
gender	0.000028
gender ,	1.000000
on ;	0.004717
; while	0.021277
while verbs	0.050000
verbs are	0.200000
are marked	0.004149
for tense	0.003610
tense ,	0.500000
, aspect	0.000561
aspect	0.000056
aspect ,	0.500000
things .	0.333333
In part-of-speech	0.009524
tagging by	0.040000
computer ,	0.022727
is typical	0.002033
typical to	0.111111
distinguish from	0.200000
from 50	0.009615
50	0.000084
50 to	0.333333
to 150	0.001328
150	0.000056
150 separate	0.500000
separate parts	0.100000
for English	0.003610
, NN	0.000561
NN	0.000028
NN for	1.000000
for singular	0.007220
singular common	0.250000
common nouns	0.080000
, NNS	0.000561
NNS	0.000028
NNS for	1.000000
for plural	0.003610
plural common	0.200000
, NP	0.000561
NP	0.000028
NP for	1.000000
singular proper	0.250000
proper nouns	0.142857
nouns -LRB-	0.111111
POS tags	0.153846
tags used	0.333333
Corpus -RRB-	0.187500
<s> Work	0.001537
Work	0.000056
Work on	0.500000
on stochastic	0.004717
stochastic methods	0.125000
methods for	0.045455
for tagging	0.003610
tagging Koine	0.040000
Koine	0.000028
Koine Greek	1.000000
Greek	0.000084
Greek -LRB-	0.333333
-LRB- DeRose	0.002710
DeRose	0.000140
DeRose 1990	0.200000
1990	0.000084
1990 -RRB-	0.666667
has used	0.011905
used over	0.008850
over 1,000	0.083333
1,000	0.000056
1,000 parts	0.500000
and found	0.001445
that about	0.003546
about as	0.025000
as many	0.006969
many words	0.038462
words were	0.018349
were ambiguous	0.024390
ambiguous there	0.083333
there as	0.025000
A morphosyntactic	0.020000
morphosyntactic	0.000028
morphosyntactic descriptor	1.000000
descriptor	0.000028
descriptor in	1.000000
of morphologically	0.000891
morphologically	0.000028
morphologically rich	1.000000
rich languages	0.200000
languages can	0.040000
be expressed	0.012658
expressed	0.000167
expressed like	0.166667
like Ncmsan	0.035714
Ncmsan	0.000028
Ncmsan ,	1.000000
means Category	0.166667
Category	0.000056
Category =	0.500000
= Noun	0.111111
Noun	0.000028
Noun ,	1.000000
, Type	0.000561
Type	0.000028
Type =	1.000000
= common	0.111111
common ,	0.080000
, Gender	0.000561
Gender	0.000028
Gender =	1.000000
= masculine	0.111111
masculine	0.000028
masculine ,	1.000000
, Number	0.000561
Number	0.000028
Number =	1.000000
= singular	0.111111
singular ,	0.250000
, Case	0.000561
Case	0.000028
Case =	1.000000
= accusative	0.111111
accusative	0.000028
accusative ,	1.000000
, Animate	0.000561
Animate	0.000028
Animate =	1.000000
= no.	0.111111
no.	0.000028
no. .	1.000000
<s> History	0.001537
History	0.000056
History The	0.500000
The Brown	0.015625
Corpus Research	0.062500
Research on	0.125000
on part-of-speech	0.004717
tagging has	0.040000
been closely	0.014706
closely tied	0.200000
tied	0.000028
tied to	1.000000
to corpus	0.001328
linguistics .	0.050000
first major	0.060606
major corpus	0.083333
English for	0.027027
computer analysis	0.022727
analysis was	0.030769
was the	0.051948
Corpus developed	0.062500
at Brown	0.029412
Brown University	0.142857
University by	0.111111
by Henry	0.005714
Henry Kucera	0.500000
Kucera	0.000028
Kucera and	1.000000
and Nelson	0.001445
Nelson	0.000028
Nelson Francis	1.000000
Francis	0.000028
Francis ,	1.000000
the mid-1960s	0.000692
mid-1960s	0.000028
mid-1960s .	1.000000
It consists	0.026316
of about	0.000891
about 1,000,000	0.025000
1,000,000	0.000028
1,000,000 words	1.000000
of running	0.000891
running English	0.333333
English prose	0.027027
prose	0.000028
prose text	1.000000
, made	0.001123
made up	0.062500
up of	0.045455
500 samples	0.500000
samples	0.000056
samples from	0.500000
from randomly	0.009615
randomly	0.000028
randomly chosen	1.000000
chosen publications	0.200000
publications	0.000028
publications .	1.000000
Each sample	0.166667
sample is	0.333333
is 2,000	0.002033
2,000	0.000056
2,000 or	0.500000
more words	0.010526
-LRB- ending	0.002710
ending	0.000028
ending at	1.000000
first sentence-end	0.030303
sentence-end	0.000028
sentence-end after	1.000000
after 2,000	0.083333
2,000 words	0.500000
the corpus	0.000692
corpus contains	0.032258
contains only	0.100000
only complete	0.026316
complete	0.000028
complete sentences	1.000000
Corpus was	0.125000
was painstakingly	0.012987
painstakingly	0.000028
painstakingly ``	1.000000
`` tagged	0.010582
tagged	0.000084
tagged ''	0.666667
with part-of-speech	0.005464
part-of-speech markers	0.066667
markers	0.000084
markers over	0.333333
over many	0.083333
many years	0.019231
A first	0.020000
first approximation	0.030303
approximation was	0.166667
was done	0.012987
done with	0.181818
a program	0.004908
program by	0.045455
by Greene	0.005714
Greene	0.000028
Greene and	1.000000
and Rubin	0.001445
Rubin	0.000028
Rubin ,	1.000000
which consisted	0.007246
consisted	0.000028
consisted of	1.000000
a huge	0.001227
huge	0.000028
huge handmade	1.000000
handmade	0.000028
handmade list	1.000000
what categories	0.031250
categories could	0.111111
could co-occur	0.062500
co-occur at	0.500000
all .	0.023256
article then	0.034483
then noun	0.028571
noun can	0.071429
can occur	0.005525
occur	0.000140
occur ,	0.200000
but article	0.014706
article verb	0.034483
-LRB- arguably	0.002710
arguably	0.000056
arguably -RRB-	0.500000
-RRB- can	0.008130
not .	0.017857
The program	0.005208
program got	0.045455
got	0.000028
got about	1.000000
about 70	0.050000
70 %	0.750000
% correct	0.025641
Its results	0.500000
results were	0.047619
were repeatedly	0.024390
repeatedly	0.000028
repeatedly reviewed	1.000000
reviewed	0.000028
reviewed and	1.000000
and corrected	0.001445
corrected	0.000028
corrected by	1.000000
and later	0.001445
later users	0.100000
users sent	0.111111
sent	0.000028
sent in	1.000000
in errata	0.001873
errata	0.000028
errata ,	1.000000
that by	0.003546
late 70s	0.111111
70s	0.000028
70s the	1.000000
tagging was	0.080000
was nearly	0.012987
nearly	0.000056
nearly perfect	0.500000
perfect	0.000028
perfect -LRB-	1.000000
-LRB- allowing	0.002710
allowing for	0.333333
cases on	0.055556
which even	0.007246
even human	0.037037
human speakers	0.021739
speakers might	0.250000
not agree	0.008929
agree	0.000084
agree -RRB-	0.333333
corpus has	0.032258
for innumerable	0.003610
innumerable	0.000028
innumerable studies	1.000000
studies of	0.250000
of word-frequency	0.000891
word-frequency	0.000028
word-frequency and	1.000000
and of	0.001445
part-of-speech ,	0.066667
and inspired	0.001445
inspired	0.000028
inspired the	1.000000
similar ``	0.037037
'' corpora	0.005155
corpora in	0.090909
Statistics derived	0.333333
analyzing it	0.200000
it formed	0.008547
formed the	0.200000
most later	0.017241
later part-of-speech	0.100000
tagging systems	0.040000
as CLAWS	0.003484
CLAWS	0.000112
CLAWS -LRB-	0.250000
and VOLSUNGA	0.001445
VOLSUNGA	0.000028
VOLSUNGA .	1.000000
by this	0.005714
time -LRB-	0.060606
-LRB- 2005	0.002710
2005	0.000028
2005 -RRB-	1.000000
-RRB- it	0.002710
been superseded	0.014706
superseded	0.000028
superseded by	1.000000
by larger	0.005714
larger corpora	0.062500
corpora such	0.090909
the 100	0.000692
100 million	0.333333
million word	0.333333
word British	0.016667
British National	0.333333
National Corpus	0.333333
Corpus .	0.062500
For some	0.016393
some time	0.024096
was considered	0.012987
considered an	0.111111
an inseparable	0.007576
inseparable	0.000028
inseparable part	1.000000
are certain	0.004149
certain cases	0.142857
cases where	0.166667
speech can	0.013158
be decided	0.004219
decided without	0.333333
without understanding	0.076923
or even	0.022523
even the	0.074074
the pragmatics	0.000692
pragmatics of	0.333333
extremely expensive	0.250000
especially because	0.066667
because analyzing	0.033333
analyzing the	0.200000
the higher	0.000692
higher levels	0.285714
levels is	0.045455
much harder	0.045455
harder when	0.142857
when multiple	0.028571
multiple part-of-speech	0.076923
part-of-speech possibilities	0.066667
possibilities	0.000140
possibilities must	0.200000
considered for	0.111111
<s> Use	0.000769
Use	0.000056
Use of	0.500000
of Hidden	0.000891
Hidden	0.000167
Hidden Markov	1.000000
Markov Models	0.166667
Models	0.000084
Models In	0.333333
the mid	0.000692
mid	0.000028
mid 1980s	1.000000
, researchers	0.001123
researchers in	0.100000
Europe began	0.200000
use hidden	0.013889
hidden Markov	0.875000
models -LRB-	0.115385
-LRB- HMMs	0.005420
HMMs	0.000223
HMMs -RRB-	0.250000
disambiguate parts	0.333333
when working	0.028571
working to	0.142857
to tag	0.001328
tag the	0.062500
the Lancaster-Oslo-Bergen	0.000692
Lancaster-Oslo-Bergen	0.000028
Lancaster-Oslo-Bergen Corpus	1.000000
Corpus of	0.062500
of British	0.000891
British English	0.333333
<s> HMMs	0.002306
HMMs involve	0.125000
involve counting	0.166667
counting	0.000028
counting cases	1.000000
cases -LRB-	0.055556
as from	0.003484
and making	0.002890
making a	0.142857
a table	0.003681
table of	0.428571
the probabilities	0.002768
probabilities of	0.272727
of certain	0.000891
certain sequences	0.142857
sequences .	0.222222
, once	0.000561
once	0.000028
once you	1.000000
you 've	0.153846
've	0.000056
've seen	0.500000
seen an	0.200000
an article	0.015152
article such	0.034483
as `	0.003484
` the	0.062500
the '	0.000692
perhaps the	0.166667
next word	0.285714
noun 40	0.071429
40	0.000056
40 %	1.000000
adjective 40	0.142857
% ,	0.051282
number 20	0.023256
20	0.000028
20 %	1.000000
<s> Knowing	0.000769
Knowing	0.000028
Knowing this	1.000000
program can	0.045455
can decide	0.005525
decide that	0.250000
`` can	0.005291
can ''	0.011050
in ``	0.005618
the can	0.000692
more likely	0.010526
noun than	0.071429
a modal	0.001227
modal	0.000028
modal .	1.000000
The same	0.010417
same method	0.040000
method can	0.062500
can of	0.005525
of course	0.001783
course	0.000084
course be	0.333333
to benefit	0.001328
benefit	0.000084
benefit from	1.000000
from knowledge	0.009615
about following	0.025000
following words	0.066667
More advanced	0.111111
advanced -LRB-	0.200000
`` higher	0.005291
higher order	0.142857
order ''	0.071429
-RRB- HMMs	0.002710
HMMs learn	0.125000
learn the	0.076923
probabilities not	0.090909
only of	0.026316
of pairs	0.001783
pairs	0.000056
pairs ,	1.000000
but triples	0.014706
triples	0.000084
triples or	0.333333
even larger	0.037037
larger sequences	0.062500
So ,	0.333333
've just	0.500000
just seen	0.111111
next item	0.142857
item	0.000028
item may	1.000000
be very	0.012658
very likely	0.024390
likely a	0.062500
a preposition	0.001227
or noun	0.004505
but much	0.014706
much less	0.045455
likely another	0.062500
another verb	0.076923
When several	0.142857
several ambiguous	0.045455
ambiguous words	0.166667
words occur	0.009174
occur together	0.200000
together ,	0.125000
the possibilities	0.000692
possibilities multiply	0.200000
multiply	0.000028
multiply .	1.000000
is easy	0.002033
to enumerate	0.001328
enumerate	0.000028
enumerate every	1.000000
every	0.000084
every combination	0.333333
combination and	0.200000
a relative	0.001227
relative probability	0.333333
probability to	0.142857
by multiplying	0.005714
multiplying	0.000028
multiplying together	1.000000
together the	0.125000
each choice	0.022222
choice in	0.125000
turn .	0.166667
The combination	0.005208
highest probability	0.333333
probability is	0.142857
then chosen	0.028571
chosen .	0.200000
The European	0.005208
European group	0.333333
group developed	0.250000
developed CLAWS	0.038462
CLAWS ,	0.500000
a tagging	0.001227
tagging program	0.040000
that did	0.007092
did	0.000140
did exactly	0.200000
exactly this	0.333333
and achieved	0.001445
achieved accuracy	0.200000
accuracy in	0.032258
the 93-95	0.000692
93-95	0.000028
93-95 %	1.000000
% range	0.025641
range .	0.142857
worth remembering	0.500000
remembering	0.000028
remembering ,	1.000000
as Eugene	0.003484
Eugene	0.000028
Eugene Charniak	1.000000
Charniak	0.000028
Charniak points	1.000000
points out	0.500000
out in	0.142857
in Statistical	0.001873
Statistical techniques	0.111111
language parsing	0.013514
parsing ,	0.071429
that merely	0.003546
merely assigning	0.500000
assigning	0.000028
assigning the	1.000000
common tag	0.040000
tag to	0.062500
each known	0.022222
known word	0.038462
word and	0.016667
the tag	0.001384
tag ``	0.062500
`` proper	0.005291
proper noun	0.142857
noun ''	0.071429
all unknowns	0.023256
unknowns	0.000028
unknowns ,	1.000000
, will	0.001123
will approach	0.028571
approach 90	0.028571
accuracy because	0.032258
because many	0.033333
are unambiguous	0.004149
unambiguous	0.000056
unambiguous .	0.500000
<s> CLAWS	0.001537
CLAWS pioneered	0.250000
pioneered the	0.333333
of HMM-based	0.000891
HMM-based	0.000084
HMM-based part	0.333333
but was	0.014706
was quite	0.012987
quite expensive	0.125000
expensive since	0.142857
it enumerated	0.008547
enumerated	0.000028
enumerated all	1.000000
all possibilities	0.023256
possibilities .	0.200000
It sometimes	0.026316
sometimes had	0.076923
had to	0.071429
to resort	0.001328
resort	0.000028
resort to	1.000000
to backup	0.001328
backup	0.000028
backup methods	1.000000
methods when	0.022727
when there	0.028571
there were	0.075000
were simply	0.024390
simply too	0.083333
many -LRB-	0.019231
Corpus contains	0.062500
contains a	0.100000
a case	0.001227
case with	0.058824
with 17	0.005464
17	0.000028
17 ambiguous	1.000000
a row	0.001227
row	0.000028
row ,	1.000000
are words	0.004149
words such	0.018349
`` still	0.005291
still ''	0.066667
represent as	0.111111
many as	0.019231
as 7	0.003484
7 distinct	0.142857
distinct parts	0.142857
speech -RRB-	0.026316
HMMs underlie	0.125000
underlie	0.000028
underlie the	1.000000
the functioning	0.000692
functioning	0.000084
functioning of	0.333333
of stochastic	0.000891
stochastic taggers	0.125000
taggers and	0.285714
in various	0.005618
various algorithms	0.055556
algorithms one	0.028571
most widely	0.017241
used being	0.008850
being the	0.055556
the bi-directional	0.000692
bi-directional	0.000028
bi-directional inference	1.000000
inference algorithm	0.250000
<s> Dynamic	0.002306
Dynamic	0.000140
Dynamic Programming	0.200000
Programming	0.000084
Programming methods	0.333333
methods In	0.022727
In 1987	0.009524
1987 ,	0.666667
, Steven	0.000561
Steven	0.000028
Steven DeRose	1.000000
DeRose and	0.200000
and Ken	0.001445
Ken	0.000028
Ken Church	1.000000
Church	0.000084
Church independently	0.333333
independently	0.000028
independently developed	1.000000
developed dynamic	0.038462
dynamic programming	0.200000
programming	0.000140
programming algorithms	0.200000
solve the	0.250000
same problem	0.040000
problem in	0.090909
in vastly	0.001873
vastly	0.000028
vastly less	1.000000
less time	0.083333
<s> Their	0.001537
Their	0.000056
Their methods	0.500000
were similar	0.024390
the Viterbi	0.002768
Viterbi	0.000112
Viterbi algorithm	1.000000
algorithm known	0.035714
known for	0.038462
time in	0.030303
other fields	0.014286
fields .	0.166667
<s> DeRose	0.001537
DeRose used	0.200000
while Church	0.050000
Church used	0.333333
of triples	0.000891
triples and	0.333333
of estimating	0.000891
estimating	0.000028
estimating the	1.000000
the values	0.000692
values for	0.125000
for triples	0.003610
triples that	0.333333
were rare	0.024390
rare or	0.250000
or nonexistent	0.004505
nonexistent	0.000028
nonexistent in	1.000000
Corpus -LRB-	0.062500
-LRB- actual	0.002710
actual measurement	0.200000
measurement of	0.500000
of triple	0.000891
triple	0.000028
triple probabilities	1.000000
probabilities would	0.090909
much larger	0.090909
<s> Both	0.001537
Both	0.000084
Both methods	0.333333
methods achieved	0.022727
accuracy over	0.032258
over 95	0.083333
DeRose 's	0.400000
's 1990	0.019608
1990 dissertation	0.333333
dissertation at	0.333333
University included	0.111111
included analyses	0.125000
analyses of	0.200000
specific error	0.047619
error types	0.083333
types ,	0.071429
, probabilities	0.000561
probabilities ,	0.090909
other related	0.014286
related data	0.066667
and replicated	0.001445
replicated	0.000028
replicated his	1.000000
his work	0.083333
work for	0.041667
for Greek	0.003610
Greek ,	0.333333
where it	0.028571
it proved	0.008547
proved similarly	0.333333
similarly	0.000028
similarly effective	1.000000
effective .	0.333333
These findings	0.058824
findings	0.000028
findings were	1.000000
were surprisingly	0.024390
surprisingly disruptive	0.333333
disruptive	0.000028
disruptive to	1.000000
The accuracy	0.010417
accuracy reported	0.032258
reported was	0.200000
was higher	0.012987
higher than	0.142857
the typical	0.000692
typical accuracy	0.111111
of very	0.001783
very sophisticated	0.024390
sophisticated algorithms	0.285714
that integrated	0.003546
integrated part	0.333333
speech choice	0.006579
choice with	0.125000
with many	0.005464
many higher	0.019231
linguistic analysis	0.062500
: syntax	0.009804
, DeRose	0.000561
's and	0.019608
and Church	0.001445
Church 's	0.333333
's methods	0.019608
methods did	0.022727
did fail	0.200000
fail for	0.333333
known cases	0.038462
where semantics	0.028571
semantics is	0.071429
required ,	0.142857
but those	0.014706
those proved	0.045455
proved negligibly	0.333333
negligibly	0.000028
negligibly rare	1.000000
This convinced	0.015873
convinced	0.000028
convinced many	1.000000
many in	0.019231
that part-of-speech	0.003546
tagging could	0.040000
could usefully	0.062500
usefully	0.000028
usefully be	1.000000
be separated	0.004219
separated out	0.333333
out from	0.071429
other levels	0.014286
of processing	0.000891
processing ;	0.018519
this in	0.010989
turn simplified	0.166667
simplified the	0.500000
theory and	0.076923
and practice	0.001445
practice	0.000056
practice of	0.500000
of computerized	0.001783
computerized	0.000056
computerized language	0.500000
language analysis	0.006757
and encouraged	0.001445
encouraged	0.000028
encouraged researchers	1.000000
researchers to	0.100000
find ways	0.076923
ways to	0.125000
to separate	0.001328
separate out	0.100000
out other	0.071429
other pieces	0.014286
pieces	0.000028
pieces as	1.000000
<s> Markov	0.000769
Models are	0.333333
now the	0.076923
the standard	0.001384
standard method	0.071429
method for	0.125000
for part-of-speech	0.007220
part-of-speech assignment	0.066667
assignment	0.000056
assignment .	0.500000
Unsupervised taggers	0.166667
taggers The	0.142857
methods already	0.022727
already discussed	0.200000
discussed involve	0.142857
involve working	0.166667
working from	0.142857
a pre-existing	0.001227
pre-existing corpus	0.500000
corpus to	0.032258
learn tag	0.076923
tag probabilities	0.062500
to bootstrap	0.001328
bootstrap	0.000028
bootstrap using	1.000000
using ``	0.016949
`` unsupervised	0.005291
unsupervised ''	0.125000
'' tagging	0.005155
tagging .	0.080000
Unsupervised tagging	0.166667
tagging techniques	0.040000
techniques use	0.043478
use an	0.013889
an untagged	0.007576
untagged	0.000028
untagged corpus	1.000000
corpus for	0.032258
their training	0.029412
the tagset	0.000692
tagset	0.000028
tagset by	1.000000
by induction	0.005714
<s> That	0.002306
That	0.000084
That is	1.000000
they observe	0.025000
observe	0.000028
observe patterns	1.000000
patterns in	0.200000
in word	0.001873
word use	0.016667
and derive	0.001445
derive part-of-speech	0.500000
part-of-speech categories	0.066667
categories themselves	0.111111
themselves .	0.250000
, statistics	0.001123
statistics readily	0.125000
readily reveal	0.333333
reveal	0.000028
reveal that	1.000000
the ''	0.000692
`` a	0.005291
a ''	0.001227
`` an	0.005291
an ''	0.007576
'' occur	0.005155
occur in	0.400000
in similar	0.001873
similar contexts	0.037037
contexts ,	0.285714
while ``	0.050000
`` eat	0.005291
eat	0.000028
eat ''	1.000000
'' occurs	0.005155
occurs	0.000084
occurs in	1.000000
different ones	0.020408
ones .	0.200000
With sufficient	0.142857
sufficient iteration	0.200000
iteration	0.000028
iteration ,	1.000000
, similarity	0.000561
similarity classes	0.100000
words emerge	0.009174
emerge	0.000028
emerge that	1.000000
are remarkably	0.004149
remarkably	0.000028
remarkably similar	1.000000
to those	0.002656
those human	0.045455
human linguists	0.021739
linguists would	0.333333
would expect	0.018868
expect	0.000084
expect ;	0.333333
the differences	0.000692
differences themselves	0.333333
themselves sometimes	0.250000
sometimes suggest	0.076923
suggest valuable	0.333333
valuable	0.000056
valuable new	0.500000
new insights	0.041667
insights	0.000028
insights .	1.000000
These two	0.058824
two categories	0.034483
categories can	0.111111
be further	0.004219
further subdivided	0.125000
subdivided	0.000028
subdivided into	1.000000
into rule-based	0.012821
rule-based ,	0.142857
, stochastic	0.000561
and neural	0.002890
neural approaches	0.066667
Other taggers	0.142857
and methods	0.001445
methods Some	0.022727
current major	0.142857
major algorithms	0.083333
tagging include	0.040000
, Brill	0.000561
Brill Tagger	0.333333
Tagger	0.000028
Tagger ,	1.000000
, Constraint	0.000561
Constraint	0.000028
Constraint Grammar	1.000000
Grammar	0.000028
Grammar ,	1.000000
the Baum-Welch	0.000692
Baum-Welch	0.000028
Baum-Welch algorithm	1.000000
algorithm -LRB-	0.035714
the forward-backward	0.000692
forward-backward	0.000028
forward-backward algorithm	1.000000
algorithm -RRB-	0.035714
<s> Hidden	0.002306
Markov model	0.444444
model and	0.033333
and visible	0.001445
visible Markov	0.333333
model taggers	0.033333
taggers can	0.142857
can both	0.005525
both be	0.032258
be implemented	0.008439
implemented using	0.200000
The Brill	0.005208
Brill tagger	0.333333
tagger is	0.111111
is unusual	0.002033
unusual	0.000028
unusual in	1.000000
in that	0.003745
it learns	0.008547
learns	0.000028
learns a	1.000000
of patterns	0.000891
patterns ,	0.200000
then applies	0.028571
applies those	0.142857
those patterns	0.045455
patterns rather	0.200000
than optimizing	0.022222
optimizing	0.000028
optimizing a	1.000000
statistical quantity	0.030303
quantity	0.000084
quantity .	0.333333
Many machine	0.083333
learning methods	0.023256
have also	0.009615
of POS	0.001783
Methods such	0.250000
as SVM	0.003484
SVM	0.000028
SVM ,	1.000000
, Maximum	0.000561
entropy classifier	0.200000
classifier ,	0.142857
, Perceptron	0.000561
Perceptron	0.000028
Perceptron ,	1.000000
and Nearest-neighbor	0.001445
Nearest-neighbor	0.000028
Nearest-neighbor have	1.000000
have all	0.009615
all been	0.023256
been tried	0.029412
tried ,	0.333333
and most	0.001445
most can	0.017241
can achieve	0.005525
achieve	0.000056
achieve accuracy	0.500000
accuracy above	0.032258
above 95	0.076923
A direct	0.020000
direct comparison	0.166667
comparison of	0.333333
several methods	0.045455
methods is	0.022727
is reported	0.002033
reported -LRB-	0.200000
-LRB- with	0.008130
with references	0.005464
-RRB- at	0.002710
at .	0.014706
This comparison	0.015873
comparison uses	0.333333
uses the	0.071429
Penn tag	0.222222
tag set	0.312500
set on	0.025641
Treebank data	0.166667
are directly	0.004149
directly comparable	0.200000
comparable	0.000028
comparable .	1.000000
many significant	0.019231
significant taggers	0.111111
taggers are	0.142857
not included	0.008929
included -LRB-	0.125000
-LRB- perhaps	0.002710
perhaps because	0.166667
the labor	0.000692
labor involved	0.500000
involved in	0.166667
in reconfiguring	0.001873
reconfiguring	0.000028
reconfiguring them	1.000000
them for	0.052632
this particular	0.010989
particular dataset	0.076923
dataset	0.000028
dataset -RRB-	1.000000
it should	0.008547
should not	0.052632
be assumed	0.004219
assumed	0.000028
assumed that	1.000000
results reported	0.047619
reported there	0.200000
best that	0.111111
achieved with	0.200000
given approach	0.083333
approach ;	0.028571
; nor	0.021277
nor	0.000028
nor even	1.000000
been achieved	0.029412
approach .	0.057143
<s> Issues	0.001537
Issues	0.000056
Issues While	0.500000
While there	0.200000
is broad	0.002033
broad agreement	0.250000
agreement about	0.333333
about basic	0.025000
basic categories	0.076923
categories ,	0.111111
of edge	0.000891
edge cases	0.333333
cases make	0.055556
make it	0.100000
to settle	0.001328
settle	0.000028
settle on	1.000000
single ``	0.071429
`` correct	0.005291
correct ''	0.066667
'' set	0.005155
of tags	0.000891
tags ,	0.333333
even in	0.074074
single language	0.071429
language such	0.006757
is hard	0.002033
to say	0.003984
say whether	0.142857
whether ``	0.076923
`` fire	0.005291
fire	0.000056
fire ''	0.500000
is functioning	0.002033
functioning as	0.666667
noun in	0.071429
the big	0.000692
big	0.000056
big green	0.500000
green	0.000028
green fire	1.000000
fire truck	0.500000
truck	0.000028
truck A	1.000000
A second	0.020000
second important	0.100000
important example	0.062500
example is	0.012346
the use\/mention	0.000692
use\/mention	0.000028
use\/mention distinction	1.000000
distinction ,	0.200000
following example	0.133333
where ``	0.028571
`` blue	0.010582
blue	0.000056
blue ''	1.000000
clearly not	0.333333
not functioning	0.008929
adjective -LRB-	0.142857
Corpus tag	0.125000
set appends	0.025641
appends	0.000028
appends the	1.000000
the suffix	0.000692
suffix	0.000028
suffix ''	1.000000
- NC	0.062500
NC	0.000028
NC ''	1.000000
such cases	0.016260
cases -RRB-	0.055556
: the	0.019608
word ``	0.016667
'' has	0.010309
has 4	0.011905
4 letters	0.200000
Words in	0.250000
language other	0.006757
other than	0.014286
`` main	0.005291
main ''	0.125000
'' text	0.005155
, are	0.001123
are commonly	0.004149
commonly tagged	0.125000
tagged as	0.333333
`` foreign	0.005291
foreign	0.000056
foreign ''	0.500000
usually in	0.093750
in addition	0.005618
a tag	0.001227
tag for	0.062500
the role	0.001384
role the	0.250000
the foreign	0.000692
foreign word	0.500000
is actually	0.004065
actually playing	0.333333
playing	0.000028
playing in	1.000000
also many	0.014493
where POS	0.028571
POS categories	0.076923
`` words	0.005291
words ''	0.018349
'' do	0.005155
not map	0.008929
map one	0.500000
to one	0.002656
: David	0.009804
David 's	0.250000
's gonna	0.019608
gonna	0.000028
gonna do	1.000000
do n't	0.076923
n't	0.000112
n't vice	0.250000
vice	0.000028
vice versa	1.000000
versa	0.000028
versa first-cut	1.000000
first-cut	0.000028
first-cut can	1.000000
not pre	0.008929
pre	0.000028
pre -	1.000000
and post-secondary	0.001445
post-secondary	0.000028
post-secondary look	1.000000
look -LRB-	0.200000
word -RRB-	0.016667
-RRB- up	0.002710
up In	0.045455
the last	0.002076
last example	0.200000
`` look	0.005291
look ''	0.200000
`` up	0.005291
up ''	0.045455
'' arguably	0.005155
arguably function	0.500000
function as	0.125000
single verbal	0.071429
verbal	0.000028
verbal unit	1.000000
unit ,	0.333333
, despite	0.000561
despite the	0.333333
words coming	0.009174
coming	0.000028
coming between	1.000000
between them	0.051282
them .	0.105263
Some tag	0.047619
tag sets	0.250000
sets -LRB-	0.090909
as Penn	0.003484
Penn -RRB-	0.111111
-RRB- break	0.002710
break hyphenated	0.500000
hyphenated	0.000028
hyphenated words	1.000000
, contractions	0.000561
contractions	0.000056
contractions ,	0.500000
and possessives	0.001445
possessives	0.000028
possessives into	1.000000
separate tokens	0.100000
tokens ,	0.142857
, thus	0.001123
thus avoiding	0.100000
avoiding	0.000056
avoiding some	0.500000
some but	0.012048
but far	0.014706
far from	0.125000
from all	0.009615
all such	0.023256
such problems	0.008130
is unclear	0.002033
unclear	0.000028
unclear whether	1.000000
whether it	0.076923
is best	0.002033
to treat	0.001328
treat	0.000056
treat words	0.500000
`` be	0.010582
be ''	0.008439
`` have	0.005291
have ''	0.009615
`` do	0.005291
do ''	0.038462
as categories	0.003484
categories in	0.111111
their own	0.029412
own right	0.166667
right -LRB-	0.100000
or as	0.009009
as simply	0.003484
simply verbs	0.083333
verbs -LRB-	0.200000
the LOB	0.000692
LOB	0.000056
LOB Corpus	1.000000
Corpus and	0.125000
Treebank -RRB-	0.166667
has more	0.011905
more forms	0.010526
forms than	0.166667
than other	0.022222
other English	0.014286
English verbs	0.027027
and occurs	0.001445
in quite	0.001873
different grammatical	0.020408
grammatical contexts	0.090909
, complicating	0.000561
complicating	0.000028
complicating the	1.000000
issue .	0.250000
popular ``	0.111111
'' for	0.005155
for POS	0.003610
tagging for	0.040000
for American	0.003610
American English	0.200000
English is	0.027027
is probably	0.002033
probably the	0.250000
set ,	0.051282
, developed	0.000561
Treebank project	0.166667
project .	0.076923
is largely	0.002033
largely similar	0.200000
the earlier	0.000692
earlier	0.000112
earlier Brown	0.250000
and LOB	0.001445
sets ,	0.090909
though much	0.100000
much smaller	0.045455
smaller .	0.142857
, tag	0.000561
sets from	0.090909
the Eagles	0.000692
Eagles	0.000028
Eagles Guidelines	1.000000
Guidelines	0.000056
Guidelines see	0.500000
see wide	0.050000
wide use	0.250000
include versions	0.037037
versions for	0.333333
for multiple	0.003610
multiple languages	0.076923
<s> POS	0.000769
tagging work	0.040000
of languages	0.000891
the set	0.001384
used varies	0.008850
greatly with	0.142857
with language	0.005464
<s> Tags	0.000769
Tags	0.000028
Tags usually	1.000000
usually are	0.031250
are designed	0.004149
include overt	0.037037
overt	0.000028
overt morphological	1.000000
morphological distinctions	0.333333
distinctions -LRB-	0.500000
-LRB- this	0.002710
this makes	0.010989
sets for	0.090909
for heavily	0.003610
heavily	0.000028
heavily inflected	1.000000
inflected	0.000056
inflected languages	1.000000
as Greek	0.003484
Greek and	0.333333
and Latin	0.001445
Latin	0.000112
Latin very	0.250000
very large	0.024390
large ;	0.043478
and makes	0.001445
makes tagging	0.125000
tagging words	0.040000
in agglutinative	0.001873
agglutinative	0.000028
agglutinative languages	1.000000
an Inuit	0.007576
Inuit	0.000028
Inuit virtually	1.000000
virtually impossible	0.500000
impossible .	0.500000
, Petrov	0.000561
Petrov	0.000028
Petrov ,	1.000000
, D.	0.000561
D. Das	0.200000
Das	0.000028
Das ,	1.000000
and R.	0.001445
R. McDonald	0.166667
McDonald	0.000028
McDonald -LRB-	1.000000
`` A	0.005291
A Universal	0.020000
Universal	0.000028
Universal Part-of-Speech	1.000000
Part-of-Speech	0.000028
Part-of-Speech Tagset	1.000000
Tagset	0.000028
Tagset ''	1.000000
'' http:\/\/arxiv.org\/abs\/1104.2086	0.005155
http:\/\/arxiv.org\/abs\/1104.2086	0.000028
http:\/\/arxiv.org\/abs\/1104.2086 -RRB-	1.000000
have proposed	0.009615
universal ''	0.333333
'' tag	0.005155
with 12	0.005464
12 categories	0.200000
categories -LRB-	0.111111
, no	0.001684
no subtypes	0.076923
subtypes	0.000028
subtypes of	1.000000
of nouns	0.000891
, punctuation	0.000561
punctuation ,	0.285714
etc. ;	0.045455
; no	0.021277
no distinction	0.076923
distinction of	0.200000
an infinitive	0.007576
infinitive	0.000028
infinitive marker	1.000000
marker	0.000028
marker vs.	1.000000
vs. preposition	0.083333
Whether a	0.500000
small set	0.111111
broad tags	0.250000
larger set	0.062500
more precise	0.010526
precise ones	0.333333
is preferable	0.002033
preferable	0.000028
preferable ,	1.000000
, depends	0.000561
purpose at	0.200000
at hand	0.014706
hand .	0.071429
Automatic tagging	0.111111
easier on	0.125000
on smaller	0.004717
smaller tag-sets	0.142857
tag-sets	0.000028
tag-sets .	1.000000
A different	0.040000
different issue	0.020408
issue is	0.125000
cases are	0.055556
in fact	0.001873
fact ambiguous	0.090909
<s> Beatrice	0.000769
Beatrice	0.000028
Beatrice Santorini	1.000000
Santorini	0.000028
Santorini gives	1.000000
gives	0.000056
gives examples	0.500000
examples in	0.041667
`` Part-of-speech	0.005291
Part-of-speech Tagging	0.500000
Tagging	0.000028
Tagging Guidelines	1.000000
Guidelines for	0.500000
Treebank Project	0.166667
Project	0.000028
Project ,	1.000000
, ''	0.001684
-LRB- 3rd	0.002710
3rd	0.000028
3rd rev	1.000000
rev	0.000028
rev ,	1.000000
, June	0.000561
June	0.000028
June 1990	1.000000
including the	0.071429
following -LRB-	0.066667
-LRB- p.	0.002710
p.	0.000028
p. 32	1.000000
32	0.000028
32 -RRB-	1.000000
-RRB- case	0.002710
case in	0.058824
which entertaining	0.007246
entertaining	0.000056
entertaining can	0.500000
can function	0.005525
function either	0.125000
is no	0.002033
no evident	0.076923
evident	0.000056
evident way	0.500000
decide :	0.250000
The Duchess	0.005208
Duchess	0.000028
Duchess was	1.000000
was entertaining	0.012987
entertaining last	0.500000
last night	0.200000
night	0.000028
night .	1.000000
In computer	0.009524
science and	0.100000
, parsing	0.001684
or ,	0.004505
more formally	0.010526
formally	0.000056
formally ,	0.500000
syntactic analysis	0.153846
of analyzing	0.000891
analyzing a	0.200000
a sequence	0.007362
sequence	0.000223
sequence of	0.875000
tokens -LRB-	0.142857
, words	0.001123
its grammatical	0.028571
grammatical structure	0.090909
structure with	0.083333
given -LRB-	0.041667
less -RRB-	0.083333
-RRB- formal	0.002710
formal grammar	0.222222
Parsing can	0.200000
linguistic term	0.062500
term ,	0.055556
instance when	0.071429
when discussing	0.057143
discussing	0.000056
discussing how	0.500000
how phrases	0.034483
phrases are	0.062500
are divided	0.004149
divided up	0.333333
in garden	0.001873
garden	0.000028
garden path	1.000000
path	0.000056
path sentences	0.500000
Parsing is	0.400000
also an	0.014493
an earlier	0.007576
earlier term	0.250000
the diagramming	0.001384
diagramming	0.000056
diagramming of	1.000000
still used	0.066667
of inflected	0.000891
the Romance	0.000692
Romance	0.000028
Romance languages	1.000000
languages or	0.020000
or Latin	0.004505
Latin .	0.250000
The term	0.020833
term parsing	0.055556
parsing comes	0.035714
from Latin	0.009615
Latin pars	0.250000
pars	0.000028
pars -LRB-	1.000000
-LRB- ōrātiōnis	0.002710
ōrātiōnis	0.000028
ōrātiōnis -RRB-	1.000000
, meaning	0.000561
meaning part	0.043478
part -LRB-	0.037037
-LRB- of	0.005420
a common	0.002454
common term	0.040000
term used	0.111111
in psycholinguistics	0.001873
psycholinguistics when	0.500000
describing language	0.250000
language comprehension	0.006757
parsing refers	0.035714
refers	0.000140
refers to	1.000000
the way	0.002768
human beings	0.021739
beings	0.000028
beings ,	1.000000
, rather	0.001123
than computers	0.022222
computers ,	0.222222
, analyze	0.000561
analyze a	0.250000
or phrase	0.004505
phrase -LRB-	0.100000
-LRB- in	0.005420
in spoken	0.003745
spoken language	0.142857
text -RRB-	0.006289
-RRB- ``	0.002710
of grammatical	0.000891
grammatical constituents	0.090909
constituents	0.000056
constituents ,	0.500000
, identifying	0.001684
the parts	0.000692
syntactic relations	0.076923
etc. ''	0.045455
'' This	0.005155
This term	0.015873
term is	0.055556
common when	0.040000
discussing what	0.500000
what linguistic	0.031250
linguistic cues	0.062500
cues	0.000028
cues help	1.000000
help speakers	0.111111
speakers to	0.250000
parse garden-path	0.111111
garden-path	0.000028
garden-path sentences	1.000000
The parser	0.005208
parser often	0.062500
often uses	0.022727
separate lexical	0.100000
lexical analyser	0.076923
analyser	0.000028
analyser to	1.000000
create tokens	0.058824
tokens from	0.142857
the sequence	0.000692
input characters	0.024390
<s> Parsers	0.001537
Parsers	0.000056
Parsers may	0.500000
programmed by	0.500000
hand or	0.142857
or may	0.009009
be -LRB-	0.004219
-LRB- semi	0.002710
semi	0.000028
semi -	1.000000
- -RRB-	0.062500
-RRB- automatically	0.002710
generated -LRB-	0.066667
some programming	0.012048
programming languages	0.600000
-RRB- by	0.002710
tool .	0.500000
Human languages	0.200000
languages See	0.020000
: Category	0.009804
Category :	0.500000
: Natural	0.009804
parsing In	0.035714
some machine	0.012048
and natural	0.005780
human languages	0.021739
languages are	0.020000
are parsed	0.004149
parsed	0.000112
parsed by	0.750000
Human sentences	0.200000
easily parsed	0.111111
by programs	0.005714
programs ,	0.181818
as there	0.003484
is substantial	0.002033
substantial ambiguity	0.200000
ambiguity in	0.125000
whose usage	0.333333
usage	0.000028
usage is	1.000000
convey meaning	0.333333
meaning -LRB-	0.043478
or semantics	0.004505
semantics -RRB-	0.071429
-RRB- amongst	0.002710
amongst	0.000028
amongst a	1.000000
a potentially	0.001227
potentially unlimited	0.333333
unlimited	0.000028
unlimited range	1.000000
of possibilities	0.000891
possibilities but	0.200000
but only	0.014706
are germane	0.004149
germane	0.000028
germane to	1.000000
particular case	0.076923
So an	0.333333
an utterance	0.015152
utterance	0.000084
utterance ``	0.333333
`` Man	0.010582
Man	0.000056
Man bites	0.500000
bites	0.000084
bites dog	0.333333
dog ''	0.333333
'' versus	0.005155
versus	0.000028
versus ``	1.000000
`` Dog	0.005291
Dog	0.000028
Dog bites	1.000000
bites man	0.333333
man	0.000028
man ''	1.000000
is definite	0.002033
definite	0.000028
definite on	1.000000
one detail	0.015385
detail	0.000056
detail but	0.500000
but in	0.029412
language might	0.006757
might appear	0.038462
appear as	0.062500
Man dog	0.500000
dog bites	0.333333
bites ''	0.333333
a reliance	0.001227
reliance	0.000028
reliance on	1.000000
the larger	0.000692
larger context	0.062500
context to	0.030303
between those	0.025641
those two	0.045455
two possibilities	0.034483
possibilities ,	0.200000
if indeed	0.035714
indeed	0.000084
indeed that	0.333333
that difference	0.003546
difference was	0.250000
was of	0.012987
of concern	0.000891
concern	0.000028
concern .	1.000000
is difficult	0.008130
to prepare	0.001328
prepare	0.000028
prepare formal	1.000000
formal rules	0.111111
describe informal	0.166667
informal behavior	0.500000
behavior	0.000056
behavior even	0.500000
even though	0.074074
though it	0.200000
is clear	0.002033
clear that	0.250000
some rules	0.012048
rules are	0.023256
are being	0.008299
being followed	0.055556
followed .	0.250000
In order	0.019048
parse natural	0.111111
language data	0.006757
researchers must	0.100000
must first	0.071429
first agree	0.030303
agree on	0.333333
grammar to	0.027027
The choice	0.005208
choice of	0.250000
syntax is	0.090909
is affected	0.002033
affected	0.000028
affected by	1.000000
by both	0.005714
both linguistic	0.032258
linguistic and	0.062500
and computational	0.001445
computational concerns	0.100000
concerns ;	0.500000
; for	0.042553
instance some	0.071429
some parsing	0.012048
parsing systems	0.035714
systems use	0.053571
use lexical	0.027778
lexical functional	0.076923
functional grammar	0.500000
parsing for	0.035714
for grammars	0.003610
grammars of	0.071429
type is	0.142857
be NP-complete	0.004219
NP-complete	0.000028
NP-complete .	1.000000
<s> Head-driven	0.000769
Head-driven	0.000028
Head-driven phrase	1.000000
structure grammar	0.083333
grammar is	0.054054
another linguistic	0.076923
linguistic formalism	0.062500
formalism	0.000028
formalism which	1.000000
been popular	0.014706
popular in	0.111111
parsing community	0.035714
community	0.000028
community ,	1.000000
but other	0.014706
other research	0.014286
research efforts	0.023810
efforts have	0.571429
have focused	0.019231
on less	0.004717
less complex	0.083333
complex formalisms	0.041667
formalisms	0.000056
formalisms such	0.500000
the one	0.000692
one used	0.015385
Shallow parsing	0.500000
parsing aims	0.035714
aims to	0.666667
find only	0.076923
the boundaries	0.002076
boundaries of	0.090909
of major	0.000891
major constituents	0.083333
constituents such	0.500000
as noun	0.003484
noun phrases	0.071429
Another popular	0.076923
popular strategy	0.111111
strategy for	0.200000
for avoiding	0.003610
avoiding linguistic	0.500000
linguistic controversy	0.062500
controversy	0.000028
controversy is	1.000000
is dependency	0.002033
dependency grammar	0.200000
grammar parsing	0.027027
parsing .	0.107143
<s> Most	0.001537
Most	0.000056
Most modern	0.500000
modern parsers	0.200000
parsers are	0.153846
least partly	0.200000
partly	0.000028
partly statistical	1.000000
statistical ;	0.030303
; that	0.021277
they rely	0.025000
data which	0.012987
has already	0.011905
been annotated	0.014706
annotated -LRB-	0.500000
-LRB- parsed	0.002710
hand -RRB-	0.071429
approach allows	0.028571
to gather	0.001328
gather	0.000028
gather information	1.000000
the frequency	0.000692
frequency	0.000056
frequency with	0.500000
with which	0.005464
which various	0.007246
various constructions	0.055556
constructions	0.000028
constructions occur	1.000000
in specific	0.003745
specific contexts	0.047619
-LRB- See	0.010840
See machine	0.166667
Approaches which	0.333333
used include	0.008850
include straightforward	0.037037
straightforward	0.000028
straightforward PCFGs	1.000000
PCFGs	0.000028
PCFGs -LRB-	1.000000
-LRB- probabilistic	0.002710
probabilistic context-free	0.142857
context-free	0.000307
context-free grammars	0.363636
, maximum	0.000561
entropy ,	0.200000
neural nets	0.066667
nets	0.000028
nets .	1.000000
Most of	0.500000
successful systems	0.111111
lexical statistics	0.076923
statistics -LRB-	0.125000
-LRB- that	0.010840
they consider	0.025000
the identities	0.000692
identities	0.000028
identities of	1.000000
words involved	0.009174
involved ,	0.166667
as their	0.006969
their part	0.029412
However such	0.027027
such systems	0.008130
are vulnerable	0.004149
vulnerable	0.000028
vulnerable to	1.000000
to overfitting	0.001328
overfitting and	0.500000
require some	0.045455
of smoothing	0.000891
smoothing	0.000028
smoothing to	1.000000
-RRB- Parsing	0.002710
Parsing algorithms	0.200000
language can	0.006757
grammar having	0.027027
having `	0.200000
` nice	0.062500
nice '	0.250000
' properties	0.052632
properties as	0.250000
as with	0.006969
with manually	0.005464
manually designed	0.250000
designed grammars	0.142857
grammars for	0.071429
for programming	0.003610
mentioned earlier	0.333333
earlier some	0.250000
some grammar	0.012048
grammar formalisms	0.027027
formalisms are	0.500000
parse computationally	0.111111
computationally	0.000056
computationally ;	0.500000
; in	0.021277
even if	0.111111
desired structure	0.200000
structure is	0.083333
not context-free	0.008929
context-free ,	0.090909
of context-free	0.000891
context-free approximation	0.090909
first pass	0.030303
pass	0.000028
pass .	1.000000
<s> Algorithms	0.001537
Algorithms	0.000056
Algorithms which	0.500000
which use	0.007246
use context-free	0.013889
grammars often	0.071429
often rely	0.022727
some variant	0.012048
variant	0.000028
variant of	1.000000
the CKY	0.000692
CKY	0.000028
CKY algorithm	1.000000
usually with	0.062500
some heuristic	0.012048
heuristic to	0.333333
to prune	0.001328
prune	0.000028
prune away	1.000000
away	0.000056
away unlikely	0.500000
unlikely	0.000028
unlikely analyses	1.000000
analyses to	0.200000
to save	0.001328
save	0.000028
save time	1.000000
See chart	0.166667
chart	0.000028
chart parsing	1.000000
However some	0.027027
systems trade	0.008929
trade speed	0.500000
speed for	0.142857
for accuracy	0.003610
accuracy using	0.032258
using ,	0.016949
, linear-time	0.000561
linear-time	0.000028
linear-time versions	1.000000
versions of	0.333333
the shift-reduce	0.000692
shift-reduce	0.000028
shift-reduce algorithm	1.000000
A somewhat	0.020000
somewhat recent	0.500000
recent development	0.125000
development has	0.083333
been parse	0.014706
parse reranking	0.111111
reranking	0.000028
reranking in	1.000000
parser proposes	0.062500
proposes	0.000028
proposes some	1.000000
some large	0.012048
of analyses	0.000891
analyses ,	0.200000
system selects	0.010753
selects the	0.500000
best option	0.055556
option	0.000028
option .	1.000000
<s> Programming	0.001537
Programming languages	0.666667
languages The	0.020000
parser is	0.187500
is as	0.002033
compiler or	0.333333
or interpreter	0.009009
interpreter	0.000056
interpreter .	0.500000
This parses	0.015873
parses the	0.500000
source code	0.041667
code of	0.142857
computer programming	0.022727
programming language	0.200000
create some	0.058824
of internal	0.000891
languages tend	0.020000
tend	0.000056
tend to	1.000000
be specified	0.004219
specified	0.000028
specified in	1.000000
a context-free	0.003681
context-free grammar	0.454545
grammar because	0.027027
because fast	0.033333
fast	0.000028
fast and	1.000000
and efficient	0.002890
efficient parsers	0.333333
parsers can	0.076923
be written	0.004219
written for	0.038462
Parsers are	0.500000
are written	0.004149
or generated	0.004505
generated by	0.066667
by parser	0.005714
parser generators	0.062500
generators .	0.500000
<s> Context-free	0.000769
Context-free	0.000028
Context-free grammars	1.000000
grammars are	0.071429
are limited	0.004149
limited in	0.100000
extent to	0.250000
which they	0.014493
express all	0.200000
the requirements	0.000692
requirements of	0.500000
<s> Informally	0.000769
Informally	0.000028
Informally ,	1.000000
the reason	0.000692
reason	0.000112
reason is	0.250000
the memory	0.000692
memory of	0.500000
is limited	0.002033
limited .	0.200000
grammar can	0.027027
not remember	0.008929
remember	0.000028
remember the	1.000000
the presence	0.000692
presence	0.000028
presence of	1.000000
a construct	0.001227
construct over	0.333333
over an	0.083333
an arbitrarily	0.007576
arbitrarily	0.000028
arbitrarily long	1.000000
long input	0.500000
input ;	0.024390
language in	0.006757
which ,	0.007246
a name	0.002454
name must	0.200000
be declared	0.004219
declared before	0.500000
before it	0.166667
it may	0.017094
be referenced	0.004219
referenced	0.000028
referenced .	1.000000
More powerful	0.111111
powerful	0.000028
powerful grammars	1.000000
grammars that	0.071429
express this	0.200000
this constraint	0.010989
constraint	0.000028
constraint ,	1.000000
be parsed	0.004219
parsed efficiently	0.250000
efficiently	0.000028
efficiently .	1.000000
common strategy	0.040000
a relaxed	0.001227
relaxed	0.000028
relaxed parser	1.000000
parser for	0.062500
grammar which	0.054054
which accepts	0.007246
accepts	0.000056
accepts a	0.500000
a superset	0.001227
superset	0.000028
superset of	1.000000
desired language	0.200000
language constructs	0.006757
constructs	0.000084
constructs -LRB-	0.333333
it accepts	0.008547
accepts some	0.500000
some invalid	0.012048
invalid	0.000028
invalid constructs	1.000000
constructs -RRB-	0.333333
; later	0.021277
the unwanted	0.000692
unwanted	0.000028
unwanted constructs	1.000000
constructs can	0.333333
filtered out	0.333333
out .	0.071429
of process	0.000891
process Flow	0.027778
Flow	0.000028
Flow of	1.000000
typical parser	0.111111
parser The	0.125000
example demonstrates	0.012346
demonstrates	0.000028
demonstrates the	1.000000
the common	0.000692
common case	0.040000
of parsing	0.001783
parsing a	0.035714
computer language	0.022727
language with	0.006757
with two	0.010929
two levels	0.034483
of grammar	0.001783
grammar :	0.027027
: lexical	0.009804
lexical and	0.153846
and syntactic	0.002890
syntactic .	0.076923
first stage	0.030303
stage is	0.400000
the token	0.000692
token	0.000112
token generation	0.250000
generation ,	0.111111
lexical analysis	0.076923
by which	0.005714
input character	0.024390
character stream	0.045455
stream is	0.500000
is split	0.004065
split	0.000112
split into	0.500000
into meaningful	0.025641
meaningful symbols	0.125000
symbols defined	0.333333
defined by	0.166667
a grammar	0.002454
of regular	0.000891
regular	0.000028
regular expressions	1.000000
expressions .	0.666667
a calculator	0.002454
calculator	0.000056
calculator program	0.500000
program would	0.045455
at an	0.014706
input such	0.024390
`` 12	0.010582
12 \*	0.400000
\*	0.000112
\* -LRB-	0.250000
-LRB- 3	0.005420
3	0.000140
3 +4	0.200000
+4	0.000028
+4 -RRB-	1.000000
-RRB- ^	0.002710
^	0.000084
^ 2	0.333333
2 ''	0.200000
and split	0.001445
split it	0.250000
the tokens	0.001384
tokens 12	0.142857
12 ,	0.200000
, \*	0.000561
\* ,	0.500000
-LRB- ,	0.002710
, 3	0.000561
3 ,	0.200000
, +	0.001123
+ ,	0.333333
, 4	0.000561
4 ,	0.200000
, -RRB-	0.000561
, ^	0.001123
^ ,	0.666667
, 2	0.000561
2 ,	0.200000
meaningful symbol	0.125000
symbol in	0.250000
an arithmetic	0.007576
arithmetic	0.000028
arithmetic expression	1.000000
expression .	0.200000
The lexer	0.005208
lexer	0.000028
lexer would	1.000000
would contain	0.018868
contain rules	0.083333
to tell	0.001328
tell it	0.333333
it that	0.008547
the characters	0.000692
characters \*	0.062500
and -RRB-	0.001445
-RRB- mark	0.002710
mark the	0.333333
new token	0.041667
token ,	0.250000
so meaningless	0.033333
meaningless	0.000028
meaningless tokens	1.000000
tokens like	0.142857
like ``	0.107143
\* ''	0.250000
or ''	0.004505
3 ''	0.200000
'' will	0.005155
be generated	0.004219
The next	0.005208
next stage	0.285714
is parsing	0.002033
parsing or	0.071429
or syntactic	0.004505
is checking	0.002033
checking	0.000028
checking that	1.000000
tokens form	0.142857
form an	0.050000
an allowable	0.007576
allowable expression	0.500000
usually done	0.062500
with reference	0.005464
which recursively	0.007246
recursively defines	0.500000
defines components	0.500000
components that	0.200000
up an	0.045455
an expression	0.007576
order in	0.071429
they must	0.025000
must appear	0.071429
appear .	0.062500
not all	0.017857
all rules	0.023256
rules defining	0.023256
defining	0.000028
defining programming	1.000000
expressed by	0.166667
by context-free	0.005714
grammars alone	0.071429
alone ,	0.250000
example type	0.012346
type validity	0.071429
validity	0.000028
validity and	1.000000
and proper	0.001445
proper declaration	0.142857
declaration	0.000028
declaration of	1.000000
of identifiers	0.000891
identifiers	0.000028
identifiers .	1.000000
These rules	0.058824
be formally	0.004219
formally expressed	0.500000
expressed with	0.166667
with attribute	0.005464
attribute	0.000056
attribute grammars	0.500000
The final	0.005208
final phase	0.111111
phase	0.000028
phase is	1.000000
is semantic	0.002033
semantic parsing	0.047619
or analysis	0.004505
working out	0.142857
out the	0.214286
the implications	0.000692
implications	0.000028
implications of	1.000000
the expression	0.001384
expression just	0.100000
just validated	0.111111
validated	0.000028
validated and	1.000000
and taking	0.001445
taking the	0.600000
appropriate action	0.250000
action .	0.200000
calculator or	0.500000
interpreter ,	0.500000
the action	0.000692
action is	0.200000
expression or	0.100000
or program	0.004505
program ,	0.045455
compiler ,	0.333333
, would	0.001684
would generate	0.018868
generate some	0.055556
of code	0.000891
<s> Attribute	0.000769
Attribute	0.000028
Attribute grammars	1.000000
grammars can	0.071429
define these	0.500000
these actions	0.023810
actions	0.000056
actions .	1.000000
of parser	0.001783
essentially to	0.125000
if and	0.035714
how the	0.034483
input can	0.024390
start symbol	0.285714
symbol of	0.250000
This can	0.015873
in essentially	0.001873
essentially two	0.125000
two ways	0.034483
: Top-down	0.009804
Top-down	0.000056
Top-down parsing	1.000000
parsing -	0.071429
- Top-down	0.062500
parsing can	0.071429
find left-most	0.076923
left-most	0.000056
left-most derivations	0.500000
derivations	0.000056
derivations of	1.000000
an input-stream	0.007576
input-stream	0.000028
input-stream by	1.000000
by searching	0.005714
searching for	0.333333
for parse	0.003610
parse trees	0.222222
trees using	0.166667
a top-down	0.001227
top-down	0.000112
top-down expansion	0.250000
expansion of	0.333333
the given	0.001384
given formal	0.041667
<s> Tokens	0.000769
Tokens	0.000028
Tokens are	1.000000
are consumed	0.004149
consumed	0.000028
consumed from	1.000000
from left	0.009615
left to	0.166667
to right	0.001328
right .	0.300000
<s> Inclusive	0.000769
Inclusive	0.000028
Inclusive choice	1.000000
to accommodate	0.002656
accommodate	0.000140
accommodate ambiguity	0.400000
ambiguity by	0.125000
by expanding	0.005714
expanding	0.000028
expanding all	1.000000
all alternative	0.023256
alternative right-hand-sides	0.333333
right-hand-sides	0.000028
right-hand-sides of	1.000000
<s> Bottom-up	0.000769
Bottom-up	0.000028
Bottom-up parsing	1.000000
- A	0.062500
A parser	0.020000
parser can	0.062500
can start	0.005525
start with	0.142857
and attempt	0.001445
to rewrite	0.001328
rewrite	0.000028
rewrite it	1.000000
<s> Intuitively	0.000769
Intuitively	0.000028
Intuitively ,	1.000000
parser attempts	0.062500
attempts to	0.500000
to locate	0.001328
locate	0.000028
locate the	1.000000
most basic	0.017241
basic elements	0.076923
elements	0.000112
elements ,	0.250000
then the	0.057143
the elements	0.000692
elements containing	0.250000
containing these	0.125000
<s> LR	0.000769
LR	0.000056
LR parsers	1.000000
of bottom-up	0.000891
bottom-up	0.000028
bottom-up parsers	1.000000
parsers .	0.076923
Another term	0.076923
is Shift-Reduce	0.002033
Shift-Reduce	0.000028
Shift-Reduce parsing	1.000000
<s> LL	0.001537
LL	0.000056
LL parsers	1.000000
parsers and	0.076923
and recursive-descent	0.001445
recursive-descent	0.000028
recursive-descent parser	1.000000
parser are	0.062500
of top-down	0.001783
top-down parsers	0.250000
parsers which	0.076923
not accommodate	0.017857
accommodate left	0.200000
left recursive	0.166667
recursive	0.000028
recursive productions	1.000000
productions	0.000028
productions .	1.000000
Although it	0.125000
been believed	0.014706
believed	0.000028
believed that	1.000000
that simple	0.003546
simple implementations	0.038462
top-down parsing	0.500000
accommodate direct	0.200000
direct and	0.166667
and indirect	0.001445
indirect	0.000028
indirect left-recursion	1.000000
left-recursion	0.000028
left-recursion and	1.000000
and may	0.002890
may require	0.038462
require exponential	0.045455
exponential	0.000056
exponential time	0.500000
and space	0.001445
space	0.000140
space complexity	0.200000
complexity while	0.083333
while parsing	0.050000
parsing ambiguous	0.035714
ambiguous context-free	0.083333
more sophisticated	0.021053
for top-down	0.003610
parsing have	0.035714
by Frost	0.005714
Frost	0.000028
Frost ,	1.000000
, Hafiz	0.000561
Hafiz	0.000028
Hafiz ,	1.000000
and Callaghan	0.001445
Callaghan	0.000028
Callaghan which	1.000000
which accommodate	0.007246
ambiguity and	0.125000
and left	0.001445
left recursion	0.166667
recursion	0.000028
recursion in	1.000000
in polynomial	0.001873
polynomial	0.000028
polynomial time	1.000000
and which	0.001445
generate polynomial-size	0.055556
polynomial-size	0.000028
polynomial-size representations	1.000000
representations of	0.250000
the potentially	0.000692
potentially exponential	0.333333
exponential number	0.500000
of parse	0.000891
Their algorithm	0.500000
is able	0.002033
produce both	0.045455
both left-most	0.032258
left-most and	0.500000
and right-most	0.001445
right-most	0.000028
right-most derivations	1.000000
input with	0.024390
given CFG	0.041667
CFG	0.000028
CFG -LRB-	1.000000
-LRB- context-free	0.002710
distinction with	0.200000
to parsers	0.001328
parsers is	0.076923
is whether	0.002033
parser generates	0.062500
a leftmost	0.002454
leftmost	0.000056
leftmost derivation	1.000000
derivation	0.000112
derivation or	0.250000
a rightmost	0.002454
rightmost	0.000056
rightmost derivation	1.000000
derivation -LRB-	0.500000
see context-free	0.050000
parsers will	0.153846
derivation and	0.250000
and LR	0.001445
although usually	0.166667
in reverse	0.001873
reverse -RRB-	0.500000
In information	0.009524
retrieval and	0.285714
, question	0.001123
question answering	0.214286
answering -LRB-	0.083333
-LRB- QA	0.002710
QA	0.000586
QA -RRB-	0.047619
automatically answering	0.047619
answering a	0.083333
a question	0.008589
question posed	0.047619
posed	0.000084
posed in	0.666667
To find	0.111111
the answer	0.009689
answer to	0.166667
a QA	0.004908
QA computer	0.047619
program may	0.045455
may use	0.019231
use either	0.013889
either a	0.200000
a pre-structured	0.001227
pre-structured	0.000028
pre-structured database	1.000000
database or	0.200000
language documents	0.006757
corpus such	0.032258
Web or	0.111111
or some	0.013514
some local	0.012048
local collection	0.333333
collection -RRB-	0.200000
<s> QA	0.000769
QA research	0.047619
research attempts	0.047619
to deal	0.001328
a wide	0.002454
wide range	0.500000
of question	0.002674
question types	0.023810
types including	0.071429
including :	0.142857
: fact	0.009804
, list	0.000561
list ,	0.090909
, definition	0.000561
, How	0.000561
How ,	0.142857
, Why	0.000561
Why ,	0.142857
, hypothetical	0.000561
hypothetical	0.000028
hypothetical ,	1.000000
, semantically	0.000561
semantically	0.000028
semantically constrained	1.000000
constrained	0.000028
constrained ,	1.000000
and cross-lingual	0.001445
cross-lingual	0.000056
cross-lingual questions	0.500000
questions .	0.076923
<s> Search	0.001537
Search	0.000056
Search collections	0.500000
collections	0.000112
collections vary	0.250000
vary from	0.166667
small local	0.111111
local document	0.333333
document collections	0.027778
collections ,	0.500000
to internal	0.001328
internal organization	0.200000
organization documents	0.200000
to compiled	0.001328
compiled	0.000028
compiled newswire	1.000000
newswire	0.000028
newswire reports	1.000000
reports ,	0.200000
Web .	0.222222
<s> Closed-domain	0.000769
Closed-domain	0.000028
Closed-domain question	1.000000
answering deals	0.166667
with questions	0.010929
questions under	0.038462
under a	0.200000
domain -LRB-	0.050000
, medicine	0.000561
medicine	0.000028
medicine or	1.000000
or automotive	0.004505
automotive	0.000028
automotive maintenance	1.000000
maintenance	0.000028
maintenance -RRB-	1.000000
an easier	0.007576
easier task	0.125000
task because	0.023810
because NLP	0.033333
can exploit	0.005525
exploit	0.000028
exploit domain-specific	1.000000
domain-specific knowledge	0.500000
knowledge frequently	0.037037
frequently	0.000056
frequently formalized	0.500000
formalized	0.000028
formalized in	1.000000
in ontologies	0.001873
ontologies .	0.500000
<s> Alternatively	0.001537
Alternatively	0.000056
Alternatively ,	1.000000
, closed-domain	0.000561
closed-domain	0.000028
closed-domain might	1.000000
might refer	0.038462
a situation	0.001227
situation	0.000056
situation where	0.500000
where only	0.028571
only a	0.052632
limited type	0.100000
of questions	0.004456
are accepted	0.004149
accepted	0.000028
accepted ,	1.000000
as questions	0.003484
questions asking	0.038462
asking	0.000056
asking for	0.500000
for descriptive	0.003610
descriptive rather	0.333333
than procedural	0.022222
procedural	0.000028
procedural information	1.000000
<s> Open-domain	0.000769
Open-domain	0.000028
Open-domain question	1.000000
questions about	0.153846
about nearly	0.025000
nearly anything	0.500000
anything	0.000028
anything ,	1.000000
only rely	0.026316
on general	0.004717
general ontologies	0.045455
ontologies and	0.166667
and world	0.001445
world knowledge	0.133333
knowledge .	0.037037
, these	0.001123
usually have	0.031250
have much	0.009615
available from	0.058824
which to	0.007246
extract the	0.250000
, current	0.000561
current QA	0.142857
QA systems	0.285714
use text	0.013889
text documents	0.006289
documents as	0.026316
their underlying	0.029412
underlying knowledge	0.333333
knowledge source	0.037037
source and	0.041667
and combine	0.001445
various natural	0.055556
processing techniques	0.037037
to search	0.001328
search for	0.090909
the answers	0.000692
answers .	0.083333
Current QA	0.200000
systems typically	0.008929
typically include	0.055556
question classifier	0.023810
classifier module	0.142857
module	0.000084
module that	0.333333
question and	0.023810
of answer	0.000891
After the	0.333333
the question	0.011073
question is	0.095238
analyzed ,	0.200000
system typically	0.010753
typically uses	0.055556
uses several	0.071429
several modules	0.045455
modules that	0.500000
that apply	0.003546
apply increasingly	0.200000
increasingly complex	0.333333
complex NLP	0.083333
NLP techniques	0.042553
techniques on	0.043478
a gradually	0.001227
gradually	0.000028
gradually reduced	1.000000
reduced amount	0.250000
document retrieval	0.027778
retrieval module	0.142857
module uses	0.333333
uses search	0.071429
engines to	0.333333
documents or	0.026316
paragraphs in	0.250000
document set	0.027778
set that	0.025641
to contain	0.001328
<s> Subsequently	0.000769
Subsequently	0.000028
Subsequently a	1.000000
a filter	0.001227
filter	0.000056
filter preselects	0.500000
preselects	0.000028
preselects small	1.000000
small text	0.111111
text fragments	0.006289
fragments	0.000028
fragments that	1.000000
that contain	0.010638
contain strings	0.083333
same type	0.040000
type as	0.071429
the expected	0.001384
expected answer	0.142857
is ``	0.004065
`` Who	0.010582
Who	0.000056
Who invented	0.500000
invented Penicillin	0.500000
Penicillin	0.000028
Penicillin ''	1.000000
'' the	0.005155
the filter	0.000692
filter returns	0.500000
returns	0.000028
returns text	1.000000
contain names	0.083333
names of	0.142857
of people	0.000891
<s> Finally	0.000769
Finally	0.000028
Finally ,	1.000000
an answer	0.015152
answer extraction	0.066667
extraction module	0.032258
module looks	0.333333
looks for	0.250000
further clues	0.125000
clues	0.000084
clues in	0.333333
answer candidate	0.033333
candidate can	0.333333
can indeed	0.005525
indeed answer	0.333333
answer the	0.033333
question .	0.047619
answering methods	0.083333
methods QA	0.022727
QA is	0.047619
very dependent	0.024390
dependent	0.000084
dependent on	0.666667
good search	0.076923
search corpus	0.090909
corpus -	0.032258
- for	0.062500
for without	0.003610
without documents	0.076923
documents containing	0.026316
answer ,	0.033333
is little	0.002033
little any	0.333333
any QA	0.032258
QA system	0.238095
system can	0.010753
do .	0.038462
It thus	0.026316
thus makes	0.100000
makes sense	0.125000
sense that	0.125000
that larger	0.003546
larger collection	0.062500
collection sizes	0.200000
sizes generally	0.333333
generally lend	0.090909
lend	0.000028
lend well	1.000000
well to	0.035714
better QA	0.111111
QA performance	0.047619
performance ,	0.111111
, unless	0.000561
unless	0.000028
unless the	1.000000
question domain	0.023810
domain is	0.050000
is orthogonal	0.002033
orthogonal	0.000028
orthogonal to	1.000000
the collection	0.000692
collection .	0.200000
The notion	0.010417
data redundancy	0.012987
in massive	0.001873
massive	0.000028
massive collections	1.000000
the web	0.001384
web ,	0.125000
, means	0.000561
that nuggets	0.003546
nuggets	0.000028
nuggets of	1.000000
information are	0.021739
be phrased	0.004219
phrased	0.000028
phrased in	1.000000
different ways	0.020408
ways in	0.125000
in differing	0.001873
differing	0.000056
differing contexts	1.000000
contexts and	0.142857
and documents	0.001445
, leading	0.000561
to two	0.001328
two benefits	0.034483
benefits	0.000056
benefits :	0.500000
: By	0.009804
By	0.000084
By having	0.333333
having the	0.200000
right information	0.100000
information appear	0.021739
many forms	0.019231
forms ,	0.166667
the burden	0.000692
burden	0.000028
burden on	1.000000
the QA	0.000692
perform complex	0.090909
is lessened	0.002033
lessened	0.000028
lessened .	1.000000
<s> Correct	0.000769
Correct	0.000028
Correct answers	1.000000
answers can	0.083333
filtered from	0.333333
from false	0.009615
false	0.000056
false positives	0.500000
positives	0.000028
positives by	1.000000
by relying	0.005714
relying	0.000028
relying on	1.000000
correct answer	0.066667
appear more	0.062500
more times	0.010526
times in	0.200000
documents than	0.026316
than instances	0.022222
of incorrect	0.000891
incorrect ones	0.333333
Issues In	0.500000
In 2002	0.009524
2002 a	0.500000
a group	0.001227
of researchers	0.000891
researchers wrote	0.100000
wrote a	0.166667
a roadmap	0.001227
roadmap	0.000028
roadmap of	1.000000
answering .	0.166667
following issues	0.066667
issues were	0.200000
were identified	0.024390
identified .	0.200000
Question classes	0.285714
classes Different	0.200000
questions -LRB-	0.038462
of Lichtenstein	0.000891
Lichtenstein	0.000028
Lichtenstein ?	1.000000
'' </s>	0.041237
<s> vs.	0.001537
vs. ``	0.166667
a rainbow	0.001227
rainbow	0.000028
rainbow form	1.000000
form ?	0.050000
`` Did	0.005291
Did	0.000028
Did Marilyn	1.000000
Marilyn	0.000028
Marilyn Monroe	1.000000
Monroe	0.000028
Monroe and	1.000000
and Cary	0.001445
Cary	0.000028
Cary Grant	1.000000
Grant	0.000028
Grant ever	1.000000
ever	0.000028
ever appear	1.000000
a movie	0.002454
movie	0.000084
movie together	0.333333
together ?	0.125000
<s> require	0.000769
of different	0.001783
different strategies	0.020408
strategies to	0.500000
classes are	0.200000
are arranged	0.004149
arranged	0.000028
arranged hierarchically	1.000000
hierarchically	0.000028
hierarchically in	1.000000
in taxonomies	0.001873
taxonomies	0.000028
taxonomies .	1.000000
Question processing	0.142857
processing The	0.018519
same information	0.040000
information request	0.021739
request	0.000028
request can	1.000000
expressed in	0.166667
various ways	0.111111
some interrogative	0.012048
interrogative	0.000028
interrogative -LRB-	1.000000
Who is	0.500000
the president	0.001384
president	0.000056
president of	1.000000
States ?	0.142857
<s> and	0.000769
and some	0.002890
some assertive	0.012048
assertive	0.000028
assertive -LRB-	1.000000
`` Tell	0.005291
Tell	0.000028
Tell me	1.000000
me	0.000028
me the	1.000000
name of	0.200000
A semantic	0.020000
semantic model	0.047619
model of	0.033333
question understanding	0.023810
understanding and	0.030303
and processing	0.001445
processing would	0.018519
would recognize	0.018868
recognize equivalent	0.111111
equivalent questions	0.200000
questions ,	0.307692
of how	0.000891
are presented	0.004149
presented .	0.166667
would enable	0.018868
complex question	0.041667
question into	0.023810
of simpler	0.001783
simpler	0.000084
simpler questions	0.333333
would identify	0.018868
identify ambiguities	0.083333
ambiguities and	0.250000
and treat	0.001445
treat them	0.500000
them in	0.052632
context or	0.030303
or by	0.004505
by interactive	0.005714
interactive clarification	0.250000
clarification	0.000084
clarification .	0.333333
<s> Context	0.000769
Context	0.000028
Context and	1.000000
and QA	0.001445
QA Questions	0.047619
Questions	0.000028
Questions are	1.000000
usually asked	0.031250
asked	0.000084
asked within	0.333333
a context	0.002454
context and	0.121212
and answers	0.001445
answers are	0.083333
are provided	0.004149
provided within	0.200000
within that	0.055556
that specific	0.003546
specific context	0.047619
The context	0.005208
context can	0.030303
to clarify	0.001328
clarify	0.000028
clarify a	1.000000
, resolve	0.000561
ambiguities or	0.250000
or keep	0.004505
keep track	0.333333
track	0.000028
track of	1.000000
an investigation	0.007576
investigation	0.000028
investigation performed	1.000000
performed through	0.100000
through a	0.250000
-LRB- For	0.005420
Why did	0.142857
did Joe	0.200000
Joe	0.000028
Joe Biden	1.000000
Biden	0.000084
Biden visit	0.333333
visit	0.000056
visit Iraq	0.500000
Iraq	0.000056
Iraq in	0.500000
in January	0.003745
January 2010	0.500000
2010 ?	0.333333
<s> might	0.000769
be asking	0.004219
asking why	0.500000
why Vice	0.142857
Vice	0.000028
Vice President	1.000000
President Biden	0.250000
Biden visited	0.333333
visited	0.000028
visited and	1.000000
not President	0.008929
President Obama	0.250000
Obama	0.000028
Obama ,	1.000000
, why	0.001123
why he	0.285714
he went	0.285714
went to	0.400000
to Iraq	0.001328
Iraq and	0.500000
not Afghanistan	0.008929
Afghanistan	0.000028
Afghanistan or	1.000000
other country	0.014286
country ,	0.250000
went in	0.200000
2010 and	0.333333
not before	0.008929
before or	0.166667
or after	0.004505
after ,	0.083333
or what	0.009009
what Biden	0.031250
Biden was	0.333333
was hoping	0.012987
hoping	0.000028
hoping to	1.000000
to accomplish	0.001328
accomplish	0.000028
accomplish with	1.000000
with his	0.005464
his visit	0.083333
visit .	0.500000
If the	0.400000
related questions	0.066667
previous questions	0.333333
questions and	0.038462
their answers	0.029412
answers might	0.083333
might shed	0.038462
shed	0.000028
shed light	1.000000
light on	0.333333
the questioner	0.002768
questioner	0.000112
questioner 's	0.250000
's intent	0.019608
intent	0.000028
intent .	1.000000
<s> Data	0.000769
Data	0.000028
Data sources	1.000000
sources for	0.166667
for QA	0.010830
QA Before	0.047619
Before a	0.500000
question can	0.023810
be answered	0.004219
answered	0.000140
answered ,	0.200000
it must	0.008547
be known	0.004219
known what	0.038462
what knowledge	0.031250
knowledge sources	0.037037
sources are	0.166667
available and	0.058824
and relevant	0.001445
relevant .	0.142857
not present	0.026786
data sources	0.025974
sources ,	0.166667
no matter	0.076923
matter how	0.333333
well the	0.035714
question processing	0.071429
and answer	0.001445
extraction is	0.064516
correct result	0.066667
result will	0.090909
be obtained	0.004219
obtained .	0.142857
<s> Answer	0.001537
Answer	0.000084
Answer extraction	0.666667
extraction Answer	0.032258
extraction depends	0.032258
answer type	0.066667
type provided	0.071429
provided by	0.400000
by question	0.005714
actual data	0.200000
data where	0.012987
answer is	0.066667
is searched	0.002033
the search	0.000692
search method	0.090909
method and	0.062500
and on	0.002890
question focus	0.023810
focus and	0.142857
Answer formulation	0.333333
formulation	0.000028
formulation The	1.000000
The result	0.005208
system should	0.010753
be presented	0.004219
way as	0.041667
as natural	0.003484
natural as	0.013333
, simple	0.001123
simple extraction	0.038462
is sufficient	0.004065
question classification	0.023810
classification indicates	0.058824
indicates	0.000028
indicates that	1.000000
name -LRB-	0.200000
organization ,	0.200000
, shop	0.000561
shop	0.000028
shop or	1.000000
or disease	0.004505
disease	0.000028
disease ,	1.000000
a quantity	0.001227
quantity -LRB-	0.333333
-LRB- monetary	0.002710
monetary	0.000028
monetary value	1.000000
value	0.000084
value ,	0.333333
, length	0.000561
, size	0.000561
size ,	0.166667
, distance	0.000561
distance	0.000084
distance ,	0.666667
a date	0.001227
date	0.000084
date -LRB-	0.666667
`` On	0.005291
On what	0.166667
what day	0.031250
day	0.000028
day did	1.000000
did Christmas	0.200000
Christmas	0.000028
Christmas fall	1.000000
fall in	0.250000
in 1989	0.001873
1989 ?	0.500000
<s> the	0.000769
single datum	0.071429
datum	0.000028
datum is	1.000000
For other	0.016393
other cases	0.028571
the presentation	0.000692
presentation	0.000028
presentation of	1.000000
answer may	0.033333
of fusion	0.000891
fusion	0.000028
fusion techniques	1.000000
techniques that	0.086957
that combine	0.003546
combine the	0.333333
the partial	0.000692
partial	0.000028
partial answers	1.000000
answers from	0.166667
from multiple	0.009615
Real time	0.500000
time question	0.030303
answering There	0.083333
is need	0.002033
developing Q&A	0.250000
Q&A	0.000028
Q&A systems	1.000000
of extracting	0.000891
extracting answers	0.200000
from large	0.009615
large data	0.043478
sets in	0.090909
several seconds	0.045455
seconds	0.000028
seconds ,	1.000000
size and	0.333333
and multitude	0.001445
multitude	0.000028
multitude of	1.000000
sources or	0.166667
the ambiguity	0.000692
ambiguity of	0.125000
<s> Multilingual	0.000769
Multilingual	0.000028
Multilingual -LRB-	1.000000
or cross-lingual	0.004505
cross-lingual -RRB-	0.500000
-RRB- question	0.002710
answering The	0.083333
The ability	0.005208
to answer	0.003984
answer a	0.033333
language using	0.006757
answer corpus	0.033333
even several	0.037037
several -RRB-	0.045455
This allows	0.031746
to consult	0.001328
consult	0.000028
consult information	1.000000
information that	0.021739
not use	0.017857
use directly	0.013889
directly .	0.200000
also Machine	0.014493
<s> Interactive	0.000769
Interactive	0.000056
Interactive QA	0.500000
QA It	0.047619
often the	0.045455
case that	0.058824
information need	0.021739
need is	0.047619
not well	0.008929
well captured	0.035714
captured	0.000028
captured by	1.000000
processing part	0.018519
part may	0.037037
may fail	0.019231
fail to	0.333333
to classify	0.001328
classify	0.000056
classify properly	0.500000
properly the	0.500000
question or	0.023810
information needed	0.021739
for extracting	0.003610
extracting and	0.200000
and generating	0.001445
generating the	0.200000
easily retrieved	0.111111
retrieved	0.000028
retrieved .	1.000000
In such	0.009524
questioner might	0.250000
might want	0.038462
want not	0.166667
only to	0.026316
to reformulate	0.001328
reformulate	0.000028
reformulate the	1.000000
but to	0.014706
dialogue with	0.500000
system might	0.010753
might ask	0.038462
ask for	0.250000
a clarification	0.001227
clarification of	0.666667
what sense	0.031250
sense a	0.125000
being used	0.055556
what type	0.031250
information is	0.043478
being asked	0.055556
asked for	0.333333
for .	0.003610
Advanced reasoning	0.200000
reasoning for	0.142857
QA More	0.047619
More sophisticated	0.333333
sophisticated questioners	0.142857
questioners	0.000028
questioners expect	1.000000
expect answers	0.333333
answers that	0.083333
are outside	0.004149
outside the	0.500000
of written	0.003565
texts or	0.058824
or structured	0.004505
structured databases	0.166667
To upgrade	0.111111
upgrade	0.000028
upgrade a	1.000000
system with	0.010753
with such	0.005464
such capabilities	0.008130
capabilities ,	0.200000
to integrate	0.001328
integrate	0.000028
integrate reasoning	1.000000
reasoning components	0.142857
components operating	0.200000
operating	0.000056
operating on	0.500000
knowledge bases	0.037037
bases	0.000028
bases ,	1.000000
, encoding	0.000561
encoding	0.000028
encoding world	1.000000
knowledge and	0.074074
and common-sense	0.001445
common-sense	0.000028
common-sense reasoning	1.000000
reasoning mechanisms	0.142857
mechanisms ,	0.500000
as knowledge	0.003484
knowledge specific	0.037037
specific to	0.047619
of domains	0.000891
domains .	0.250000
<s> User	0.000769
User	0.000056
User profiling	0.500000
profiling	0.000028
profiling for	1.000000
QA The	0.047619
The user	0.005208
user profile	0.071429
profile	0.000084
profile captures	0.333333
captures	0.000028
captures data	1.000000
data about	0.012987
questioner ,	0.500000
, comprising	0.000561
comprising context	0.500000
context data	0.030303
, domain	0.000561
domain of	0.100000
interest ,	0.090909
, reasoning	0.000561
reasoning schemes	0.142857
schemes frequently	0.500000
frequently used	0.500000
, common	0.000561
common ground	0.040000
ground	0.000028
ground established	1.000000
established	0.000028
established within	1.000000
within different	0.055556
different dialogues	0.020408
dialogues	0.000028
dialogues between	1.000000
user ,	0.071429
so forth	0.033333
forth	0.000028
forth .	1.000000
The profile	0.005208
profile may	0.333333
be represented	0.008439
represented	0.000167
represented as	0.333333
a predefined	0.001227
predefined	0.000028
predefined template	1.000000
template ,	0.250000
where each	0.028571
each template	0.022222
template slot	0.250000
slot	0.000028
slot represents	1.000000
represents	0.000112
represents a	0.750000
different profile	0.020408
profile feature	0.333333
<s> Profile	0.000769
Profile	0.000028
Profile templates	1.000000
templates	0.000028
templates may	1.000000
be nested	0.004219
nested	0.000028
nested one	1.000000
one within	0.015385
within another	0.055556
History Some	0.500000
early AI	0.200000
AI systems	0.666667
were question	0.024390
answering systems	0.083333
Two of	0.285714
most famous	0.034483
famous QA	0.333333
of that	0.001783
that time	0.003546
time are	0.030303
are BASEBALL	0.004149
BASEBALL	0.000056
BASEBALL and	0.500000
and LUNAR	0.001445
LUNAR	0.000084
LUNAR ,	0.666667
1960s .	0.333333
<s> BASEBALL	0.000769
BASEBALL answered	0.500000
answered questions	0.600000
US baseball	0.142857
baseball	0.000028
baseball league	1.000000
league	0.000028
league over	1.000000
over a	0.083333
a period	0.002454
period	0.000056
period of	0.500000
one year	0.015385
<s> LUNAR	0.000769
turn ,	0.166667
, answered	0.000561
the geological	0.000692
geological	0.000028
geological analysis	1.000000
of rocks	0.000891
rocks	0.000028
rocks returned	1.000000
returned by	0.250000
the Apollo	0.000692
Apollo	0.000028
Apollo moon	1.000000
moon	0.000028
moon missions	1.000000
missions	0.000028
missions .	1.000000
Both QA	0.333333
were very	0.048780
very effective	0.024390
their chosen	0.029412
chosen domains	0.200000
, LUNAR	0.000561
LUNAR was	0.333333
was demonstrated	0.012987
demonstrated	0.000028
demonstrated at	1.000000
a lunar	0.001227
lunar	0.000028
lunar science	1.000000
science convention	0.100000
convention	0.000028
convention in	1.000000
in 1971	0.001873
1971 and	0.333333
answer 90	0.033333
the questions	0.000692
questions in	0.038462
its domain	0.057143
domain posed	0.050000
posed by	0.333333
people untrained	0.062500
untrained	0.000028
untrained on	1.000000
<s> Further	0.002306
Further	0.000084
Further restricted-domain	0.333333
restricted-domain	0.000028
restricted-domain QA	1.000000
following years	0.066667
The common	0.005208
common feature	0.040000
feature of	0.230769
all these	0.023256
systems is	0.026786
they had	0.025000
had a	0.285714
a core	0.001227
core	0.000056
core database	0.500000
or knowledge	0.004505
knowledge system	0.037037
system that	0.032258
that was	0.010638
was hand-written	0.012987
hand-written by	0.142857
by experts	0.005714
experts	0.000028
experts of	1.000000
the chosen	0.000692
chosen domain	0.200000
systems included	0.008929
included question-answering	0.125000
question-answering	0.000056
question-answering abilities	0.500000
abilities	0.000028
abilities .	1.000000
famous early	0.333333
early systems	0.100000
are SHRDLU	0.004149
SHRDLU and	0.166667
ELIZA .	0.111111
SHRDLU simulated	0.166667
simulated	0.000056
simulated the	0.500000
the operation	0.000692
operation of	0.500000
a robot	0.001227
robot	0.000056
robot in	0.500000
toy world	0.500000
world -LRB-	0.066667
blocks world	0.250000
world ''	0.066667
it offered	0.008547
offered	0.000028
offered the	1.000000
possibility to	0.250000
to ask	0.001328
the robot	0.000692
robot questions	0.500000
the state	0.001384
<s> Again	0.000769
Again	0.000028
Again ,	1.000000
the strength	0.001384
strength	0.000140
strength of	0.600000
this system	0.010989
the choice	0.000692
very specific	0.024390
domain and	0.050000
very simple	0.048780
simple world	0.038462
world with	0.066667
with rules	0.005464
of physics	0.000891
physics	0.000028
physics that	1.000000
were easy	0.024390
to encode	0.001328
encode	0.000028
encode in	1.000000
in contrast	0.001873
, simulated	0.000561
simulated a	0.500000
a conversation	0.001227
a psychologist	0.001227
psychologist	0.000028
psychologist .	1.000000
ELIZA was	0.111111
to converse	0.001328
converse	0.000028
converse on	1.000000
topic by	0.125000
by resorting	0.005714
resorting	0.000028
resorting to	1.000000
to very	0.001328
simple rules	0.038462
that detected	0.003546
detected	0.000056
detected important	0.500000
important words	0.062500
the person	0.002768
person 's	0.210526
's input	0.019608
It had	0.026316
very rudimentary	0.024390
rudimentary way	0.500000
answer questions	0.033333
own it	0.166667
it led	0.008547
led to	0.666667
of chatterbots	0.000891
chatterbots such	0.500000
ones that	0.100000
that participate	0.003546
participate	0.000028
participate in	1.000000
the annual	0.000692
annual Loebner	0.500000
Loebner	0.000028
Loebner prize	1.000000
prize	0.000028
prize .	1.000000
The 1970s	0.005208
1980s saw	0.111111
saw	0.000028
saw the	1.000000
of comprehensive	0.000891
comprehensive theories	0.200000
theories in	0.200000
which led	0.007246
of ambitious	0.000891
ambitious	0.000028
ambitious projects	1.000000
projects	0.000056
projects in	0.500000
in text	0.001873
text comprehension	0.006289
comprehension and	0.142857
and question	0.001445
One example	0.076923
the Unix	0.001384
Unix	0.000056
Unix Consultant	0.500000
Consultant	0.000028
Consultant -LRB-	1.000000
-LRB- UC	0.002710
UC	0.000056
UC -RRB-	0.500000
that answered	0.003546
questions pertaining	0.038462
pertaining	0.000028
pertaining to	1.000000
Unix operating	0.500000
operating system	0.500000
system had	0.010753
comprehensive hand-crafted	0.200000
hand-crafted knowledge	0.500000
base of	0.250000
it aimed	0.008547
at phrasing	0.014706
phrasing	0.000028
phrasing the	1.000000
accommodate various	0.200000
Another project	0.076923
project was	0.076923
was LILOG	0.012987
LILOG	0.000056
LILOG ,	0.500000
a text-understanding	0.001227
text-understanding	0.000028
text-understanding system	1.000000
that operated	0.003546
operated	0.000056
operated on	0.500000
of tourism	0.000891
tourism	0.000028
tourism information	1.000000
information in	0.043478
a German	0.001227
German city	0.250000
city	0.000028
city .	1.000000
The systems	0.005208
the UC	0.000692
UC and	0.500000
and LILOG	0.001445
LILOG projects	0.500000
projects never	0.500000
never went	0.200000
went past	0.200000
past	0.000084
past the	0.333333
the stage	0.000692
stage of	0.400000
simple demonstrations	0.038462
demonstrations	0.000028
demonstrations ,	1.000000
they helped	0.025000
helped the	0.333333
of theories	0.000891
theories on	0.200000
on computational	0.004717
linguistics and	0.050000
and reasoning	0.002890
reasoning .	0.142857
An increasing	0.062500
increasing number	0.333333
systems include	0.008929
Web as	0.111111
one more	0.015385
more corpus	0.010526
. .	0.000780
these tools	0.023810
tools mostly	0.166667
mostly	0.000056
mostly work	0.500000
using shallow	0.016949
shallow methods	0.166667
above --	0.076923
-- thus	0.040000
thus returning	0.100000
returning	0.000056
returning a	1.000000
an excerpt	0.007576
excerpt	0.000028
excerpt containing	1.000000
the probable	0.000692
probable	0.000028
probable answer	1.000000
answer highlighted	0.033333
highlighted	0.000028
highlighted ,	1.000000
, plus	0.000561
plus	0.000028
plus some	1.000000
some context	0.012048
, highly-specialized	0.000561
highly-specialized	0.000028
highly-specialized natural	1.000000
language question-answering	0.006757
question-answering engines	0.500000
engines ,	0.333333
as EAGLi	0.003484
EAGLi	0.000028
EAGLi for	1.000000
for health	0.003610
health	0.000028
health and	1.000000
and life	0.001445
life scientists	0.250000
scientists	0.000028
scientists ,	1.000000
been made	0.014706
made available	0.062500
The Future	0.005208
Future	0.000056
Future of	0.500000
of Question	0.000891
Question Answering	0.142857
Answering	0.000028
Answering QA	1.000000
been extended	0.014706
extended	0.000028
extended in	1.000000
years to	0.047619
explore critical	0.250000
critical new	0.250000
new scientific	0.041667
scientific and	0.500000
and practical	0.001445
practical dimensions	0.500000
dimensions For	0.333333
been developed	0.014706
developed to	0.038462
automatically answer	0.047619
answer temporal	0.033333
temporal	0.000056
temporal and	0.500000
and geospatial	0.001445
geospatial	0.000028
geospatial questions	1.000000
, definitional	0.000561
definitional	0.000028
definitional questions	1.000000
, biographical	0.000561
biographical	0.000028
biographical questions	1.000000
, multilingual	0.000561
multilingual questions	0.333333
and questions	0.001445
questions from	0.038462
from multimedia	0.009615
multimedia -LRB-	0.500000
, audio	0.001123
audio	0.000056
audio ,	1.000000
, imagery	0.000561
imagery	0.000028
imagery ,	1.000000
, video	0.001123
video	0.000140
video -RRB-	0.200000
<s> Additional	0.000769
Additional	0.000028
Additional aspects	1.000000
aspects such	0.142857
as interactivity	0.003484
interactivity	0.000028
interactivity -LRB-	1.000000
often required	0.022727
required for	0.142857
for clarification	0.003610
questions or	0.038462
or answers	0.004505
answers -RRB-	0.083333
, answer	0.000561
answer reuse	0.033333
reuse	0.000028
reuse ,	1.000000
and knowledge	0.001445
knowledge representation	0.037037
reasoning to	0.142857
support question	0.250000
answering have	0.083333
been explored	0.014706
explored .	0.500000
<s> Future	0.000769
Future research	0.500000
research may	0.023810
may explore	0.019231
explore what	0.250000
what kinds	0.031250
kinds	0.000028
kinds of	1.000000
questions can	0.038462
be asked	0.004219
asked and	0.333333
and answered	0.001445
answered about	0.200000
about social	0.025000
including sentiment	0.071429
sentiment	0.000698
sentiment analysis	0.520000
analysis .	0.092308
A relationship	0.020000
extraction task	0.032258
task requires	0.023810
the detection	0.000692
detection	0.000056
detection and	0.500000
and classification	0.001445
classification of	0.058824
semantic relationship	0.047619
relationship mentions	0.166667
mentions within	0.333333
of artifacts	0.000891
artifacts	0.000028
artifacts ,	1.000000
typically from	0.055556
or XML	0.004505
XML	0.000028
XML documents	1.000000
information extraction	0.021739
but IE	0.014706
IE additionally	0.333333
additionally	0.000028
additionally requires	1.000000
the removal	0.000692
removal	0.000028
removal of	1.000000
of repeated	0.000891
repeated relations	0.500000
relations -LRB-	0.083333
-LRB- disambiguation	0.002710
and generally	0.001445
generally refers	0.090909
different relationships	0.020408
relationships .	0.166667
Approaches One	0.333333
One approach	0.076923
problem involves	0.045455
of domain	0.000891
domain ontologies	0.100000
Another approach	0.153846
approach involves	0.028571
involves visual	0.100000
visual	0.000056
visual detection	0.500000
detection of	0.500000
of meaningful	0.000891
meaningful relationships	0.125000
relationships in	0.166667
in parametric	0.001873
parametric	0.000028
parametric values	1.000000
of objects	0.000891
objects listed	0.200000
listed	0.000028
listed on	1.000000
a data	0.001227
data table	0.012987
table that	0.142857
that shift	0.003546
shift	0.000028
shift positions	1.000000
positions	0.000028
positions as	1.000000
table is	0.142857
is permuted	0.002033
permuted	0.000028
permuted automatically	1.000000
automatically as	0.047619
as controlled	0.003484
controlled	0.000028
controlled by	1.000000
software user	0.037037
The poor	0.005208
poor	0.000028
poor coverage	1.000000
coverage ,	0.333333
, rarity	0.000561
rarity	0.000028
rarity and	1.000000
development cost	0.083333
cost related	0.500000
to structured	0.001328
structured resources	0.166667
resources such	0.166667
as semantic	0.003484
semantic lexicons	0.047619
lexicons -LRB-	0.500000
e.g. WordNet	0.017857
WordNet ,	0.500000
, UMLS	0.000561
UMLS	0.000028
UMLS -RRB-	1.000000
and domain	0.001445
ontologies -LRB-	0.166667
the Gene	0.000692
Gene	0.000028
Gene Ontology	1.000000
Ontology	0.000028
Ontology -RRB-	1.000000
has given	0.011905
given rise	0.041667
rise	0.000056
rise to	0.500000
new approaches	0.041667
approaches based	0.035714
on broad	0.004717
broad ,	0.250000
, dynamic	0.000561
dynamic background	0.200000
background	0.000084
background knowledge	0.333333
knowledge on	0.037037
the Web	0.001384
the ARCHILES	0.000692
ARCHILES	0.000028
ARCHILES technique	1.000000
technique uses	0.142857
uses only	0.071429
only Wikipedia	0.026316
Wikipedia and	0.500000
and search	0.001445
search engine	0.090909
engine page	0.166667
page count	0.142857
count for	0.200000
for acquiring	0.003610
acquiring	0.000028
acquiring coarse-grained	1.000000
coarse-grained	0.000028
coarse-grained relations	1.000000
relations to	0.083333
to construct	0.001328
construct lightweight	0.333333
lightweight	0.000028
lightweight ontologies	1.000000
The relationships	0.005208
relationships can	0.166667
represented using	0.166667
of formalisms\/languages	0.000891
formalisms\/languages	0.000028
formalisms\/languages .	1.000000
One such	0.076923
such representation	0.008130
data on	0.012987
Web is	0.111111
is RDF	0.002033
RDF	0.000028
RDF .	1.000000
<s> Jump	0.000769
Jump	0.000028
Jump to	1.000000
to :	0.001328
: navigation	0.009804
navigation	0.000056
navigation ,	0.500000
, search	0.001123
search Sentence	0.090909
Sentence boundary	0.200000
disambiguation -LRB-	0.100000
-LRB- SBD	0.002710
SBD	0.000028
SBD -RRB-	1.000000
sentence breaking	0.020833
breaking ,	0.500000
processing of	0.037037
of deciding	0.000891
deciding where	0.166667
where sentences	0.057143
sentences begin	0.013158
begin and	0.333333
and end	0.001445
end .	0.125000
Often natural	0.333333
processing tools	0.018519
tools require	0.166667
require their	0.045455
their input	0.029412
be divided	0.004219
into sentences	0.012821
sentences for	0.013158
of reasons	0.000891
reasons .	0.500000
However sentence	0.027027
boundary identification	0.166667
identification is	0.200000
is challenging	0.002033
challenging	0.000028
challenging because	1.000000
because punctuation	0.033333
marks are	0.250000
often ambiguous	0.022727
period may	0.500000
may denote	0.019231
denote	0.000056
denote an	0.500000
an abbreviation	0.007576
abbreviation ,	0.500000
, decimal	0.000561
decimal	0.000028
decimal point	1.000000
point	0.000084
point ,	0.666667
an ellipsis	0.007576
ellipsis	0.000028
ellipsis ,	1.000000
or an	0.009009
an email	0.007576
email	0.000056
email address	0.500000
address -	0.250000
- not	0.062500
<s> About	0.000769
About 47	0.500000
47	0.000028
47 %	1.000000
the periods	0.000692
periods in	0.333333
Journal corpus	0.333333
corpus denote	0.032258
denote abbreviations	0.500000
abbreviations .	0.200000
As well	0.055556
well ,	0.035714
question marks	0.023810
marks and	0.250000
and exclamation	0.001445
exclamation	0.000028
exclamation marks	1.000000
marks may	0.250000
may appear	0.019231
in embedded	0.001873
embedded quotations	0.250000
quotations	0.000028
quotations ,	1.000000
, emoticons	0.000561
emoticons	0.000028
emoticons ,	1.000000
computer code	0.022727
code ,	0.142857
and slang	0.001445
slang	0.000028
slang .	1.000000
Languages like	0.333333
like Japanese	0.035714
and Chinese	0.001445
Chinese have	0.142857
have unambiguous	0.009615
unambiguous sentence-ending	0.500000
sentence-ending	0.000028
sentence-ending markers	1.000000
markers .	0.333333
-LRB- b	0.002710
b	0.000028
b -RRB-	1.000000
-RRB- If	0.005420
the preceding	0.000692
preceding	0.000028
preceding token	1.000000
token is	0.500000
is on	0.004065
on my	0.004717
my	0.000028
my hand-compiled	1.000000
hand-compiled	0.000028
hand-compiled list	1.000000
of abbreviations	0.001783
abbreviations ,	0.400000
then it	0.057143
it does	0.008547
does n't	0.100000
n't end	0.250000
end a	0.125000
-LRB- c	0.002710
c	0.000028
c -RRB-	1.000000
next token	0.142857
is capitalized	0.002033
ends a	0.500000
This strategy	0.015873
strategy gets	0.200000
gets about	0.500000
about 95	0.025000
sentences correct	0.013158
rules from	0.023256
documents where	0.026316
sentence breaks	0.020833
breaks	0.000056
breaks are	0.500000
are pre-marked	0.004149
pre-marked	0.000028
pre-marked .	1.000000
<s> Solutions	0.000769
Solutions	0.000028
Solutions have	1.000000
been based	0.014706
entropy model	0.200000
The SATZ	0.005208
SATZ	0.000028
SATZ architecture	1.000000
architecture uses	0.500000
a neural	0.001227
neural network	0.333333
network to	0.166667
disambiguate sentence	0.333333
boundaries and	0.090909
and achieves	0.001445
achieves 98.5	0.500000
98.5	0.000028
98.5 %	1.000000
or opinion	0.004505
opinion mining	0.200000
mining refers	0.200000
the application	0.000692
, computational	0.000561
text analytics	0.006289
analytics	0.000028
analytics to	1.000000
identify and	0.083333
and extract	0.001445
extract subjective	0.250000
in source	0.001873
source materials	0.041667
materials .	0.500000
, sentiment	0.000561
analysis aims	0.015385
the attitude	0.000692
attitude	0.000056
attitude of	0.500000
a speaker	0.002454
speaker or	0.055556
a writer	0.001227
writer	0.000028
writer with	1.000000
some topic	0.012048
topic or	0.125000
overall contextual	0.166667
contextual polarity	0.500000
polarity of	0.250000
The attitude	0.005208
attitude may	0.500000
be his	0.004219
his or	0.083333
her judgement	0.500000
judgement or	0.333333
or evaluation	0.004505
evaluation -LRB-	0.018519
see appraisal	0.050000
appraisal	0.000028
appraisal theory	1.000000
theory -RRB-	0.076923
, affective	0.000561
affective	0.000056
affective state	1.000000
state -LRB-	0.071429
say ,	0.285714
the emotional	0.001384
emotional	0.000112
emotional state	0.250000
author when	0.333333
writing -RRB-	0.111111
intended emotional	0.200000
emotional communication	0.250000
communication -LRB-	0.400000
emotional effect	0.250000
effect the	0.500000
author wishes	0.333333
wishes	0.000028
wishes to	1.000000
have on	0.009615
reader -RRB-	0.100000
Advanced ,	0.200000
`` beyond	0.005291
beyond polarity	0.166667
'' sentiment	0.005155
sentiment classification	0.040000
classification looks	0.058824
looks ,	0.250000
at emotional	0.014706
emotional states	0.250000
states such	0.250000
`` angry	0.005291
angry	0.000056
angry ,	0.500000
'' ``	0.005155
`` sad	0.005291
sad	0.000028
sad ,	1.000000
`` happy	0.005291
happy	0.000028
happy .	1.000000
Early work	0.500000
work in	0.125000
that area	0.003546
area includes	0.090909
includes Turney	0.142857
and Pang	0.001445
Pang	0.000084
Pang who	0.333333
who applied	0.100000
applied different	0.066667
different methods	0.020408
for detecting	0.003610
detecting	0.000028
detecting the	1.000000
the polarity	0.000692
of product	0.000891
product reviews	0.142857
reviews and	0.166667
and movie	0.001445
movie reviews	0.333333
reviews respectively	0.166667
respectively	0.000028
respectively .	1.000000
is at	0.002033
document level	0.027778
level .	0.050000
One can	0.076923
also classify	0.014493
classify a	0.500000
document 's	0.027778
's polarity	0.019608
polarity on	0.125000
a multi-way	0.001227
multi-way	0.000028
multi-way scale	1.000000
scale	0.000167
scale ,	0.333333
was attempted	0.012987
attempted	0.000028
attempted by	1.000000
by Pang	0.005714
Pang and	0.333333
and Snyder	0.001445
Snyder	0.000056
Snyder -LRB-	0.500000
among others	0.125000
others -RRB-	0.083333
: expanded	0.009804
expanded	0.000028
expanded the	1.000000
the basic	0.001384
basic task	0.076923
of classifying	0.000891
classifying a	0.400000
movie review	0.333333
review as	0.333333
as either	0.003484
either positive	0.100000
negative to	0.125000
to predicting	0.001328
predicting	0.000056
predicting star	0.500000
star	0.000056
star ratings	0.500000
ratings on	0.111111
on either	0.004717
a 3	0.001227
3 or	0.200000
a 4	0.001227
4 star	0.200000
star scale	0.500000
while Snyder	0.050000
Snyder performed	0.500000
performed an	0.100000
an in-depth	0.007576
in-depth analysis	0.333333
of restaurant	0.000891
restaurant	0.000056
restaurant reviews	0.500000
reviews ,	0.500000
, predicting	0.000561
predicting ratings	0.500000
ratings for	0.111111
for various	0.003610
various aspects	0.055556
given restaurant	0.041667
restaurant ,	0.500000
the food	0.000692
food	0.000028
food and	1.000000
and atmosphere	0.001445
atmosphere	0.000028
atmosphere -LRB-	1.000000
a five-star	0.001227
five-star	0.000028
five-star scale	1.000000
scale -RRB-	0.166667
different method	0.020408
determining sentiment	0.166667
sentiment is	0.040000
a scaling	0.001227
scaling	0.000028
scaling system	1.000000
system whereby	0.010753
whereby	0.000028
whereby words	1.000000
words commonly	0.009174
commonly associated	0.125000
associated with	0.250000
with having	0.005464
having a	0.200000
a negative	0.001227
negative ,	0.125000
, neutral	0.000561
neutral	0.000056
neutral or	0.500000
or positive	0.004505
positive sentiment	0.142857
sentiment with	0.040000
with them	0.016393
given an	0.041667
an associated	0.007576
associated number	0.250000
number on	0.023256
a -5	0.001227
-5	0.000028
-5 to	1.000000
to +5	0.001328
+5	0.000028
+5 scale	1.000000
scale -LRB-	0.166667
most negative	0.017241
negative up	0.125000
to most	0.001328
most positive	0.017241
positive -RRB-	0.142857
and when	0.001445
of unstructured	0.000891
unstructured	0.000028
unstructured text	1.000000
analyzed using	0.200000
using natural	0.016949
the subsequent	0.000692
subsequent concepts	0.500000
are analyzed	0.004149
analyzed for	0.200000
for an	0.003610
they relate	0.025000
relate	0.000056
relate to	1.000000
the concept	0.001384
concept -LRB-	0.250000
Each concept	0.166667
concept is	0.250000
then given	0.028571
a score	0.001227
score based	0.166667
way sentiment	0.041667
sentiment words	0.040000
words relate	0.009174
concept ,	0.250000
their associated	0.029412
associated score	0.250000
allows movement	0.125000
movement	0.000028
movement to	1.000000
sophisticated understanding	0.142857
of sentiment	0.004456
sentiment based	0.040000
an 11	0.007576
11	0.000028
11 point	1.000000
point scale	0.333333
scale .	0.166667
, texts	0.000561
texts can	0.058824
be given	0.004219
a positive	0.001227
negative sentiment	0.125000
sentiment strength	0.040000
strength score	0.200000
score if	0.166667
the sentiment	0.001384
sentiment in	0.080000
text rather	0.006289
overall polarity	0.166667
polarity and	0.125000
and strength	0.001445
Another research	0.076923
research direction	0.023810
direction is	0.333333
is subjectivity\/objectivity	0.002033
subjectivity\/objectivity	0.000028
subjectivity\/objectivity identification	1.000000
identification .	0.200000
commonly defined	0.125000
defined as	0.166667
as classifying	0.003484
given text	0.041667
-LRB- usually	0.005420
usually a	0.031250
sentence -RRB-	0.041667
two classes	0.034483
classes :	0.200000
: objective	0.009804
objective or	0.200000
problem can	0.022727
can sometimes	0.005525
sometimes be	0.076923
than polarity	0.022222
polarity classification	0.125000
classification :	0.058824
the subjectivity	0.000692
subjectivity	0.000056
subjectivity of	0.500000
phrases may	0.062500
may depend	0.019231
their context	0.029412
an objective	0.007576
objective document	0.200000
document may	0.055556
may contain	0.038462
contain subjective	0.083333
subjective sentences	0.166667
article quoting	0.034483
quoting	0.000028
quoting people	1.000000
people 's	0.062500
's opinions	0.019608
opinions	0.000056
opinions -RRB-	0.500000
as mentioned	0.003484
mentioned by	0.166667
by Su	0.005714
Su	0.000028
Su ,	1.000000
, results	0.000561
are largely	0.004149
largely dependent	0.200000
of subjectivity	0.000891
subjectivity used	0.500000
when annotating	0.028571
annotating	0.000028
annotating texts	1.000000
, Pang	0.000561
Pang showed	0.333333
that removing	0.003546
removing objective	0.500000
objective sentences	0.200000
document before	0.027778
before classifying	0.166667
classifying its	0.200000
its polarity	0.028571
polarity helped	0.125000
helped improve	0.333333
improve performance	0.076923
performance .	0.166667
The more	0.005208
more fine-grained	0.010526
fine-grained	0.000028
fine-grained analysis	1.000000
analysis model	0.015385
the feature\/aspect-based	0.000692
feature\/aspect-based	0.000028
feature\/aspect-based sentiment	1.000000
It refers	0.026316
to determining	0.001328
the opinions	0.000692
opinions or	0.500000
or sentiments	0.004505
sentiments	0.000028
sentiments expressed	1.000000
expressed on	0.333333
on different	0.004717
different features	0.020408
features or	0.038462
or aspects	0.004505
of entities	0.000891
entities ,	0.285714
a cell	0.002454
cell	0.000056
cell phone	1.000000
phone ,	0.500000
a digital	0.002454
digital camera	0.142857
camera	0.000056
camera ,	0.500000
a bank	0.001227
bank	0.000028
bank .	1.000000
A feature	0.020000
feature or	0.076923
or aspect	0.004505
aspect is	0.500000
an attribute	0.007576
attribute or	0.500000
or component	0.004505
an entity	0.007576
the screen	0.000692
screen	0.000028
screen of	1.000000
the picture	0.001384
picture quality	0.250000
a camera	0.001227
camera .	0.500000
involves several	0.100000
several sub-problems	0.045455
sub-problems	0.000028
sub-problems ,	1.000000
identifying relevant	0.166667
relevant entities	0.142857
, extracting	0.000561
extracting their	0.200000
their features\/aspects	0.029412
features\/aspects	0.000028
features\/aspects ,	1.000000
and determining	0.001445
determining whether	0.166667
whether an	0.076923
an opinion	0.007576
opinion expressed	0.200000
on each	0.004717
each feature\/aspect	0.022222
feature\/aspect	0.000028
feature\/aspect is	1.000000
is positive	0.002033
positive ,	0.142857
, negative	0.000561
negative or	0.125000
or neutral	0.004505
neutral .	0.500000
More detailed	0.111111
detailed	0.000056
detailed discussions	0.500000
discussions	0.000084
discussions about	0.333333
about this	0.025000
this level	0.010989
in Liu	0.001873
Liu	0.000028
Liu 's	1.000000
's NLP	0.019608
NLP Handbook	0.021277
Handbook	0.000028
Handbook chapter	1.000000
chapter	0.000028
chapter ,	1.000000
`` Sentiment	0.005291
Sentiment Analysis	0.166667
Analysis and	0.200000
and Subjectivity	0.001445
Subjectivity	0.000028
Subjectivity ''	1.000000
Methods Computers	0.250000
Computers	0.000028
Computers can	1.000000
can perform	0.005525
perform automated	0.090909
automated sentiment	0.142857
of digital	0.000891
digital texts	0.142857
texts ,	0.117647
using elements	0.016949
elements from	0.250000
learning such	0.023256
as latent	0.003484
latent	0.000028
latent semantic	1.000000
, support	0.000561
support vector	0.250000
vector machines	0.333333
machines ,	0.250000
`` bag	0.005291
bag	0.000028
bag of	1.000000
and Semantic	0.001445
Semantic Orientation	0.333333
Orientation	0.000028
Orientation --	1.000000
-- Pointwise	0.040000
Pointwise	0.000028
Pointwise Mutual	1.000000
Mutual	0.000028
Mutual Information	1.000000
Information -LRB-	0.200000
See Peter	0.166667
Peter	0.000028
Peter Turney	1.000000
's work	0.019608
area -RRB-	0.090909
sophisticated methods	0.142857
methods try	0.022727
to detect	0.001328
detect	0.000028
detect the	1.000000
the holder	0.000692
holder	0.000028
holder of	1.000000
a sentiment	0.002454
sentiment -LRB-	0.040000
person who	0.052632
who maintains	0.100000
maintains	0.000028
maintains that	1.000000
that affective	0.003546
state -RRB-	0.071429
target -LRB-	0.090909
the entity	0.000692
entity about	0.200000
about which	0.025000
the affect	0.000692
affect is	0.333333
is felt	0.002033
felt	0.000028
felt -RRB-	1.000000
To mine	0.111111
mine	0.000028
mine the	1.000000
the opinion	0.000692
and get	0.001445
the feature	0.000692
feature which	0.076923
been opinionated	0.014706
opinionated	0.000028
opinionated ,	1.000000
grammatical relationships	0.090909
relationships of	0.166667
<s> Grammatical	0.000769
Grammatical	0.000028
Grammatical dependency	1.000000
dependency relations	0.200000
relations are	0.083333
are obtained	0.008299
by deep	0.005714
deep parsing	0.142857
parsing of	0.071429
<s> Open	0.000769
Open	0.000028
Open source	1.000000
source software	0.041667
software tools	0.037037
tools deploy	0.166667
deploy	0.000028
deploy machine	1.000000
statistics ,	0.125000
to automate	0.002656
automate sentiment	0.333333
analysis on	0.015385
on large	0.004717
large collections	0.043478
collections of	0.250000
including web	0.071429
web pages	0.125000
, online	0.001684
online news	0.125000
news ,	0.076923
, internet	0.000561
internet	0.000028
internet discussion	1.000000
discussion groups	0.500000
groups ,	0.200000
, web	0.000561
web blogs	0.125000
blogs	0.000056
blogs ,	0.500000
media .	0.166667
Evaluation The	0.111111
analysis system	0.015385
in principle	0.001873
principle	0.000028
principle ,	1.000000
, how	0.000561
it agrees	0.008547
agrees	0.000028
agrees with	1.000000
human judgments	0.021739
judgments	0.000028
judgments .	1.000000
usually measured	0.031250
by precision	0.005714
human raters	0.021739
raters	0.000028
raters typically	1.000000
typically agree	0.055556
agree about	0.333333
see Inter-rater	0.050000
Inter-rater	0.000028
Inter-rater reliability	1.000000
reliability -RRB-	0.500000
a 70	0.001227
accurate program	0.142857
program is	0.045455
is doing	0.002033
doing as	0.500000
as humans	0.003484
humans ,	0.166667
though such	0.100000
such accuracy	0.008130
accuracy may	0.032258
not sound	0.008929
sound impressive	0.050000
impressive .	0.500000
If a	0.100000
program were	0.045455
were ``	0.024390
`` right	0.005291
right ''	0.100000
'' 100	0.005155
, humans	0.000561
humans would	0.083333
would still	0.018868
still disagree	0.066667
disagree	0.000084
disagree with	0.333333
it about	0.008547
about 30	0.025000
30 %	0.333333
since they	0.100000
they disagree	0.025000
disagree that	0.333333
that much	0.003546
much about	0.045455
about any	0.025000
any answer	0.032258
sophisticated measures	0.142857
applied ,	0.066667
but evaluation	0.014706
analysis systems	0.015385
systems remains	0.008929
remains a	0.250000
complex matter	0.041667
matter .	0.333333
For sentiment	0.016393
analysis tasks	0.015385
tasks returning	0.031250
a scale	0.001227
scale rather	0.166667
binary judgement	0.250000
judgement ,	0.333333
, correlation	0.000561
correlation	0.000056
correlation is	0.500000
a better	0.002454
better measure	0.111111
measure than	0.090909
than precision	0.022222
precision because	0.200000
it takes	0.008547
takes into	0.333333
account how	0.333333
how close	0.034483
close	0.000028
close the	1.000000
the predicted	0.000692
predicted value	0.500000
value is	0.333333
target value	0.090909
value .	0.333333
test the	0.100000
the relationship	0.000692
relationship between	0.166667
between Internet	0.025641
Internet financial	0.500000
financial message	0.250000
message boards	0.500000
boards	0.000028
boards and	1.000000
the behavior	0.000692
behavior of	0.500000
the stock	0.000692
stock market	0.333333
market	0.000084
market to	0.333333
find a	0.153846
a strong	0.002454
strong	0.000112
strong correlation	0.250000
correlation between	0.500000
between posts	0.025641
posts	0.000028
posts and	1.000000
and volume	0.001445
of stock	0.000891
and Web	0.001445
Web 2.0	0.111111
2.0	0.000056
2.0 The	0.500000
The rise	0.005208
rise of	0.500000
media such	0.166667
as blogs	0.003484
blogs and	0.500000
networks has	0.071429
has fueled	0.011905
fueled	0.000028
fueled interest	1.000000
in sentiment	0.001873
the proliferation	0.000692
proliferation	0.000028
proliferation of	1.000000
of reviews	0.000891
, ratings	0.000561
ratings ,	0.111111
, recommendations	0.000561
recommendations	0.000028
recommendations and	1.000000
other forms	0.014286
of online	0.000891
online expression	0.125000
online opinion	0.125000
opinion has	0.200000
has turned	0.011905
turned	0.000056
turned into	1.000000
a kind	0.001227
of virtual	0.000891
virtual	0.000028
virtual currency	1.000000
currency	0.000028
currency for	1.000000
for businesses	0.003610
businesses	0.000056
businesses looking	0.500000
looking to	0.400000
to market	0.001328
market their	0.333333
their products	0.029412
products ,	0.250000
identify new	0.083333
new opportunities	0.041667
opportunities	0.000028
opportunities and	1.000000
and manage	0.001445
manage	0.000028
manage their	1.000000
their reputations	0.029412
reputations	0.000028
reputations .	1.000000
As businesses	0.055556
businesses look	0.500000
look to	0.200000
automate the	0.333333
of filtering	0.000891
filtering	0.000028
filtering out	1.000000
the noise	0.000692
noise	0.000223
noise ,	0.125000
, understanding	0.000561
the conversations	0.000692
conversations ,	0.333333
the relevant	0.000692
relevant content	0.142857
and actioning	0.001445
actioning	0.000028
actioning it	1.000000
it appropriately	0.008547
appropriately	0.000056
appropriately ,	0.500000
many are	0.019231
now looking	0.076923
If web	0.100000
web 2.0	0.125000
2.0 was	0.500000
was all	0.012987
all about	0.023256
about democratizing	0.025000
democratizing	0.000056
democratizing publishing	0.500000
publishing	0.000028
publishing ,	1.000000
web may	0.125000
may well	0.019231
well be	0.035714
be based	0.004219
on democratizing	0.004717
democratizing data	0.500000
mining of	0.200000
content that	0.083333
getting published	0.250000
published .	0.142857
One step	0.076923
step towards	0.066667
towards	0.000028
towards this	1.000000
this aim	0.010989
aim is	0.500000
is accomplished	0.002033
accomplished	0.000028
accomplished in	1.000000
in research	0.001873
Several research	0.333333
research teams	0.023810
teams in	0.500000
in universities	0.001873
universities	0.000028
universities around	1.000000
world currently	0.066667
currently focus	0.142857
on understanding	0.004717
the dynamics	0.000692
dynamics	0.000056
dynamics of	0.500000
in e-communities	0.001873
e-communities	0.000056
e-communities through	0.500000
through sentiment	0.125000
The CyberEmotions	0.005208
CyberEmotions	0.000028
CyberEmotions project	1.000000
, recently	0.000561
recently identified	0.333333
identified the	0.200000
role of	0.250000
of negative	0.000891
negative emotions	0.125000
emotions	0.000028
emotions in	1.000000
in driving	0.001873
driving	0.000028
driving social	1.000000
networks discussions	0.071429
discussions .	0.333333
analysis could	0.015385
could therefore	0.062500
therefore help	0.200000
help understand	0.111111
understand why	0.142857
why certain	0.142857
certain e-communities	0.142857
e-communities die	0.500000
die	0.000028
die or	1.000000
or fade	0.004505
fade	0.000028
fade away	1.000000
away -LRB-	0.500000
, MySpace	0.000561
MySpace	0.000028
MySpace -RRB-	1.000000
-RRB- while	0.002710
others seem	0.083333
seem to	0.500000
to grow	0.001328
grow	0.000028
grow without	1.000000
without limits	0.076923
limits	0.000028
limits -LRB-	1.000000
, Facebook	0.000561
Facebook	0.000028
Facebook -RRB-	1.000000
problem is	0.113636
that most	0.003546
most sentiment	0.017241
analysis algorithms	0.015385
algorithms use	0.028571
use simple	0.013889
simple terms	0.038462
terms to	0.076923
to express	0.001328
express sentiment	0.200000
sentiment about	0.040000
about a	0.025000
a product	0.001227
product or	0.142857
or service	0.004505
service .	0.200000
, cultural	0.000561
cultural	0.000028
cultural factors	1.000000
factors ,	0.333333
, linguistic	0.000561
linguistic nuances	0.062500
nuances	0.000028
nuances and	1.000000
and differing	0.001445
contexts make	0.142857
it extremely	0.008547
to turn	0.001328
turn a	0.166667
a string	0.002454
written text	0.115385
a simple	0.001227
simple pro	0.038462
pro	0.000028
pro or	1.000000
or con	0.004505
con	0.000028
con sentiment	1.000000
sentiment .	0.040000
The fact	0.005208
humans often	0.083333
often disagree	0.022727
disagree on	0.333333
sentiment of	0.040000
text illustrates	0.006289
illustrates how	0.500000
how big	0.034483
big a	0.500000
task it	0.023810
is for	0.002033
for computers	0.003610
computers to	0.111111
get this	0.142857
this right	0.010989
The shorter	0.005208
shorter	0.000056
shorter the	0.500000
the string	0.000692
harder it	0.142857
it becomes	0.034188
becomes	0.000112
becomes .	0.250000
<s> n	0.000769
n	0.000056
n Computer	0.500000
Computer Science	0.166667
Science ,	0.500000
, Speech	0.001684
of spoken	0.001783
spoken words	0.142857
into text	0.038462
`` automatic	0.005291
recognition ''	0.016529
`` ASR	0.005291
ASR	0.000167
ASR ''	0.166667
`` computer	0.005291
computer speech	0.022727
`` speech	0.005291
to text	0.002656
text ''	0.006289
or just	0.004505
just ``	0.111111
`` STT	0.005291
STT	0.000028
STT ''	1.000000
Speech Recognition	0.096774
Recognition is	0.125000
is technology	0.002033
technology that	0.045455
can translate	0.005525
translate spoken	0.166667
Some SR	0.047619
SR	0.000084
SR systems	0.333333
use ``	0.013889
`` training	0.005291
training ''	0.035714
'' where	0.005155
where an	0.028571
an individual	0.007576
individual speaker	0.083333
speaker reads	0.055556
reads	0.000056
reads sections	0.500000
the SR	0.000692
SR system	0.333333
systems analyze	0.008929
analyze the	0.250000
's specific	0.019608
specific voice	0.047619
voice	0.000363
voice and	0.076923
and use	0.004335
to fine	0.001328
fine tune	0.500000
tune	0.000028
tune the	1.000000
that person	0.003546
's speech	0.019608
, resulting	0.000561
resulting in	0.250000
in more	0.001873
accurate transcription	0.142857
transcription	0.000056
transcription .	0.500000
that do	0.003546
use training	0.027778
training are	0.071429
are called	0.008299
`` Speaker	0.010582
Speaker	0.000167
Speaker Independent	0.166667
Independent	0.000028
Independent ''	1.000000
Speaker Dependent	0.166667
Dependent	0.000028
Dependent ''	1.000000
recognition applications	0.008264
include voice	0.037037
voice user	0.076923
interfaces such	0.500000
as voice	0.003484
voice dialing	0.076923
dialing	0.000028
dialing -LRB-	1.000000
`` Call	0.005291
Call	0.000028
Call home	1.000000
home	0.000028
home ''	1.000000
, call	0.000561
call routing	0.333333
routing -LRB-	0.333333
`` I	0.005291
I	0.000028
I would	1.000000
a collect	0.001227
collect	0.000028
collect call	1.000000
call ''	0.333333
, domotic	0.000561
domotic	0.000028
domotic appliance	1.000000
appliance	0.000028
appliance control	1.000000
control ,	0.200000
search -LRB-	0.090909
a podcast	0.001227
podcast	0.000028
podcast where	1.000000
where particular	0.028571
particular words	0.076923
were spoken	0.024390
spoken -RRB-	0.071429
simple data	0.038462
entry -LRB-	0.250000
, entering	0.001123
entering	0.000056
entering a	0.500000
card number	0.250000
, preparation	0.000561
preparation	0.000028
preparation of	1.000000
of structured	0.000891
structured documents	0.166667
a radiology	0.001227
radiology	0.000028
radiology report	1.000000
report -RRB-	0.250000
, speech-to-text	0.000561
speech-to-text	0.000056
speech-to-text processing	0.500000
, word	0.000561
word processors	0.016667
processors	0.000028
processors or	1.000000
or emails	0.004505
emails -RRB-	0.500000
and aircraft	0.001445
aircraft	0.000195
aircraft -LRB-	0.285714
usually termed	0.031250
termed Direct	0.250000
Direct	0.000028
Direct Voice	1.000000
Voice	0.000140
Voice Input	0.200000
Input -RRB-	0.500000
term voice	0.055556
voice recognition	0.076923
recognition refers	0.008264
to finding	0.001328
`` who	0.005291
who ''	0.100000
is speaking	0.002033
than what	0.022222
what they	0.031250
are saying	0.004149
saying	0.000028
saying .	1.000000
<s> Recognizing	0.000769
Recognizing	0.000028
Recognizing the	1.000000
the speaker	0.001384
speaker can	0.055556
can simplify	0.005525
simplify	0.000028
simplify the	1.000000
of translating	0.000891
translating speech	0.250000
been trained	0.014706
trained	0.000084
trained on	0.333333
on specific	0.004717
specific person	0.047619
's voices	0.019608
voices	0.000028
voices or	1.000000
or it	0.004505
to authenticate	0.001328
authenticate	0.000028
authenticate or	1.000000
or verify	0.004505
verify	0.000028
verify the	1.000000
speaker as	0.055556
a security	0.001227
security	0.000028
security process	1.000000
<s> Front-End	0.000769
Front-End	0.000028
Front-End speech	1.000000
is where	0.004065
the provider	0.001384
provider	0.000056
provider dictates	1.000000
dictates	0.000056
dictates into	1.000000
a speech-recognition	0.003681
speech-recognition	0.000084
speech-recognition engine	0.666667
engine ,	0.166667
the recognized	0.001384
recognized words	0.166667
are displayed	0.004149
displayed as	0.500000
are spoken	0.004149
the dictator	0.000692
dictator	0.000028
dictator is	1.000000
is responsible	0.002033
responsible	0.000028
responsible for	1.000000
for editing	0.003610
editing and	0.500000
and signing	0.001445
signing	0.000028
signing off	1.000000
off on	0.500000
<s> Back-End	0.000769
Back-End	0.000028
Back-End or	1.000000
or deferred	0.004505
deferred	0.000028
deferred speech	1.000000
digital dictation	0.142857
dictation	0.000028
dictation system	1.000000
the voice	0.000692
voice is	0.076923
is routed	0.004065
routed	0.000056
routed through	0.500000
speech-recognition machine	0.333333
machine and	0.012658
recognized draft	0.166667
draft	0.000056
draft document	0.500000
document is	0.027778
routed along	0.500000
original voice	0.076923
voice file	0.076923
file	0.000028
file to	1.000000
the editor	0.000692
editor	0.000028
editor ,	1.000000
the draft	0.000692
draft is	0.500000
is edited	0.002033
edited	0.000028
edited and	1.000000
and report	0.001445
report finalized	0.250000
finalized	0.000028
finalized .	1.000000
<s> Deferred	0.000769
Deferred	0.000028
Deferred speech	1.000000
the industry	0.000692
industry currently	0.333333
currently .	0.142857
Many Electronic	0.083333
Electronic	0.000056
Electronic Medical	0.500000
Medical Records	0.500000
Records	0.000028
Records -LRB-	1.000000
-LRB- EMR	0.002710
EMR	0.000084
EMR -RRB-	0.333333
-RRB- applications	0.002710
applications can	0.040000
more effective	0.010526
effective and	0.166667
performed more	0.100000
more easily	0.010526
easily when	0.111111
when deployed	0.028571
deployed in	0.500000
in conjunction	0.003745
conjunction with	0.666667
<s> Searches	0.000769
Searches	0.000028
Searches ,	1.000000
, queries	0.000561
queries ,	0.333333
and form	0.001445
form filling	0.050000
filling	0.000028
filling may	1.000000
may all	0.019231
all be	0.023256
be faster	0.004219
faster	0.000084
faster to	0.333333
perform by	0.090909
by voice	0.011429
voice than	0.076923
than by	0.022222
a keyboard	0.001227
keyboard	0.000084
keyboard .	0.333333
major issues	0.083333
issues relating	0.200000
relating	0.000028
relating to	1.000000
recognition in	0.024793
in healthcare	0.001873
healthcare	0.000028
healthcare is	1.000000
American Recovery	0.200000
Recovery	0.000028
Recovery and	1.000000
and Reinvestment	0.001445
Reinvestment	0.000028
Reinvestment Act	1.000000
Act	0.000028
Act of	1.000000
of 2009	0.000891
2009 -LRB-	0.333333
-LRB- ARRA	0.002710
ARRA	0.000028
ARRA -RRB-	1.000000
-RRB- provides	0.002710
provides	0.000056
provides for	0.500000
for substantial	0.003610
substantial financial	0.200000
financial benefits	0.250000
benefits to	0.500000
to physicians	0.001328
physicians	0.000028
physicians who	1.000000
who utilize	0.100000
utilize	0.000056
utilize an	0.500000
an EMR	0.007576
EMR according	0.333333
`` Meaningful	0.005291
Meaningful	0.000028
Meaningful Use	1.000000
Use ''	0.500000
'' standards	0.005155
These standards	0.058824
standards require	0.200000
require that	0.045455
a substantial	0.001227
substantial amount	0.200000
data be	0.012987
be maintained	0.004219
maintained	0.000056
maintained by	0.500000
the EMR	0.000692
EMR -LRB-	0.333333
-LRB- now	0.008130
now more	0.076923
commonly referred	0.125000
an Electronic	0.007576
Electronic Health	0.500000
Health Record	0.500000
Record	0.000028
Record or	1.000000
or EHR	0.004505
EHR	0.000084
EHR -RRB-	0.333333
<s> Unfortunately	0.000769
Unfortunately	0.000028
Unfortunately ,	1.000000
many instances	0.019231
instances ,	0.333333
recognition within	0.008264
within an	0.055556
an EHR	0.007576
EHR will	0.333333
not lead	0.008929
data maintained	0.012987
maintained within	0.500000
database ,	0.100000
rather to	0.062500
to narrative	0.001328
narrative	0.000028
narrative text	1.000000
this reason	0.021978
reason ,	0.500000
, substantial	0.000561
substantial resources	0.200000
resources are	0.166667
being expended	0.055556
expended	0.000028
expended to	1.000000
allow for	0.200000
of front-end	0.000891
front-end	0.000028
front-end SR	1.000000
SR while	0.333333
while capturing	0.050000
capturing	0.000028
capturing data	1.000000
data within	0.012987
the EHR	0.000692
EHR .	0.333333
<s> Military	0.000769
Military	0.000028
Military High-performance	1.000000
High-performance	0.000028
High-performance fighter	1.000000
fighter	0.000167
fighter aircraft	0.500000
aircraft Substantial	0.142857
Substantial	0.000056
Substantial efforts	0.500000
been devoted	0.014706
devoted in	0.200000
last decade	0.400000
decade	0.000084
decade to	0.333333
test and	0.200000
in fighter	0.005618
aircraft .	0.142857
<s> Of	0.000769
Of	0.000028
Of particular	1.000000
particular note	0.076923
note	0.000028
note is	1.000000
U.S. program	0.142857
program in	0.090909
in speech	0.013109
recognition for	0.008264
the Advanced	0.000692
Advanced Fighter	0.200000
Fighter	0.000028
Fighter Technology	1.000000
Technology	0.000084
Technology Integration	0.333333
Integration	0.000028
Integration -LRB-	1.000000
-LRB- AFTI	0.002710
AFTI	0.000028
AFTI -RRB-	1.000000
-RRB- \/	0.002710
\/ F-16	0.333333
F-16	0.000056
F-16 aircraft	0.500000
-LRB- F-16	0.002710
F-16 VISTA	0.500000
VISTA	0.000028
VISTA -RRB-	1.000000
in France	0.003745
France installing	0.250000
installing	0.000028
installing speech	1.000000
systems on	0.008929
on Mirage	0.004717
Mirage	0.000028
Mirage aircraft	1.000000
aircraft ,	0.285714
and also	0.001445
also programs	0.014493
programs in	0.090909
UK dealing	0.250000
of aircraft	0.000891
aircraft platforms	0.142857
platforms	0.000028
platforms .	1.000000
In these	0.009524
these programs	0.023810
speech recognizers	0.006579
recognizers	0.000056
recognizers have	0.500000
been operated	0.014706
operated successfully	0.500000
successfully in	0.333333
with applications	0.005464
applications including	0.040000
: setting	0.009804
setting radio	0.200000
radio	0.000028
radio frequencies	1.000000
frequencies ,	0.500000
, commanding	0.000561
commanding	0.000028
commanding an	1.000000
an autopilot	0.007576
autopilot	0.000028
autopilot system	1.000000
, setting	0.001123
setting steer-point	0.200000
steer-point	0.000028
steer-point coordinates	1.000000
coordinates	0.000028
coordinates and	1.000000
and weapons	0.001445
weapons	0.000028
weapons release	1.000000
release parameters	0.333333
parameters ,	0.250000
and controlling	0.001445
controlling	0.000028
controlling flight	1.000000
flight displays	0.500000
displays	0.000028
displays .	1.000000
<s> Working	0.000769
Working	0.000028
Working with	1.000000
with Swedish	0.005464
Swedish	0.000028
Swedish pilots	1.000000
pilots	0.000056
pilots flying	0.500000
flying	0.000028
flying in	1.000000
the JAS-39	0.000692
JAS-39	0.000028
JAS-39 Gripen	1.000000
Gripen	0.000028
Gripen cockpit	1.000000
cockpit	0.000056
cockpit ,	0.500000
, Englund	0.000561
Englund	0.000028
Englund -LRB-	1.000000
-LRB- 2004	0.002710
2004 -RRB-	0.333333
-RRB- found	0.002710
found recognition	0.071429
recognition deteriorated	0.008264
deteriorated	0.000028
deteriorated with	1.000000
with increasing	0.005464
increasing G-loads	0.333333
G-loads	0.000028
G-loads .	1.000000
It was	0.052632
also concluded	0.014493
that adaptation	0.003546
adaptation	0.000084
adaptation greatly	0.333333
greatly improved	0.142857
improved the	0.250000
results in	0.047619
in all	0.005618
all cases	0.023256
cases and	0.055556
and introducing	0.001445
introducing	0.000028
introducing models	1.000000
for breathing	0.003610
breathing	0.000028
breathing was	1.000000
improve recognition	0.153846
recognition scores	0.008264
scores significantly	0.200000
significantly	0.000028
significantly .	1.000000
what might	0.031250
be expected	0.012658
expected ,	0.142857
no effects	0.076923
effects	0.000028
effects of	1.000000
the broken	0.000692
broken	0.000140
broken English	0.200000
English of	0.027027
the speakers	0.000692
speakers were	0.250000
found .	0.071429
was evident	0.012987
evident that	0.500000
that spontaneous	0.003546
spontaneous	0.000084
spontaneous speech	1.000000
speech caused	0.006579
caused	0.000028
caused problems	1.000000
problems for	0.058824
the recognizer	0.000692
recognizer	0.000028
recognizer ,	1.000000
as could	0.003484
expected .	0.142857
A restricted	0.020000
restricted vocabulary	0.250000
vocabulary ,	0.125000
and above	0.001445
above all	0.076923
a proper	0.001227
proper syntax	0.142857
, could	0.000561
could thus	0.062500
thus be	0.100000
accuracy substantially	0.032258
substantially	0.000028
substantially .	1.000000
The Eurofighter	0.005208
Eurofighter	0.000028
Eurofighter Typhoon	1.000000
Typhoon	0.000028
Typhoon currently	1.000000
currently in	0.142857
in service	0.001873
service with	0.200000
UK RAF	0.250000
RAF	0.000028
RAF employs	1.000000
employs a	0.500000
a speaker-dependent	0.001227
speaker-dependent	0.000028
speaker-dependent system	1.000000
i.e. it	0.052632
requires each	0.062500
each pilot	0.022222
pilot	0.000140
pilot to	0.400000
template .	0.250000
for any	0.003610
any safety	0.032258
safety	0.000028
safety critical	1.000000
critical or	0.250000
or weapon	0.004505
weapon	0.000056
weapon critical	0.500000
critical tasks	0.250000
as weapon	0.003484
weapon release	0.500000
release or	0.333333
or lowering	0.004505
lowering	0.000028
lowering of	1.000000
the undercarriage	0.000692
undercarriage	0.000028
undercarriage ,	1.000000
other cockpit	0.014286
cockpit functions	0.500000
functions .	0.500000
<s> Voice	0.000769
Voice commands	0.200000
commands are	0.200000
are confirmed	0.004149
confirmed	0.000028
confirmed by	1.000000
by visual	0.005714
visual and\/or	0.500000
and\/or aural	0.333333
aural	0.000028
aural feedback	1.000000
feedback .	0.500000
is seen	0.002033
major design	0.083333
design feature	0.250000
feature in	0.076923
the reduction	0.000692
reduction	0.000056
reduction of	0.500000
of pilot	0.000891
pilot workload	0.200000
workload	0.000028
workload ,	1.000000
even allows	0.037037
the pilot	0.000692
assign targets	0.200000
targets	0.000028
targets to	1.000000
to himself	0.001328
himself with	0.500000
two simple	0.034483
simple voice	0.038462
voice commands	0.076923
commands or	0.200000
his wingmen	0.083333
wingmen	0.000028
wingmen with	1.000000
with only	0.005464
only five	0.026316
five commands	0.200000
commands .	0.400000
<s> Speaker	0.001537
Speaker independent	0.166667
independent	0.000056
independent systems	0.500000
also being	0.014493
being developed	0.055556
developed and	0.038462
in testing	0.001873
testing for	0.200000
the F35	0.000692
F35	0.000028
F35 Lightning	1.000000
Lightning	0.000028
Lightning II	1.000000
II -LRB-	0.500000
-LRB- JSF	0.002710
JSF	0.000028
JSF -RRB-	1.000000
the Alenia	0.000692
Alenia	0.000028
Alenia Aermacchi	1.000000
Aermacchi	0.000028
Aermacchi M-346	1.000000
M-346	0.000028
M-346 Master	1.000000
Master	0.000028
Master lead-in	1.000000
lead-in	0.000028
lead-in fighter	1.000000
fighter trainer	0.166667
trainer	0.000028
trainer .	1.000000
have produced	0.009615
produced word	0.111111
word accuracies	0.016667
accuracies	0.000028
accuracies in	1.000000
in excess	0.003745
excess	0.000056
excess of	1.000000
of 98	0.000891
<s> Helicopters	0.000769
Helicopters	0.000028
Helicopters The	1.000000
The problems	0.005208
problems of	0.058824
achieving high	0.500000
high recognition	0.055556
accuracy under	0.032258
under stress	0.200000
stress	0.000056
stress and	0.500000
and noise	0.001445
noise pertain	0.125000
pertain	0.000028
pertain strongly	1.000000
strongly to	0.500000
the helicopter	0.002076
helicopter	0.000112
helicopter environment	0.500000
environment as	0.166667
the jet	0.000692
jet	0.000028
jet fighter	1.000000
fighter environment	0.166667
environment .	0.333333
The acoustic	0.005208
acoustic	0.000167
acoustic noise	0.333333
noise problem	0.125000
actually more	0.333333
more severe	0.010526
severe	0.000028
severe in	1.000000
environment ,	0.166667
only because	0.026316
the high	0.001384
high noise	0.055556
noise levels	0.125000
levels but	0.045455
also because	0.014493
helicopter pilot	0.250000
pilot ,	0.200000
not wear	0.008929
wear	0.000028
wear a	1.000000
a facemask	0.001227
facemask	0.000028
facemask ,	1.000000
would reduce	0.018868
reduce	0.000028
reduce acoustic	1.000000
noise in	0.125000
the microphone	0.000692
microphone	0.000028
microphone .	1.000000
<s> Substantial	0.000769
Substantial test	0.500000
evaluation programs	0.018519
programs have	0.090909
been carried	0.014706
carried out	0.500000
the past	0.001384
past decade	0.333333
decade in	0.333333
systems applications	0.008929
in helicopters	0.003745
helicopters	0.000056
helicopters ,	0.500000
, notably	0.000561
notably by	0.333333
U.S. Army	0.142857
Army Avionics	0.250000
Avionics	0.000028
Avionics Research	1.000000
Research and	0.125000
and Development	0.001445
Development	0.000028
Development Activity	1.000000
Activity	0.000028
Activity -LRB-	1.000000
-LRB- AVRADA	0.002710
AVRADA	0.000056
AVRADA -RRB-	0.500000
and by	0.001445
the Royal	0.001384
Royal	0.000056
Royal Aerospace	0.500000
Aerospace	0.000056
Aerospace Establishment	0.500000
Establishment	0.000028
Establishment -LRB-	1.000000
-LRB- RAE	0.002710
RAE	0.000028
RAE -RRB-	1.000000
Work in	0.500000
France has	0.250000
has included	0.011905
included speech	0.125000
the Puma	0.000692
Puma	0.000028
Puma helicopter	1.000000
helicopter .	0.250000
There has	0.181818
been much	0.014706
much useful	0.045455
useful work	0.071429
in Canada	0.001873
Canada .	0.166667
<s> Results	0.000769
Results	0.000028
Results have	1.000000
been encouraging	0.014706
encouraging	0.000028
encouraging ,	1.000000
and voice	0.001445
voice applications	0.076923
have included	0.009615
included :	0.125000
: control	0.009804
control of	0.600000
communication radios	0.200000
radios	0.000028
radios ,	1.000000
setting of	0.200000
of navigation	0.000891
navigation systems	0.500000
an automated	0.007576
automated target	0.142857
target handover	0.090909
handover	0.000028
handover system	1.000000
fighter applications	0.166667
the overriding	0.000692
overriding	0.000028
overriding issue	1.000000
issue for	0.125000
for voice	0.003610
voice in	0.076923
helicopters is	0.500000
impact on	0.500000
on pilot	0.004717
pilot effectiveness	0.200000
effectiveness .	0.333333
<s> Encouraging	0.000769
Encouraging	0.000028
Encouraging results	1.000000
are reported	0.004149
reported for	0.200000
the AVRADA	0.000692
AVRADA tests	0.500000
tests ,	0.250000
although these	0.166667
these represent	0.023810
represent only	0.111111
a feasibility	0.001227
feasibility	0.000056
feasibility demonstration	0.500000
demonstration in	0.200000
test environment	0.100000
Much remains	0.333333
remains to	0.250000
done both	0.090909
both in	0.032258
in overall	0.001873
overall speech	0.166667
recognition technology	0.008264
to consistently	0.001328
consistently achieve	0.333333
achieve performance	0.500000
performance improvements	0.055556
improvements	0.000056
improvements in	0.500000
in operational	0.001873
operational	0.000028
operational settings	1.000000
settings	0.000028
settings .	1.000000
<s> Battle	0.000769
Battle	0.000056
Battle management	0.500000
management Question	0.142857
Question book-new	0.142857
book-new	0.000028
book-new .	1.000000
<s> svg	0.000769
svg	0.000028
svg This	1.000000
This unreferenced	0.015873
unreferenced	0.000028
unreferenced section	1.000000
section requires	0.333333
requires citations	0.062500
to ensure	0.001328
ensure	0.000028
ensure verifiability	1.000000
verifiability	0.000028
verifiability .	1.000000
, Battle	0.000561
Battle Management	0.500000
Management	0.000028
Management command	1.000000
command centres	0.500000
centres	0.000028
centres require	1.000000
require rapid	0.045455
rapid	0.000028
rapid access	1.000000
to and	0.001328
large ,	0.043478
, rapidly	0.000561
rapidly changing	0.500000
changing	0.000028
changing information	1.000000
information databases	0.021739
<s> Commanders	0.000769
Commanders	0.000028
Commanders and	1.000000
and system	0.001445
system operators	0.010753
operators	0.000028
operators need	1.000000
to query	0.001328
query these	0.333333
these databases	0.023810
databases as	0.125000
as conveniently	0.003484
conveniently	0.000028
conveniently as	1.000000
an eyes-busy	0.007576
eyes-busy	0.000028
eyes-busy environment	1.000000
environment where	0.166667
where much	0.028571
is presented	0.004065
a display	0.001227
display	0.000056
display format	0.500000
format .	0.500000
<s> Human-machine	0.000769
Human-machine	0.000028
Human-machine interaction	1.000000
interaction by	0.125000
voice has	0.076923
the potential	0.002076
potential to	0.285714
very useful	0.048780
these environments	0.023810
environments	0.000056
environments .	1.000000
of efforts	0.000891
been undertaken	0.014706
undertaken to	0.500000
to interface	0.001328
interface commercially	0.250000
commercially	0.000056
commercially available	1.000000
available isolated-word	0.058824
isolated-word	0.000028
isolated-word recognizers	1.000000
recognizers into	0.500000
into battle	0.012821
battle	0.000056
battle management	1.000000
management environments	0.142857
In one	0.009524
one feasibility	0.015385
feasibility study	0.500000
study ,	0.250000
recognition equipment	0.008264
equipment was	0.333333
was tested	0.012987
tested in	0.500000
an integrated	0.007576
integrated information	0.333333
information display	0.021739
display for	0.500000
for naval	0.003610
naval	0.000084
naval battle	0.333333
management applications	0.142857
<s> Users	0.000769
Users	0.000028
Users were	1.000000
very optimistic	0.024390
optimistic	0.000028
optimistic about	1.000000
although capabilities	0.166667
capabilities were	0.200000
were limited	0.024390
Speech understanding	0.032258
understanding programs	0.030303
programs sponsored	0.090909
sponsored	0.000056
sponsored by	0.500000
the Defense	0.000692
Defense	0.000028
Defense Advanced	1.000000
Advanced Research	0.200000
Research Projects	0.125000
Projects	0.000028
Projects Agency	1.000000
Agency -LRB-	0.500000
-LRB- DARPA	0.002710
DARPA -RRB-	0.250000
U.S. has	0.142857
speech interface	0.006579
interface .	0.250000
recognition efforts	0.008264
continuous speech	0.500000
-LRB- CSR	0.005420
CSR	0.000084
CSR -RRB-	0.666667
, large-vocabulary	0.000561
large-vocabulary	0.000084
large-vocabulary speech	0.666667
speech designed	0.006579
be representative	0.004219
representative	0.000028
representative of	1.000000
the naval	0.000692
naval resource	0.666667
resource management	0.400000
management task	0.142857
<s> Significant	0.000769
Significant	0.000028
Significant advances	1.000000
advances	0.000028
advances in	1.000000
the state-of-the-art	0.000692
state-of-the-art in	0.500000
in CSR	0.001873
CSR have	0.333333
and current	0.001445
current efforts	0.142857
efforts are	0.142857
are focused	0.004149
on integrating	0.004717
integrating	0.000028
integrating speech	1.000000
processing to	0.018519
allow spoken	0.200000
language interaction	0.006757
interaction with	0.125000
a naval	0.001227
management system	0.142857
<s> Training	0.000769
Training	0.000056
Training air	0.500000
air	0.000140
air traffic	0.600000
traffic	0.000084
traffic controllers	1.000000
controllers	0.000084
controllers Training	0.333333
Training for	0.500000
for air	0.003610
controllers -LRB-	0.333333
-LRB- ATC	0.002710
ATC	0.000140
ATC -RRB-	0.200000
-RRB- represents	0.002710
represents an	0.250000
an excellent	0.007576
excellent	0.000028
excellent application	1.000000
application for	0.071429
for speech	0.014440
Many ATC	0.083333
ATC training	0.400000
training systems	0.035714
systems currently	0.008929
currently require	0.142857
person to	0.105263
to act	0.002656
`` pseudo-pilot	0.005291
pseudo-pilot	0.000056
pseudo-pilot ''	0.500000
, engaging	0.000561
engaging	0.000028
engaging in	1.000000
a voice	0.001227
voice dialog	0.076923
dialog	0.000056
dialog with	0.500000
the trainee	0.000692
trainee	0.000028
trainee controller	1.000000
controller	0.000112
controller ,	0.500000
which simulates	0.007246
simulates	0.000028
simulates the	1.000000
the dialog	0.000692
dialog that	0.500000
the controller	0.001384
controller would	0.250000
to conduct	0.001328
conduct	0.000028
conduct with	1.000000
with pilots	0.005464
pilots in	0.500000
real ATC	0.111111
ATC situation	0.200000
situation .	0.500000
and synthesis	0.001445
synthesis	0.000028
synthesis techniques	1.000000
techniques offer	0.043478
offer	0.000028
offer the	1.000000
eliminate the	0.500000
as pseudo-pilot	0.003484
pseudo-pilot ,	0.500000
thus reducing	0.100000
reducing	0.000056
reducing training	0.500000
training and	0.035714
and support	0.001445
support personnel	0.250000
personnel	0.000028
personnel .	1.000000
, Air	0.000561
Air controller	0.333333
controller tasks	0.250000
also characterized	0.014493
characterized by	0.500000
by highly	0.005714
highly structured	0.111111
structured speech	0.166667
speech as	0.013158
primary output	0.500000
, hence	0.000561
hence reducing	0.500000
reducing the	0.500000
the difficulty	0.000692
recognition task	0.016529
task should	0.023810
In practice	0.009524
practice ,	0.500000
is rarely	0.002033
rarely the	0.333333
The FAA	0.005208
FAA	0.000056
FAA document	0.500000
document 7110.65	0.027778
7110.65	0.000028
7110.65 details	1.000000
details the	0.500000
the phrases	0.000692
phrases that	0.062500
by air	0.011429
controllers .	0.333333
While this	0.200000
this document	0.010989
document gives	0.027778
gives less	0.500000
than 150	0.022222
150 examples	0.500000
such phrases	0.008130
of phrases	0.000891
phrases supported	0.062500
supported	0.000028
supported by	1.000000
by one	0.005714
the simulation	0.000692
simulation vendors	0.333333
vendors speech	0.250000
of 500,000	0.000891
500,000	0.000028
500,000 .	1.000000
The USAF	0.005208
USAF	0.000028
USAF ,	1.000000
, USMC	0.000561
USMC	0.000028
USMC ,	1.000000
, US	0.001123
US Army	0.142857
Army ,	0.250000
US Navy	0.142857
Navy	0.000028
Navy ,	1.000000
and FAA	0.001445
FAA as	0.500000
of international	0.000891
international ATC	0.500000
training organizations	0.035714
organizations	0.000028
organizations such	1.000000
Royal Australian	0.500000
Australian Air	0.500000
Force and	0.500000
and Civil	0.001445
Civil	0.000028
Civil Aviation	1.000000
Aviation	0.000028
Aviation Authorities	1.000000
Authorities	0.000028
Authorities in	1.000000
in Italy	0.001873
, Brazil	0.000561
Brazil	0.000028
Brazil ,	1.000000
and Canada	0.001445
Canada are	0.166667
currently using	0.142857
using ATC	0.016949
ATC simulators	0.200000
simulators	0.000028
simulators with	1.000000
with speech	0.010929
recognition from	0.016529
different vendors	0.020408
vendors .	0.250000
<s> Telephony	0.000769
Telephony	0.000028
Telephony and	1.000000
other domains	0.014286
domains ASR	0.125000
ASR in	0.500000
of telephony	0.000891
telephony	0.000084
telephony is	0.333333
now commonplace	0.076923
commonplace	0.000028
commonplace and	1.000000
computer gaming	0.022727
gaming	0.000028
gaming and	1.000000
and simulation	0.001445
simulation is	0.333333
is becoming	0.002033
becoming	0.000028
becoming more	1.000000
more widespread	0.010526
widespread	0.000028
widespread .	1.000000
<s> Despite	0.000769
Despite	0.000028
Despite the	1.000000
of integration	0.000891
integration	0.000028
integration with	1.000000
with word	0.016393
word processing	0.016667
general personal	0.045455
personal computing	0.250000
computing	0.000056
computing .	0.500000
, ASR	0.000561
of document	0.000891
document production	0.027778
production has	0.333333
not seen	0.008929
seen the	0.100000
expected -LRB-	0.142857
-LRB- by	0.005420
by whom	0.005714
whom ?	0.500000
<s> increases	0.000769
The improvement	0.005208
of mobile	0.000891
mobile	0.000056
mobile processor	0.500000
processor	0.000028
processor speeds	1.000000
speeds	0.000056
speeds made	0.500000
made feasible	0.062500
feasible	0.000056
feasible the	0.500000
the speech-enabled	0.000692
speech-enabled	0.000028
speech-enabled Symbian	1.000000
Symbian	0.000028
Symbian and	1.000000
and Windows	0.001445
Windows	0.000028
Windows Mobile	1.000000
Mobile Smartphones	0.333333
Smartphones	0.000028
Smartphones .	1.000000
Speech is	0.064516
used mostly	0.008850
mostly as	0.500000
of User	0.000891
User Interface	0.500000
Interface	0.000028
Interface ,	1.000000
for creating	0.003610
creating pre-defined	0.142857
pre-defined or	0.500000
or custom	0.004505
custom	0.000056
custom speech	0.500000
speech commands	0.006579
<s> Leading	0.000769
Leading	0.000028
Leading software	1.000000
software vendors	0.037037
vendors in	0.250000
field are	0.037037
: Microsoft	0.009804
Microsoft	0.000056
Microsoft Corporation	0.500000
-LRB- Microsoft	0.002710
Microsoft Voice	0.500000
Voice Command	0.200000
Command	0.000056
Command -RRB-	0.500000
, Digital	0.000561
Digital	0.000028
Digital Syphon	1.000000
Syphon	0.000028
Syphon -LRB-	1.000000
-LRB- Sonic	0.002710
Sonic	0.000028
Sonic Extractor	1.000000
Extractor	0.000028
Extractor -RRB-	1.000000
, Nuance	0.000561
-LRB- Nuance	0.002710
Nuance Voice	0.333333
Voice Control	0.200000
Control	0.000028
Control -RRB-	1.000000
Speech Technology	0.032258
Technology Center	0.333333
Center	0.000028
Center ,	1.000000
, Vito	0.000561
Vito	0.000028
Vito Technology	1.000000
Technology -LRB-	0.333333
-LRB- VITO	0.002710
VITO	0.000028
VITO Voice2Go	1.000000
Voice2Go	0.000028
Voice2Go -RRB-	1.000000
, Speereo	0.000561
Speereo	0.000056
Speereo Software	0.500000
Software -LRB-	0.500000
-LRB- Speereo	0.002710
Speereo Voice	0.500000
Voice Translator	0.200000
Translator	0.000028
Translator -RRB-	1.000000
, Verbyx	0.000561
Verbyx	0.000028
Verbyx VRX	1.000000
VRX	0.000028
VRX and	1.000000
and SVOX	0.001445
SVOX	0.000028
SVOX .	1.000000
Further applications	0.333333
applications Aerospace	0.040000
Aerospace -LRB-	0.500000
e.g. space	0.017857
space exploration	0.200000
exploration	0.000028
exploration ,	1.000000
, spacecraft	0.000561
spacecraft	0.000028
spacecraft ,	1.000000
-RRB- NASA	0.002710
NASA	0.000028
NASA 's	1.000000
's Mars	0.019608
Mars	0.000056
Mars Polar	0.500000
Polar	0.000028
Polar Lander	1.000000
Lander	0.000056
Lander used	0.500000
used speech	0.008850
from technology	0.009615
technology Sensory	0.045455
Sensory	0.000028
Sensory ,	1.000000
Inc. in	0.500000
the Mars	0.000692
Mars Microphone	0.500000
Microphone	0.000028
Microphone on	1.000000
the Lander	0.000692
Lander Automatic	0.500000
Automatic translation	0.111111
translation Automotive	0.013514
Automotive	0.000028
Automotive speech	1.000000
, OnStar	0.000561
OnStar	0.000028
OnStar ,	1.000000
, Ford	0.000561
Ford	0.000028
Ford Sync	1.000000
Sync	0.000028
Sync -RRB-	1.000000
-RRB- Court	0.002710
Court	0.000056
Court reporting	1.000000
reporting -LRB-	0.333333
-LRB- Realtime	0.002710
Realtime	0.000028
Realtime Speech	1.000000
Speech Writing	0.032258
Writing	0.000028
Writing -RRB-	1.000000
-RRB- Hands-free	0.002710
Hands-free	0.000028
Hands-free computing	1.000000
computing :	0.500000
: Speech	0.009804
computer user	0.022727
user interface	0.071429
interface Home	0.250000
Home	0.000028
Home automation	1.000000
automation	0.000028
automation Interactive	1.000000
Interactive voice	0.500000
voice response	0.076923
response Mobile	0.500000
Mobile telephony	0.333333
telephony ,	0.666667
including mobile	0.071429
mobile email	0.500000
email Multimodal	0.500000
Multimodal	0.000028
Multimodal interaction	1.000000
interaction Pronunciation	0.125000
Pronunciation	0.000028
Pronunciation evaluation	1.000000
in computer-aided	0.001873
computer-aided language	0.333333
language learning	0.006757
learning applications	0.023256
applications Robotics	0.040000
Robotics	0.000028
Robotics Speech-to-text	1.000000
Speech-to-text	0.000028
Speech-to-text reporter	1.000000
reporter	0.000028
reporter -LRB-	1.000000
-LRB- transcription	0.002710
transcription of	0.500000
speech into	0.013158
video captioning	0.200000
captioning	0.000028
captioning ,	1.000000
, Court	0.000561
reporting -RRB-	0.333333
-RRB- Telematics	0.002710
Telematics	0.000028
Telematics -LRB-	1.000000
, vehicle	0.000561
vehicle	0.000028
vehicle Navigation	1.000000
Navigation	0.000028
Navigation Systems	1.000000
Systems -RRB-	0.083333
-RRB- Transcription	0.002710
Transcription	0.000028
Transcription -LRB-	1.000000
-LRB- digital	0.002710
digital speech-to-text	0.142857
speech-to-text -RRB-	0.500000
-RRB- Video	0.002710
Video	0.000028
Video games	1.000000
games	0.000028
games ,	1.000000
with Tom	0.005464
Tom	0.000028
Tom Clancy	1.000000
Clancy	0.000028
Clancy 's	1.000000
's EndWar	0.019608
EndWar	0.000028
EndWar and	1.000000
and Lifeline	0.001445
Lifeline	0.000028
Lifeline as	1.000000
as working	0.003484
working examples	0.142857
examples Performance	0.041667
Performance	0.000028
Performance The	1.000000
The performance	0.005208
usually evaluated	0.031250
evaluated in	0.142857
of accuracy	0.001783
accuracy and	0.032258
and speed	0.002890
speed .	0.428571
Accuracy is	0.142857
usually rated	0.031250
rated	0.000028
rated with	1.000000
word error	0.016667
rate -LRB-	0.090909
-LRB- WER	0.002710
WER	0.000028
WER -RRB-	1.000000
whereas speed	0.333333
speed is	0.142857
measured with	0.166667
real time	0.111111
time factor	0.030303
factor .	0.500000
Other measures	0.142857
measures of	0.166667
accuracy include	0.032258
include Single	0.037037
Single	0.000028
Single Word	1.000000
Word Error	0.142857
Error	0.000056
Error Rate	0.500000
Rate	0.000056
Rate -LRB-	1.000000
-LRB- SWER	0.002710
SWER	0.000028
SWER -RRB-	1.000000
and Command	0.001445
Command Success	0.500000
Success	0.000028
Success Rate	1.000000
machine -RRB-	0.012658
very complex	0.024390
complex problem	0.041667
<s> Vocalizations	0.000769
Vocalizations	0.000028
Vocalizations vary	1.000000
vary in	0.500000
of accent	0.000891
accent	0.000028
accent ,	1.000000
, pronunciation	0.000561
pronunciation	0.000028
pronunciation ,	1.000000
, articulation	0.000561
articulation	0.000028
articulation ,	1.000000
, roughness	0.000561
roughness	0.000028
roughness ,	1.000000
, nasality	0.000561
nasality	0.000028
nasality ,	1.000000
, pitch	0.000561
pitch	0.000028
pitch ,	1.000000
, volume	0.000561
volume ,	0.250000
is distorted	0.004065
distorted	0.000056
distorted by	0.500000
a background	0.001227
background noise	0.333333
noise and	0.125000
and echoes	0.001445
echoes	0.000056
echoes ,	1.000000
, electrical	0.000561
electrical	0.000028
electrical characteristics	1.000000
characteristics .	0.500000
recognition vary	0.008264
vary with	0.166667
following :	0.133333
: Vocabulary	0.009804
Vocabulary	0.000084
Vocabulary size	0.333333
and confusability	0.001445
confusability	0.000028
confusability Speaker	1.000000
Speaker dependence	0.166667
dependence	0.000028
dependence vs.	1.000000
vs. independence	0.083333
independence	0.000028
independence Isolated	1.000000
Isolated	0.000056
Isolated ,	1.000000
, discontinuous	0.000561
discontinuous	0.000084
discontinuous ,	0.333333
or continuous	0.004505
speech Task	0.006579
Task and	0.666667
and language	0.004335
language constraints	0.006757
constraints	0.000112
constraints Read	0.250000
Read	0.000056
Read vs.	1.000000
vs. spontaneous	0.083333
speech Adverse	0.006579
Adverse	0.000056
Adverse conditions	1.000000
conditions Accuracy	0.200000
recognition As	0.008264
earlier in	0.250000
article accuracy	0.034483
speech recogniton	0.006579
recogniton	0.000056
recogniton vary	0.500000
in following	0.001873
: Error	0.009804
Error Rates	0.500000
Rates	0.000028
Rates Increase	1.000000
Increase	0.000028
Increase as	1.000000
the Vocabulary	0.000692
Vocabulary Size	0.333333
Size	0.000028
Size Grows	1.000000
Grows	0.000028
Grows :	1.000000
: e.g.	0.019608
e.g. The	0.035714
The 10	0.005208
10 digits	0.125000
digits	0.000028
digits ``	1.000000
`` zero	0.005291
zero	0.000028
zero ''	1.000000
`` nine	0.005291
nine	0.000028
nine ''	1.000000
be recognized	0.008439
recognized essentially	0.166667
essentially perfectly	0.125000
perfectly	0.000028
perfectly ,	1.000000
but vocabulary	0.014706
vocabulary sizes	0.125000
of 200	0.000891
200 ,	0.500000
, 5000	0.000561
5000	0.000028
5000 or	1.000000
or 100000	0.004505
100000	0.000028
100000 may	1.000000
have error	0.009615
error rates	0.250000
of 3	0.000891
3 %	0.200000
, 7	0.000561
7 %	0.142857
% or	0.025641
or 45	0.004505
45	0.000028
45 %	1.000000
<s> Vocabulary	0.000769
Vocabulary is	0.333333
is Hard	0.002033
Hard	0.000056
Hard to	0.500000
to Recognize	0.001328
Recognize	0.000028
Recognize if	1.000000
it Contains	0.008547
Contains	0.000028
Contains Confusable	1.000000
Confusable	0.000028
Confusable Words	1.000000
Words :	0.250000
The 26	0.005208
26	0.000028
26 letters	1.000000
alphabet are	0.333333
are difficult	0.004149
discriminate because	0.333333
are confusable	0.004149
confusable	0.000028
confusable words	1.000000
most notoriously	0.017241
notoriously	0.000028
notoriously ,	1.000000
the E-set	0.000692
E-set	0.000028
E-set :	1.000000
`` B	0.005291
B	0.000028
B ,	1.000000
, C	0.000561
C	0.000028
C ,	1.000000
, D	0.000561
D	0.000028
D ,	1.000000
, E	0.000561
E	0.000028
E ,	1.000000
, G	0.000561
G	0.000028
G ,	1.000000
, P	0.000561
P ,	0.500000
, T	0.000561
T ,	0.166667
, V	0.000561
V	0.000028
V ,	1.000000
, Z	0.000561
Z	0.000028
Z ''	1.000000
; An	0.021277
An 8	0.062500
8	0.000028
8 %	1.000000
% error	0.025641
rate is	0.181818
considered good	0.111111
good for	0.076923
this vocabulary	0.010989
vocabulary .	0.250000
Speaker Dependence	0.166667
Dependence	0.000028
Dependence vs.	1.000000
vs. Independence	0.083333
Independence	0.000028
Independence :	1.000000
: A	0.009804
A speaker	0.040000
speaker dependent	0.055556
dependent system	0.333333
is intended	0.004065
intended for	0.400000
for use	0.007220
use by	0.027778
single speaker	0.071429
speaker independent	0.055556
independent system	0.500000
by any	0.005714
any speaker	0.032258
speaker ,	0.055556
<s> Isolated	0.000769
, Discontinuous	0.000561
Discontinuous	0.000028
Discontinuous or	1.000000
or Continuous	0.004505
Continuous	0.000028
Continuous speech	1.000000
speech With	0.006579
With isolated	0.142857
isolated speech	0.400000
speech single	0.006579
single words	0.071429
, therefore	0.001684
therefore it	0.600000
becomes easier	0.500000
easier to	0.250000
to recognize	0.007968
recognize the	0.444444
With discontinuous	0.142857
discontinuous speech	0.666667
speech full	0.006579
full sentenced	0.200000
sentenced	0.000028
sentenced separated	1.000000
by silence	0.005714
silence	0.000028
silence are	1.000000
with isolated	0.005464
With continuous	0.142857
speech naturally	0.006579
naturally spoken	0.500000
spoken sentences	0.071429
becomes harder	0.250000
from both	0.009615
both isloated	0.032258
isloated	0.000028
isloated and	1.000000
and discontinuous	0.001445
<s> Task	0.000769
and Language	0.005780
Language Constraints	0.083333
Constraints	0.000084
Constraints e.g.	0.333333
e.g. Querying	0.017857
Querying	0.000028
Querying application	1.000000
application may	0.071429
may dismiss	0.019231
dismiss	0.000028
dismiss the	1.000000
the hypothesis	0.000692
hypothesis	0.000028
hypothesis ``	1.000000
The apple	0.010417
apple	0.000084
apple is	0.666667
is red	0.002033
red	0.000028
red .	1.000000
<s> e.g.	0.001537
e.g. Constraints	0.017857
Constraints may	0.333333
be semantic	0.004219
semantic ;	0.047619
; rejecting	0.042553
rejecting ``	0.666667
is angry	0.002033
angry .	0.500000
e.g. Syntactic	0.017857
Syntactic	0.000028
Syntactic ;	1.000000
`` Red	0.005291
Red	0.000028
Red is	1.000000
is apple	0.002033
apple the	0.333333
the .	0.000692
<s> Constraints	0.000769
Constraints are	0.333333
often represented	0.022727
represented by	0.333333
<s> Read	0.000769
vs. Spontaneous	0.083333
Spontaneous	0.000028
Spontaneous Speech	1.000000
Speech When	0.032258
When a	0.142857
person reads	0.052632
reads it	0.500000
it 's	0.008547
's usually	0.019608
context that	0.030303
been previously	0.014706
previously prepared	0.500000
prepared	0.000028
prepared ,	1.000000
but when	0.014706
person uses	0.052632
uses spontaneous	0.071429
<s> because	0.000769
the disfluences	0.000692
disfluences	0.000028
disfluences -LRB-	1.000000
-LRB- like	0.002710
`` uh	0.005291
uh	0.000028
uh ''	1.000000
`` um	0.005291
um	0.000028
um ''	1.000000
, false	0.000561
false starts	0.500000
starts	0.000056
starts ,	0.500000
, incomplete	0.000561
incomplete	0.000028
incomplete sentences	1.000000
, stutering	0.000561
stutering	0.000028
stutering ,	1.000000
, coughing	0.000561
coughing	0.000028
coughing ,	1.000000
and laughter	0.001445
laughter	0.000028
laughter -RRB-	1.000000
and limited	0.001445
limited vocabulary	0.100000
<s> Adverse	0.000769
conditions Environmental	0.200000
Environmental	0.000028
Environmental noise	1.000000
noise -LRB-	0.125000
e.g. Noise	0.017857
Noise	0.000028
Noise in	1.000000
a car	0.001227
car	0.000028
car or	1.000000
a factory	0.001227
factory	0.000028
factory -RRB-	1.000000
-RRB- Acoustical	0.002710
Acoustical	0.000056
Acoustical distortions	0.500000
distortions	0.000028
distortions -LRB-	1.000000
e.g. echoes	0.017857
, room	0.000561
room	0.000028
room acoustics	1.000000
acoustics	0.000028
acoustics -RRB-	1.000000
-RRB- Speech	0.002710
a multileveled	0.001227
multileveled	0.000028
multileveled pattern	1.000000
<s> Acoustical	0.000769
Acoustical signals	0.500000
signals	0.000056
signals are	1.000000
are structured	0.004149
structured into	0.166667
a hierarchy	0.001227
hierarchy	0.000056
hierarchy of	1.000000
of units	0.000891
units ;	0.142857
; e.g.	0.042553
e.g. Phonemes	0.017857
Phonemes	0.000028
Phonemes ,	1.000000
, Words	0.000561
Words ,	0.250000
, Phrases	0.000561
Phrases	0.000028
Phrases ,	1.000000
and Sentences	0.001445
Sentences	0.000028
Sentences ;	1.000000
; Each	0.021277
Each level	0.166667
level provides	0.050000
provides additional	0.500000
additional constraints	0.166667
constraints ;	0.250000
e.g. Known	0.017857
Known	0.000028
Known word	1.000000
word pronunciations	0.016667
pronunciations	0.000028
pronunciations or	1.000000
or legal	0.004505
legal word	0.333333
word sequences	0.016667
sequences ,	0.111111
can compensate	0.005525
compensate	0.000028
compensate for	1.000000
for errors	0.003610
errors or	0.200000
or uncertainties	0.004505
uncertainties	0.000028
uncertainties at	1.000000
at lower	0.014706
lower level	0.400000
level ;	0.100000
; This	0.021277
This hierarchy	0.015873
of constraints	0.000891
constraints are	0.250000
are exploited	0.004149
exploited	0.000028
exploited ;	1.000000
; By	0.021277
By combining	0.333333
combining decisions	0.250000
decisions probabilistically	0.100000
probabilistically	0.000028
probabilistically at	1.000000
all lower	0.023256
lower levels	0.400000
levels ,	0.045455
making more	0.142857
more deterministic	0.021053
deterministic decisions	0.250000
decisions only	0.100000
only at	0.026316
the highest	0.000692
highest level	0.333333
; Speech	0.021277
Speech recogniton	0.032258
recogniton by	0.500000
machine is	0.012658
process broken	0.027778
broken into	0.600000
into several	0.012821
several phases	0.045455
phases	0.000028
phases .	1.000000
<s> Computationally	0.000769
Computationally	0.000028
Computationally ,	1.000000
which a	0.014493
sound pattern	0.050000
pattern has	0.166667
recognized or	0.166667
or classified	0.004505
classified	0.000028
classified into	1.000000
a category	0.001227
category that	0.500000
that represents	0.003546
a meaning	0.002454
meaning to	0.086957
<s> Every	0.000769
Every	0.000028
Every acoustic	1.000000
acoustic signal	0.166667
signal can	0.333333
be broken	0.004219
broken in	0.200000
in smaller	0.001873
smaller more	0.142857
more basic	0.021053
basic sub-signals	0.076923
sub-signals	0.000028
sub-signals .	1.000000
As the	0.055556
complex sound	0.125000
sound signal	0.050000
signal is	0.166667
is broken	0.002033
the smaller	0.000692
smaller sub-sounds	0.142857
sub-sounds	0.000028
sub-sounds ,	1.000000
different levels	0.020408
levels are	0.045455
where at	0.028571
top level	0.200000
level we	0.050000
have complex	0.009615
complex sounds	0.041667
sounds ,	0.133333
simpler sounds	0.666667
sounds on	0.066667
on lower	0.004717
and going	0.001445
going to	0.250000
to lower	0.001328
levels even	0.045455
even more	0.037037
more ,	0.010526
create more	0.058824
basic and	0.076923
and shorter	0.001445
shorter and	0.500000
and simpler	0.001445
sounds .	0.066667
The lowest	0.005208
lowest	0.000028
lowest level	1.000000
sounds are	0.133333
most fundamental	0.017241
fundamental ,	0.500000
would check	0.018868
check for	0.500000
for simple	0.003610
simple and	0.076923
more probabilistic	0.010526
probabilistic rules	0.142857
what sound	0.031250
sound should	0.100000
should represent	0.105263
represent .	0.222222
Once these	0.200000
these sounds	0.023810
are put	0.004149
put together	0.250000
together into	0.125000
sound on	0.050000
on upper	0.004717
upper	0.000056
upper level	1.000000
new set	0.041667
deterministic rules	0.250000
rules should	0.023256
should predict	0.052632
predict what	0.166667
what new	0.031250
new complex	0.041667
most upper	0.017241
deterministic rule	0.250000
rule should	0.333333
should figure	0.052632
figure	0.000056
figure out	0.500000
of complex	0.000891
complex expressions	0.041667
to expand	0.001328
expand	0.000028
expand our	1.000000
our knowledge	0.200000
about speech	0.050000
recognition we	0.008264
a consideration	0.001227
consideration neural	0.333333
are four	0.004149
four steps	0.142857
steps of	0.500000
of neural	0.000891
network approaches	0.333333
: Digitize	0.009804
Digitize	0.000028
Digitize the	1.000000
speech that	0.006579
recognize For	0.111111
For telephone	0.016393
telephone	0.000056
telephone speech	0.500000
speech the	0.006579
the sampling	0.000692
sampling	0.000028
sampling rate	1.000000
is 8000	0.002033
8000	0.000028
8000 samples	1.000000
samples per	0.500000
per second	0.500000
second ;	0.100000
; Compute	0.021277
Compute	0.000028
Compute features	1.000000
of spectral-domain	0.000891
spectral-domain	0.000028
spectral-domain of	1.000000
with Fourier	0.005464
Fourier	0.000084
Fourier transform	0.666667
transform	0.000140
transform -RRB-	0.200000
; Computed	0.021277
Computed	0.000028
Computed every	1.000000
every 10msec	0.333333
10msec	0.000056
10msec ,	0.500000
with one	0.005464
one 10msec	0.015385
10msec section	0.500000
section called	0.166667
called a	0.055556
a frame	0.001227
frame	0.000056
frame ;	0.500000
; Analysis	0.021277
Analysis of	0.200000
of four	0.000891
four step	0.142857
step neural	0.066667
approaches can	0.035714
be explained	0.004219
explained	0.000028
explained by	1.000000
by further	0.005714
further information	0.125000
<s> Sound	0.001537
Sound	0.000084
Sound is	0.333333
is produced	0.004065
air -LRB-	0.200000
other medium	0.014286
medium -RRB-	0.333333
-RRB- vibration	0.002710
vibration	0.000028
vibration ,	1.000000
which we	0.007246
we register	0.022222
register	0.000028
register by	1.000000
by ears	0.005714
ears	0.000028
ears ,	1.000000
but machines	0.014706
machines by	0.250000
by receivers	0.005714
receivers	0.000028
receivers .	1.000000
<s> Basic	0.000769
Basic	0.000028
Basic sound	1.000000
sound creates	0.050000
creates	0.000056
creates a	0.500000
a wave	0.004908
wave	0.000251
wave which	0.111111
has 2	0.011905
2 descriptions	0.200000
descriptions	0.000028
descriptions ;	1.000000
; Amplitude	0.021277
Amplitude	0.000028
Amplitude -LRB-	1.000000
how strong	0.034483
strong is	0.250000
is it	0.002033
it -RRB-	0.008547
and frequency	0.001445
frequency -LRB-	0.500000
how often	0.034483
often it	0.022727
it vibrates	0.008547
vibrates	0.000028
vibrates per	1.000000
second -RRB-	0.100000
<s> Digitized	0.000769
Digitized	0.000028
Digitized Sound	1.000000
Sound Graph	0.333333
Graph	0.000028
Graph This	1.000000
the wave	0.000692
wave in	0.111111
the water	0.000692
water	0.000028
water .	1.000000
<s> Big	0.000769
Big	0.000028
Big wave	1.000000
wave is	0.222222
is strong	0.002033
strong and	0.250000
and smaller	0.001445
smaller ones	0.142857
ones are	0.100000
usually faster	0.031250
faster but	0.333333
but weaker	0.014706
weaker	0.000028
weaker .	1.000000
how air	0.034483
air is	0.200000
distorted ,	0.500000
but we	0.014706
we do	0.022222
n't see	0.250000
see it	0.050000
it easily	0.008547
easily ,	0.111111
order for	0.071429
for sound	0.003610
sound to	0.050000
to travel	0.001328
travel	0.000028
travel .	1.000000
These waves	0.058824
waves	0.000195
waves can	0.142857
be digitalized	0.004219
digitalized	0.000028
digitalized :	1.000000
: Sample	0.009804
Sample	0.000028
Sample a	1.000000
a strength	0.001227
strength at	0.200000
at short	0.014706
short intervals	0.125000
intervals	0.000028
intervals like	1.000000
like in	0.035714
in picture	0.001873
picture above	0.250000
above to	0.076923
get bunch	0.142857
bunch	0.000056
bunch of	1.000000
of numbers	0.001783
numbers that	0.142857
that approximate	0.003546
approximate at	0.500000
at each	0.014706
each time	0.022222
time step	0.030303
step the	0.066667
wave .	0.222222
<s> Collection	0.000769
Collection	0.000028
Collection of	1.000000
numbers represent	0.142857
represent analog	0.111111
analog wave	0.500000
This new	0.015873
new wave	0.041667
is digital	0.002033
digital .	0.142857
Sound waves	0.333333
waves are	0.142857
are complicated	0.004149
complicated because	0.333333
they superimpose	0.025000
superimpose	0.000028
superimpose one	1.000000
one on	0.015385
on top	0.004717
top of	0.200000
other .	0.028571
<s> Like	0.000769
Like the	0.500000
the waves	0.000692
waves would	0.142857
would .	0.018868
This way	0.015873
way they	0.041667
they create	0.025000
create odd	0.058824
odd	0.000056
odd looking	1.000000
looking waves	0.200000
waves .	0.142857
if there	0.071429
are two	0.008299
two waves	0.034483
waves that	0.142857
that interact	0.003546
interact	0.000028
interact with	1.000000
with each	0.005464
other we	0.014286
can add	0.005525
add	0.000028
add them	1.000000
them which	0.052632
which creates	0.007246
creates new	0.500000
new odd	0.041667
looking wave	0.200000
wave as	0.111111
is shown	0.002033
picture on	0.250000
<s> Neural	0.001537
Neural	0.000112
Neural Network	0.250000
Network	0.000028
Network classifies	1.000000
classifies	0.000028
classifies features	1.000000
features into	0.038462
into phonetic-based	0.012821
phonetic-based	0.000028
phonetic-based categories	1.000000
categories ;	0.111111
; Given	0.021277
Given basic	0.071429
basic sound	0.153846
sound blocks	0.050000
blocks ,	0.250000
machine digitized	0.012658
digitized	0.000028
digitized ,	1.000000
a bunch	0.001227
numbers which	0.142857
which describe	0.007246
describe a	0.166667
wave and	0.111111
and waves	0.001445
waves describe	0.142857
describe words	0.166667
Each frame	0.166667
frame has	0.500000
a unit	0.001227
unit block	0.333333
block	0.000028
block of	1.000000
of sound	0.001783
sound ,	0.100000
are broken	0.004149
into basic	0.012821
sound waves	0.050000
waves and	0.142857
and represented	0.001445
by numbers	0.005714
numbers after	0.142857
after Fourier	0.083333
Fourier Transform	0.333333
Transform	0.000028
Transform ,	1.000000
be statistically	0.004219
statistically	0.000056
statistically evaluated	1.000000
evaluated to	0.285714
set to	0.025641
which class	0.007246
of sounds	0.000891
sounds it	0.066667
it belongs	0.008547
belongs	0.000028
belongs to	1.000000
The nodes	0.005208
nodes	0.000195
nodes in	0.142857
the figure	0.000692
figure on	0.500000
a slide	0.001227
slide	0.000028
slide represent	1.000000
a feature	0.002454
sound in	0.050000
wave from	0.111111
from first	0.009615
first layer	0.030303
layer	0.000084
layer of	1.000000
of nodes	0.003565
nodes to	0.142857
second layer	0.200000
nodes based	0.142857
some statistical	0.012048
statistical analysis	0.030303
This analysis	0.015873
analysis depends	0.015385
on programer	0.004717
programer	0.000028
programer 's	1.000000
's instructions	0.019608
instructions	0.000028
instructions .	1.000000
At this	0.333333
this point	0.010989
nodes represents	0.142857
higher level	0.142857
level features	0.050000
sound input	0.050000
input which	0.024390
is again	0.002033
again	0.000028
again statistically	1.000000
see what	0.050000
what class	0.031250
class they	0.250000
they belong	0.025000
belong	0.000028
belong to	1.000000
<s> Last	0.000769
Last	0.000028
Last level	1.000000
nodes should	0.142857
be output	0.004219
output nodes	0.076923
nodes that	0.142857
that tell	0.003546
tell us	0.333333
us with	0.500000
high probability	0.055556
probability what	0.142857
what original	0.031250
original sound	0.076923
sound really	0.050000
really	0.000028
really was	1.000000
was .	0.012987
Search to	0.500000
to match	0.002656
the neural-network	0.000692
neural-network	0.000028
neural-network output	1.000000
output scores	0.038462
scores for	0.200000
best word	0.055556
word ,	0.033333
was most	0.012987
likely uttered	0.062500
uttered	0.000084
uttered ;	0.333333
; A	0.021277
A machine	0.020000
machine speech	0.012658
recognition using	0.008264
using neural	0.016949
network is	0.166667
still just	0.066667
a fancy	0.001227
fancy	0.000028
fancy statistics	1.000000
<s> Artificial	0.000769
Artificial neural	0.500000
network has	0.166667
has specialized	0.011905
specialized	0.000056
specialized output	0.500000
nodes for	0.142857
for results	0.003610
, unlike	0.000561
unlike	0.000028
unlike brain	1.000000
brain	0.000084
brain .	0.333333
Our brain	0.666667
brain recognizes	0.333333
in fundamentally	0.001873
fundamentally	0.000028
fundamentally different	1.000000
different way	0.020408
way .	0.041667
brain is	0.333333
is entirely	0.002033
entirely committed	0.500000
committed	0.000028
committed into	1.000000
the perception	0.000692
perception	0.000056
perception of	0.500000
sound .	0.050000
When we	0.142857
we hear	0.022222
hear	0.000056
hear sound	0.500000
, our	0.000561
our life	0.200000
life experience	0.250000
experience	0.000056
experience is	0.500000
is brought	0.002033
brought	0.000028
brought together	1.000000
together to	0.125000
to action	0.001328
action of	0.200000
of listening	0.000891
listening	0.000028
listening to	1.000000
sound into	0.050000
a appropriate	0.001227
appropriate perspective	0.250000
perspective so	0.250000
is meaningful	0.002033
meaningful .	0.125000
<s> Brain	0.000769
Brain	0.000028
Brain has	1.000000
a purpose	0.001227
purpose when	0.200000
when it	0.028571
it listens	0.008547
listens	0.000028
listens for	1.000000
sound that	0.050000
is steered	0.002033
steered	0.000028
steered toward	1.000000
toward	0.000028
toward actions	1.000000
In 1982	0.009524
1982 ,	0.333333
Kurzweil Applied	0.142857
Applied Intelligence	0.500000
Intelligence and	0.333333
and Dragon	0.001445
Dragon	0.000056
Dragon Systems	1.000000
Systems released	0.166667
released	0.000056
released speech	0.500000
recognition products	0.008264
products .	0.250000
<s> By	0.000769
By 1985	0.333333
1985	0.000028
1985 ,	1.000000
Kurzweil 's	0.142857
's software	0.019608
software had	0.037037
a vocabulary	0.001227
vocabulary of	0.125000
of 1,000	0.000891
1,000 words	0.500000
words --	0.009174
if uttered	0.035714
uttered one	0.333333
one word	0.030769
word at	0.016667
, its	0.000561
its lexicon	0.028571
lexicon reached	0.111111
reached 20,000	0.500000
20,000	0.000028
20,000 words	1.000000
entering the	0.500000
the realm	0.000692
realm	0.000028
realm of	1.000000
human vocabularies	0.021739
which range	0.007246
from 10,000	0.009615
10,000	0.000028
10,000 to	1.000000
to 150,000	0.001328
150,000	0.000028
150,000 words	1.000000
But recognition	0.166667
accuracy was	0.032258
was only	0.012987
only 10	0.026316
% in	0.025641
in 1993	0.001873
1993	0.000084
1993 .	0.333333
the error	0.000692
rate crossed	0.090909
crossed	0.000028
crossed below	1.000000
below 50	0.200000
50 %	0.666667
<s> Dragon	0.000769
released ``	0.500000
`` Naturally	0.005291
Naturally	0.000028
Naturally Speaking	1.000000
Speaking	0.000028
Speaking ''	1.000000
1997 ,	0.500000
which recognized	0.007246
recognized normal	0.166667
normal	0.000056
normal human	0.500000
human speech	0.021739
<s> Progress	0.000769
Progress	0.000028
Progress mainly	1.000000
mainly came	0.166667
came from	0.500000
from improved	0.009615
improved computer	0.250000
computer performance	0.022727
performance and	0.111111
larger source	0.062500
text databases	0.006289
major database	0.083333
database available	0.100000
containing several	0.125000
several million	0.045455
million words	0.333333
In 2006	0.009524
2006 ,	0.333333
, Google	0.000561
Google published	0.250000
published a	0.285714
a trillion-word	0.001227
trillion-word	0.000028
trillion-word corpus	1.000000
while Carnegie	0.050000
Carnegie	0.000056
Carnegie Mellon	1.000000
Mellon	0.000056
Mellon University	1.000000
University researchers	0.111111
researchers found	0.100000
found no	0.071429
no significant	0.076923
significant increase	0.111111
Algorithms Both	0.500000
Both acoustic	0.333333
acoustic modeling	0.333333
modeling and	0.142857
language modeling	0.006757
modeling are	0.142857
are important	0.004149
important parts	0.062500
of modern	0.000891
modern statistically-based	0.200000
statistically-based	0.000028
statistically-based speech	1.000000
recognition algorithms	0.008264
are widely	0.004149
many systems	0.019231
<s> Language	0.000769
Language modeling	0.083333
modeling has	0.142857
other applications	0.014286
applications such	0.040000
as smart	0.003484
smart	0.000028
smart keyboard	1.000000
keyboard and	0.333333
document classification	0.055556
classification .	0.117647
models Main	0.038462
: Hidden	0.009804
model Modern	0.033333
Modern general-purpose	0.333333
general-purpose	0.000028
general-purpose speech	1.000000
on Hidden	0.004717
Models .	0.333333
are statistical	0.004149
that output	0.003546
output a	0.076923
of symbols	0.000891
symbols or	0.333333
or quantities	0.004505
quantities	0.000084
quantities .	0.333333
HMMs are	0.250000
recognition because	0.008264
because a	0.033333
a speech	0.002454
speech signal	0.006579
a piecewise	0.001227
piecewise	0.000028
piecewise stationary	1.000000
stationary signal	0.285714
signal or	0.166667
a short-time	0.001227
short-time	0.000056
short-time stationary	0.500000
signal .	0.166667
short time-scales	0.125000
time-scales	0.000028
time-scales -LRB-	1.000000
10 milliseconds	0.250000
milliseconds	0.000056
milliseconds -RRB-	0.500000
be approximated	0.004219
approximated	0.000028
approximated as	1.000000
stationary process	0.142857
Speech can	0.032258
be thought	0.004219
a Markov	0.001227
model for	0.066667
many stochastic	0.019231
stochastic purposes	0.125000
Another reason	0.076923
reason why	0.250000
why HMMs	0.142857
are popular	0.004149
popular is	0.111111
is because	0.002033
be trained	0.004219
trained automatically	0.333333
automatically and	0.095238
are simple	0.004149
and computationally	0.001445
computationally feasible	0.500000
feasible to	0.500000
In speech	0.009524
the hidden	0.000692
would output	0.018868
of n-dimensional	0.000891
n-dimensional	0.000028
n-dimensional real-valued	1.000000
real-valued vectors	0.333333
vectors -LRB-	0.333333
with n	0.005464
n being	0.500000
small integer	0.111111
integer	0.000028
integer ,	1.000000
as 10	0.003484
10 -RRB-	0.125000
, outputting	0.000561
outputting one	0.500000
these every	0.023810
every 10	0.333333
milliseconds .	0.500000
The vectors	0.005208
vectors would	0.333333
would consist	0.018868
consist	0.000028
consist of	1.000000
of cepstral	0.000891
cepstral	0.000056
cepstral coefficients	0.500000
coefficients	0.000112
coefficients ,	0.250000
by taking	0.005714
taking a	0.200000
a Fourier	0.001227
transform of	0.200000
short time	0.125000
time window	0.030303
and decorrelating	0.001445
decorrelating	0.000028
decorrelating the	1.000000
the spectrum	0.000692
spectrum	0.000028
spectrum using	1.000000
a cosine	0.001227
cosine transform	0.333333
transform ,	0.400000
then taking	0.028571
first -LRB-	0.030303
most significant	0.017241
significant -RRB-	0.111111
-RRB- coefficients	0.002710
coefficients .	0.250000
The hidden	0.005208
model will	0.033333
will tend	0.028571
in each	0.001873
each state	0.022222
state a	0.071429
statistical distribution	0.030303
a mixture	0.001227
mixture	0.000028
mixture of	1.000000
of diagonal	0.000891
diagonal	0.000028
diagonal covariance	1.000000
covariance	0.000056
covariance Gaussians	0.500000
Gaussians	0.000028
Gaussians ,	1.000000
will give	0.028571
give a	0.250000
a likelihood	0.001227
likelihood	0.000084
likelihood for	0.333333
each observed	0.022222
observed	0.000028
observed vector	1.000000
vector .	0.333333
Each word	0.166667
or -LRB-	0.004505
for more	0.007220
general speech	0.045455
systems -RRB-	0.008929
each phoneme	0.022222
phoneme	0.000056
phoneme ,	0.500000
different output	0.020408
output distribution	0.038462
distribution ;	0.250000
; a	0.021277
a hidden	0.001227
or phonemes	0.009009
phonemes	0.000167
phonemes is	0.166667
made by	0.062500
by concatenating	0.005714
concatenating	0.000028
concatenating the	1.000000
individual trained	0.083333
trained hidden	0.333333
the separate	0.000692
and phonemes	0.001445
phonemes .	0.166667
<s> Described	0.000769
Described	0.000028
Described above	1.000000
above are	0.076923
the core	0.000692
core elements	0.500000
elements of	0.250000
, HMM-based	0.000561
HMM-based approach	0.666667
<s> Modern	0.000769
Modern speech	0.333333
use various	0.013889
various combinations	0.055556
combinations	0.000028
combinations of	1.000000
of standard	0.000891
standard techniques	0.071429
techniques in	0.043478
improve results	0.076923
results over	0.047619
basic approach	0.076923
approach described	0.028571
above .	0.076923
typical large-vocabulary	0.111111
large-vocabulary system	0.333333
would need	0.018868
need context	0.047619
context dependency	0.030303
dependency for	0.200000
the phonemes	0.001384
phonemes -LRB-	0.166667
so phonemes	0.033333
phonemes with	0.166667
with different	0.010929
different left	0.020408
right context	0.100000
context have	0.030303
have different	0.019231
different realizations	0.020408
realizations	0.000028
realizations as	1.000000
as HMM	0.003484
HMM	0.000084
HMM states	0.333333
states -RRB-	0.250000
; it	0.021277
would use	0.018868
use cepstral	0.013889
cepstral normalization	0.500000
normalization to	0.166667
to normalize	0.001328
normalize	0.000028
normalize for	1.000000
for different	0.003610
different speaker	0.020408
speaker and	0.055556
and recording	0.001445
recording	0.000028
recording conditions	1.000000
conditions ;	0.200000
further speaker	0.125000
speaker normalization	0.055556
normalization it	0.166667
it might	0.008547
might use	0.076923
use vocal	0.013889
vocal	0.000028
vocal tract	1.000000
tract	0.000028
tract length	1.000000
length normalization	0.125000
normalization -LRB-	0.166667
-LRB- VTLN	0.002710
VTLN	0.000028
VTLN -RRB-	1.000000
-RRB- for	0.005420
for male-female	0.003610
male-female	0.000028
male-female normalization	1.000000
normalization and	0.166667
and maximum	0.001445
maximum likelihood	0.333333
likelihood linear	0.666667
linear regression	0.142857
regression	0.000028
regression -LRB-	1.000000
-LRB- MLLR	0.002710
MLLR	0.000028
MLLR -RRB-	1.000000
general speaker	0.045455
speaker adaptation	0.111111
adaptation .	0.666667
The features	0.005208
features would	0.038462
have so-called	0.009615
so-called delta	0.333333
delta	0.000056
delta and	1.000000
and delta-delta	0.002890
delta-delta	0.000056
delta-delta coefficients	1.000000
coefficients to	0.250000
to capture	0.001328
capture speech	0.500000
speech dynamics	0.006579
dynamics and	0.500000
addition might	0.166667
use heteroscedastic	0.013889
heteroscedastic	0.000056
heteroscedastic linear	1.000000
linear discriminant	0.285714
discriminant	0.000056
discriminant analysis	1.000000
-LRB- HLDA	0.002710
HLDA	0.000028
HLDA -RRB-	1.000000
; or	0.021277
or might	0.004505
might skip	0.038462
skip	0.000028
skip the	1.000000
the delta	0.000692
coefficients and	0.250000
use splicing	0.013889
splicing	0.000028
splicing and	1.000000
an LDA-based	0.007576
LDA-based	0.000028
LDA-based projection	1.000000
projection	0.000028
projection followed	1.000000
followed perhaps	0.250000
perhaps by	0.166667
by heteroscedastic	0.005714
a global	0.001227
global semitied	0.333333
semitied	0.000028
semitied covariance	1.000000
covariance transform	0.500000
transform -LRB-	0.200000
as maximum	0.003484
linear transform	0.142857
or MLLT	0.004505
MLLT	0.000028
MLLT -RRB-	1.000000
Many systems	0.083333
use so-called	0.013889
so-called discriminative	0.333333
discriminative	0.000056
discriminative training	1.000000
training techniques	0.035714
that dispense	0.003546
dispense	0.000028
dispense with	1.000000
a purely	0.001227
purely	0.000028
purely statistical	1.000000
statistical approach	0.030303
to HMM	0.001328
HMM parameter	0.333333
parameter	0.000028
parameter estimation	1.000000
estimation	0.000028
estimation and	1.000000
and instead	0.001445
instead optimize	0.142857
optimize	0.000028
optimize some	1.000000
some classification-related	0.012048
classification-related	0.000028
classification-related measure	1.000000
are maximum	0.004149
maximum mutual	0.166667
mutual	0.000028
mutual information	1.000000
information -LRB-	0.021739
-LRB- MMI	0.002710
MMI	0.000028
MMI -RRB-	1.000000
, minimum	0.000561
minimum	0.000056
minimum classification	0.500000
classification error	0.058824
error -LRB-	0.166667
-LRB- MCE	0.002710
MCE	0.000028
MCE -RRB-	1.000000
and minimum	0.001445
minimum phone	0.500000
phone error	0.250000
-LRB- MPE	0.002710
MPE	0.000028
MPE -RRB-	1.000000
<s> Decoding	0.000769
Decoding of	0.500000
what happens	0.031250
happens	0.000028
happens when	1.000000
presented with	0.166667
new utterance	0.041667
utterance and	0.333333
and must	0.001445
must compute	0.071429
compute the	0.500000
likely source	0.062500
source sentence	0.083333
would probably	0.018868
probably use	0.250000
use the	0.013889
best path	0.055556
path ,	0.500000
and here	0.001445
here there	0.500000
a choice	0.001227
choice between	0.125000
between dynamically	0.025641
dynamically creating	0.500000
combination hidden	0.200000
includes both	0.142857
the acoustic	0.000692
acoustic and	0.166667
language model	0.006757
model information	0.033333
and combining	0.001445
combining it	0.250000
it statically	0.008547
statically	0.000028
statically beforehand	1.000000
beforehand	0.000028
beforehand -LRB-	1.000000
the finite	0.000692
state transducer	0.142857
transducer	0.000056
transducer ,	0.500000
or FST	0.004505
FST	0.000028
FST ,	1.000000
, approach	0.000561
approach -RRB-	0.057143
A possible	0.020000
possible improvement	0.041667
improvement to	0.250000
to decoding	0.001328
decoding	0.000028
decoding is	1.000000
keep a	0.333333
of good	0.000891
candidates instead	0.200000
of just	0.000891
just keeping	0.111111
keeping the	0.500000
best candidate	0.055556
candidate ,	0.333333
better scoring	0.111111
scoring function	0.500000
function -LRB-	0.125000
-LRB- rescoring	0.002710
rescoring	0.000028
rescoring -RRB-	1.000000
rate these	0.090909
these good	0.023810
candidates so	0.200000
we may	0.022222
may pick	0.019231
pick	0.000028
pick the	1.000000
best one	0.055556
one according	0.015385
this refined	0.010989
refined	0.000028
refined score	1.000000
The set	0.005208
of candidates	0.000891
candidates can	0.200000
be kept	0.004219
kept	0.000028
kept either	1.000000
list -LRB-	0.090909
the N-best	0.000692
N-best	0.000028
N-best list	1.000000
list approach	0.090909
the models	0.000692
a lattice	0.001227
lattice	0.000028
lattice -RRB-	1.000000
<s> Rescoring	0.000769
Rescoring	0.000028
Rescoring is	1.000000
by trying	0.005714
to minimize	0.001328
minimize	0.000028
minimize the	1.000000
the Bayes	0.000692
Bayes risk	0.333333
risk -LRB-	0.500000
approximation thereof	0.166667
thereof	0.000028
thereof -RRB-	1.000000
: Instead	0.009804
of taking	0.000891
sentence with	0.020833
with maximal	0.005464
maximal	0.000028
maximal probability	1.000000
probability ,	0.142857
we try	0.022222
take the	0.200000
sentence that	0.041667
that minimizes	0.007092
minimizes	0.000056
minimizes the	1.000000
the expectancy	0.000692
expectancy	0.000028
expectancy of	1.000000
given loss	0.041667
loss	0.000056
loss function	1.000000
function with	0.125000
with regards	0.005464
regards	0.000028
regards to	1.000000
possible transcriptions	0.083333
transcriptions	0.000056
transcriptions -LRB-	0.500000
we take	0.022222
the average	0.000692
average distance	0.500000
distance to	0.333333
to other	0.001328
other possible	0.014286
possible sentences	0.041667
sentences weighted	0.013158
weighted by	0.333333
by their	0.005714
their estimated	0.029412
estimated	0.000028
estimated probability	1.000000
The loss	0.005208
function is	0.125000
usually the	0.031250
the Levenshtein	0.000692
Levenshtein	0.000028
Levenshtein distance	1.000000
be different	0.004219
different distances	0.020408
distances	0.000056
distances for	0.500000
for specific	0.003610
specific tasks	0.047619
tasks ;	0.031250
transcriptions is	0.500000
course ,	0.333333
, pruned	0.000561
pruned	0.000028
pruned to	1.000000
to maintain	0.001328
maintain	0.000028
maintain tractability	1.000000
tractability	0.000028
tractability .	1.000000
<s> Efficient	0.000769
Efficient	0.000028
Efficient algorithms	1.000000
been devised	0.014706
devised to	0.500000
to rescore	0.001328
rescore	0.000028
rescore lattices	1.000000
lattices	0.000028
lattices represented	1.000000
as weighted	0.003484
weighted finite	0.333333
state transducers	0.071429
transducers	0.000028
transducers with	1.000000
with edit	0.005464
edit	0.000028
edit distances	1.000000
distances represented	0.500000
represented themselves	0.166667
themselves as	0.250000
a finite	0.002454
transducer verifying	0.500000
verifying	0.000028
verifying certain	1.000000
certain assumptions	0.142857
assumptions .	0.200000
Dynamic time	0.800000
time warping	0.121212
warping	0.000112
warping -LRB-	0.250000
-LRB- DTW	0.002710
DTW	0.000084
DTW -RRB-	0.333333
-RRB- -	0.002710
based speech	0.018519
recognition Main	0.008264
: Dynamic	0.009804
warping Dynamic	0.250000
warping is	0.500000
was historically	0.012987
historically	0.000056
historically used	0.500000
recognition but	0.008264
but has	0.014706
has now	0.011905
now largely	0.076923
largely been	0.200000
been displaced	0.014706
displaced	0.000028
displaced by	1.000000
successful HMM-based	0.111111
for measuring	0.003610
measuring	0.000028
measuring similarity	1.000000
between two	0.051282
two sequences	0.034483
sequences that	0.111111
may vary	0.019231
in time	0.001873
time or	0.030303
or speed	0.004505
, similarities	0.000561
similarities in	0.500000
in walking	0.001873
walking	0.000084
walking patterns	0.333333
patterns would	0.200000
be detected	0.004219
detected ,	0.500000
if in	0.071429
one video	0.015385
video the	0.200000
person was	0.052632
was walking	0.012987
walking slowly	0.333333
slowly	0.000056
slowly and	0.500000
and if	0.001445
another he	0.076923
he or	0.142857
or she	0.004505
she	0.000028
she were	1.000000
were walking	0.024390
walking more	0.333333
more quickly	0.010526
quickly	0.000028
quickly ,	1.000000
were accelerations	0.024390
accelerations	0.000028
accelerations and	1.000000
and decelerations	0.001445
decelerations	0.000028
decelerations during	1.000000
the course	0.000692
course of	0.333333
one observation	0.015385
observation	0.000028
observation .	1.000000
<s> DTW	0.000769
DTW has	0.333333
to video	0.001328
video ,	0.200000
and graphics	0.001445
graphics	0.000028
graphics --	1.000000
-- indeed	0.040000
indeed ,	0.333333
, any	0.000561
any data	0.032258
be turned	0.004219
linear representation	0.142857
representation can	0.052632
be analyzed	0.004219
analyzed with	0.200000
with DTW	0.005464
DTW .	0.333333
A well-known	0.020000
well-known	0.000028
well-known application	1.000000
application has	0.071429
been automatic	0.014706
to cope	0.001328
cope	0.000028
cope with	1.000000
different speaking	0.020408
speaking speeds	0.125000
speeds .	0.500000
method that	0.062500
that allows	0.003546
allows a	0.125000
an optimal	0.007576
optimal	0.000028
optimal match	1.000000
match between	0.166667
two given	0.034483
given sequences	0.041667
sequences -LRB-	0.111111
, time	0.000561
time series	0.030303
series -RRB-	0.125000
with certain	0.005464
certain restrictions	0.142857
restrictions	0.000028
restrictions .	1.000000
the sequences	0.000692
sequences are	0.111111
are ``	0.004149
`` warped	0.005291
warped	0.000028
warped ''	1.000000
'' non-linearly	0.005155
non-linearly	0.000028
non-linearly to	1.000000
match each	0.166667
This sequence	0.015873
sequence alignment	0.125000
alignment	0.000056
alignment method	0.500000
method is	0.062500
often used	0.022727
of hidden	0.000891
models ...	0.038462
... .	0.500000
Neural networks	0.750000
networks Main	0.071429
: Neural	0.009804
networks Neural	0.071429
networks emerged	0.071429
emerged	0.000028
emerged as	1.000000
an attractive	0.007576
attractive acoustic	0.333333
modeling approach	0.142857
approach in	0.028571
in ASR	0.003745
1980s .	0.222222
Since then	0.200000
, neural	0.002246
networks have	0.071429
many aspects	0.019231
recognition such	0.008264
as phoneme	0.003484
phoneme classification	0.500000
classification ,	0.058824
, isolated	0.000561
isolated word	0.200000
word recognition	0.016667
and speaker	0.001445
to HMMs	0.001328
HMMs ,	0.125000
networks make	0.071429
make no	0.050000
no assumptions	0.076923
assumptions about	0.200000
about feature	0.025000
feature statistical	0.076923
statistical properties	0.030303
properties and	0.250000
and have	0.001445
have several	0.009615
several qualities	0.045455
qualities making	0.500000
making them	0.142857
them attractive	0.052632
attractive recognition	0.333333
recognition models	0.008264
When used	0.142857
speech feature	0.006579
feature segment	0.076923
segment ,	0.222222
networks allow	0.071429
allow discriminative	0.200000
training in	0.035714
efficient manner	0.333333
<s> Few	0.000769
Few	0.000028
Few assumptions	1.000000
assumptions on	0.200000
the statistics	0.000692
statistics of	0.125000
input features	0.024390
made with	0.062500
with neural	0.005464
in spite	0.001873
spite	0.000028
spite of	1.000000
of their	0.001783
their effectiveness	0.029412
effectiveness in	0.333333
in classifying	0.001873
classifying short-time	0.200000
short-time units	0.500000
units such	0.142857
as individual	0.003484
individual phones	0.083333
phones	0.000056
phones and	0.500000
and isolated	0.001445
isolated words	0.200000
networks are	0.071429
are rarely	0.004149
rarely successful	0.333333
successful for	0.111111
for continuous	0.003610
continuous recognition	0.166667
recognition tasks	0.016529
, largely	0.000561
largely because	0.200000
their lack	0.029412
lack	0.000028
lack of	1.000000
of ability	0.000891
to model	0.001328
model temporal	0.033333
temporal dependencies	0.500000
one alternative	0.015385
alternative approach	0.333333
use neural	0.013889
networks as	0.071429
a pre-processing	0.001227
pre-processing	0.000028
pre-processing e.g.	1.000000
e.g. feature	0.017857
feature transformation	0.076923
transformation	0.000028
transformation ,	1.000000
, dimensionality	0.000561
dimensionality	0.000028
dimensionality reduction	1.000000
reduction ,	0.500000
the HMM	0.000692
HMM based	0.333333
based recognition	0.018519
Further information	0.333333
information Popular	0.021739
Popular	0.000028
Popular speech	1.000000
recognition conferences	0.008264
conferences	0.000028
conferences held	1.000000
held	0.000028
held each	1.000000
each year	0.022222
year or	0.166667
two include	0.034483
include SpeechTEK	0.037037
SpeechTEK	0.000056
SpeechTEK and	0.500000
and SpeechTEK	0.001445
SpeechTEK Europe	0.500000
, ICASSP	0.000561
ICASSP	0.000028
ICASSP ,	1.000000
, Eurospeech\/ICSLP	0.000561
Eurospeech\/ICSLP	0.000028
Eurospeech\/ICSLP -LRB-	1.000000
now named	0.153846
named Interspeech	0.142857
Interspeech	0.000028
Interspeech -RRB-	1.000000
the IEEE	0.001384
IEEE	0.000084
IEEE ASRU	0.333333
ASRU	0.000028
ASRU .	1.000000
<s> Conferences	0.000769
Conferences in	0.500000
of Natural	0.000891
as ACL	0.003484
ACL ,	0.500000
, NAACL	0.000561
NAACL	0.000028
NAACL ,	1.000000
, EMNLP	0.000561
EMNLP	0.000028
EMNLP ,	1.000000
and HLT	0.001445
HLT	0.000028
HLT ,	1.000000
are beginning	0.004149
beginning to	0.500000
include papers	0.037037
on speech	0.004717
speech processing	0.006579
<s> Important	0.000769
Important	0.000028
Important journals	1.000000
journals include	0.500000
IEEE Transactions	0.666667
Transactions	0.000056
Transactions on	1.000000
on Speech	0.004717
and Audio	0.001445
Audio	0.000056
Audio Processing	0.500000
Processing	0.000112
Processing -LRB-	0.750000
named IEEE	0.142857
on Audio	0.004717
Audio ,	0.500000
Language Processing	0.250000
Processing -RRB-	0.250000
, Computer	0.000561
Computer Speech	0.333333
Language ,	0.083333
and Speech	0.001445
Speech Communication	0.032258
Communication	0.000028
Communication .	1.000000
<s> Books	0.000769
Books	0.000028
Books like	1.000000
`` Fundamentals	0.010582
Fundamentals	0.000056
Fundamentals of	1.000000
of Speech	0.000891
Recognition ''	0.375000
'' by	0.025773
by Lawrence	0.005714
Lawrence	0.000028
Lawrence Rabiner	1.000000
Rabiner	0.000028
Rabiner can	1.000000
to acquire	0.001328
acquire	0.000028
acquire basic	1.000000
basic knowledge	0.076923
knowledge but	0.037037
fully up	0.166667
to date	0.002656
-LRB- 1993	0.002710
1993 -RRB-	0.333333
A very	0.020000
very recent	0.024390
recent book	0.125000
book -LRB-	0.250000
-LRB- Dec.	0.002710
Dec.	0.000028
Dec. 2011	1.000000
2011 -RRB-	0.500000
of Speaker	0.000891
Speaker Recognition	0.166667
by Homayoon	0.005714
Homayoon	0.000028
Homayoon Beigi	1.000000
Beigi	0.000028
Beigi covers	1.000000
more recent	0.010526
recent developments	0.125000
some detail	0.012048
detail .	0.500000
the title	0.000692
title	0.000028
title concentrates	1.000000
concentrates	0.000028
concentrates on	1.000000
on speaker	0.004717
speaker recognition	0.055556
but a	0.014706
large portion	0.043478
book applies	0.125000
applies directly	0.142857
lot of	0.333333
of valuable	0.000891
valuable detailed	0.500000
detailed background	0.500000
background material	0.333333
material .	0.500000
Another good	0.076923
good source	0.076923
source can	0.041667
be ``	0.004219
`` Statistical	0.005291
Statistical Methods	0.111111
for Speech	0.003610
by Frederick	0.005714
Frederick	0.000028
Frederick Jelinek	1.000000
Jelinek and	0.500000
`` Spoken	0.005291
Spoken	0.000028
Spoken Language	1.000000
-LRB- 2001	0.002710
2001 -RRB-	0.500000
-RRB- ''	0.005420
by Xuedong	0.005714
Xuedong	0.000028
Xuedong Huang	1.000000
Huang	0.000028
Huang etc.	1.000000
More up	0.111111
date is	0.333333
`` Computer	0.005291
Speech ''	0.032258
by Manfred	0.005714
Manfred	0.000028
Manfred R.	1.000000
R. Schroeder	0.166667
Schroeder	0.000028
Schroeder ,	1.000000
, second	0.000561
second edition	0.100000
edition	0.000028
edition published	1.000000
published in	0.142857
in 2004	0.001873
2004 .	0.333333
The recently	0.005208
recently updated	0.333333
updated	0.000028
updated textbook	1.000000
textbook of	0.500000
`` Speech	0.005291
-LRB- 2008	0.002710
2008	0.000028
2008 -RRB-	1.000000
by Jurafsky	0.005714
Jurafsky	0.000028
Jurafsky and	1.000000
and Martin	0.001445
Martin presents	0.500000
presents	0.000028
presents the	1.000000
the basics	0.000692
basics	0.000028
basics and	1.000000
art for	0.500000
for ASR	0.003610
ASR .	0.166667
A good	0.020000
good insight	0.076923
insight	0.000028
insight into	1.000000
the techniques	0.000692
techniques used	0.043478
best modern	0.055556
modern systems	0.200000
be gained	0.004219
gained by	0.500000
by paying	0.005714
paying	0.000028
paying attention	1.000000
attention to	0.500000
to government	0.001328
government sponsored	0.333333
sponsored evaluations	0.500000
evaluations such	0.166667
those organised	0.045455
organised	0.000028
organised by	1.000000
by DARPA	0.005714
DARPA -LRB-	0.250000
the largest	0.000692
largest	0.000028
largest speech	1.000000
speech recognition-related	0.006579
recognition-related	0.000028
recognition-related project	1.000000
project ongoing	0.076923
ongoing as	0.500000
2007 is	0.200000
the GALE	0.001384
GALE	0.000056
GALE project	1.000000
which involves	0.007246
involves both	0.100000
both speech	0.032258
translation components	0.013514
components -RRB-	0.200000
In terms	0.009524
of freely	0.000891
freely	0.000028
freely available	1.000000
available resources	0.058824
resources ,	0.333333
, Carnegie	0.000561
University 's	0.111111
's SPHINX	0.019608
SPHINX	0.000028
SPHINX toolkit	1.000000
toolkit	0.000056
toolkit is	0.500000
one place	0.015385
place to	0.250000
to start	0.002656
start to	0.142857
both learn	0.032258
learn about	0.076923
start experimenting	0.142857
experimenting	0.000028
experimenting .	1.000000
Another resource	0.076923
resource -LRB-	0.200000
-LRB- free	0.002710
free as	0.250000
in free	0.003745
free beer	0.250000
beer	0.000028
beer ,	1.000000
not as	0.008929
free speech	0.250000
the HTK	0.000692
HTK	0.000056
HTK book	0.500000
the accompanying	0.000692
accompanying	0.000028
accompanying HTK	1.000000
HTK toolkit	0.500000
toolkit -RRB-	0.500000
The AT&T	0.005208
AT&T	0.000028
AT&T libraries	1.000000
libraries	0.000056
libraries GRM	0.500000
GRM	0.000028
GRM library	1.000000
library	0.000056
library ,	0.500000
and DCD	0.001445
DCD	0.000028
DCD library	1.000000
library are	0.500000
also general	0.014493
general software	0.045455
software libraries	0.037037
libraries for	0.500000
for large-vocabulary	0.003610
more software	0.010526
software resources	0.037037
see List	0.050000
software .	0.037037
A useful	0.020000
useful review	0.071429
review of	0.333333
of robustness	0.000891
robustness	0.000112
robustness in	0.250000
ASR is	0.166667
is provided	0.002033
by Junqua	0.005714
Junqua	0.000028
Junqua and	1.000000
and Haton	0.001445
Haton	0.000028
Haton -LRB-	1.000000
-LRB- 1995	0.002710
1995	0.000028
1995 -RRB-	1.000000
<s> People	0.000769
People	0.000056
People with	1.000000
with disabilities	0.010929
disabilities	0.000112
disabilities People	0.250000
disabilities can	0.250000
can benefit	0.011050
from speech	0.009615
recognition programs	0.008264
For individuals	0.016393
individuals	0.000028
individuals that	1.000000
are Deaf	0.004149
Deaf	0.000028
Deaf or	1.000000
or Hard	0.004505
Hard of	0.500000
of Hearing	0.000891
Hearing	0.000028
Hearing ,	1.000000
software is	0.074074
automatically generate	0.047619
a closed-captioning	0.001227
closed-captioning	0.000028
closed-captioning of	1.000000
of conversations	0.000891
conversations such	0.333333
as discussions	0.003484
discussions in	0.333333
in conference	0.001873
conference rooms	0.500000
rooms	0.000028
rooms ,	1.000000
, classroom	0.000561
classroom	0.000028
classroom lectures	1.000000
lectures	0.000028
lectures ,	1.000000
, and\/or	0.000561
and\/or religious	0.333333
religious	0.000028
religious services	1.000000
people who	0.125000
who have	0.200000
have difficulty	0.009615
difficulty using	0.142857
using their	0.016949
their hands	0.029412
hands	0.000028
hands ,	1.000000
from mild	0.009615
mild	0.000028
mild repetitive	1.000000
repetitive stress	0.500000
stress injuries	0.500000
injuries	0.000028
injuries to	1.000000
to involved	0.001328
involved disabilities	0.166667
disabilities that	0.250000
that preclude	0.003546
preclude	0.000028
preclude using	1.000000
using conventional	0.016949
conventional	0.000028
conventional computer	1.000000
computer input	0.022727
, people	0.000561
who used	0.100000
used the	0.008850
the keyboard	0.000692
keyboard a	0.333333
lot and	0.333333
developed RSI	0.038462
RSI	0.000028
RSI became	1.000000
became an	0.200000
an urgent	0.007576
urgent	0.000028
urgent early	1.000000
early market	0.100000
market for	0.333333
in deaf	0.001873
deaf	0.000028
deaf telephony	1.000000
as voicemail	0.003484
voicemail	0.000028
voicemail to	1.000000
, relay	0.000561
relay	0.000028
relay services	1.000000
services ,	0.333333
and captioned	0.001445
captioned	0.000028
captioned telephone	1.000000
telephone .	0.500000
<s> Individuals	0.000769
Individuals	0.000028
Individuals with	1.000000
with learning	0.005464
learning disabilities	0.023256
disabilities who	0.250000
have problems	0.009615
problems with	0.058824
with thought-to-paper	0.005464
thought-to-paper	0.000028
thought-to-paper communication	1.000000
-LRB- essentially	0.002710
essentially they	0.125000
they think	0.025000
think of	0.333333
an idea	0.007576
idea but	0.142857
is processed	0.002033
processed incorrectly	0.166667
incorrectly	0.000028
incorrectly causing	1.000000
causing	0.000028
causing it	1.000000
up differently	0.045455
differently	0.000028
differently on	1.000000
on paper	0.004717
paper -RRB-	0.090909
software -LRB-	0.037037
-LRB- icon	0.002710
icon	0.000028
icon -RRB-	1.000000
-RRB- This	0.002710
requires expansion	0.062500
expansion .	0.333333
Current research	0.200000
and funding	0.001445
funding Measuring	0.125000
Measuring	0.000028
Measuring progress	1.000000
progress in	0.285714
recognition performance	0.033058
performance is	0.111111
difficult and	0.035714
and controversial	0.001445
controversial	0.000028
controversial .	1.000000
Some speech	0.047619
Word error	0.142857
rates on	0.125000
some tasks	0.012048
are less	0.004149
than 1	0.022222
On others	0.166667
others they	0.083333
as high	0.003484
high as	0.055556
as 50	0.003484
<s> Sometimes	0.000769
Sometimes	0.000028
Sometimes it	1.000000
it even	0.008547
even appears	0.037037
appears that	0.200000
that performance	0.010638
is going	0.002033
going backward	0.250000
backward	0.000028
backward ,	1.000000
as researchers	0.003484
researchers undertake	0.100000
undertake	0.000028
undertake harder	1.000000
harder tasks	0.142857
tasks that	0.031250
have higher	0.009615
higher error	0.142857
rates .	0.125000
Because progress	0.500000
progress is	0.142857
is slow	0.002033
slow	0.000056
slow and	0.500000
measure ,	0.090909
is some	0.002033
some perception	0.012048
perception that	0.500000
performance has	0.055556
has plateaued	0.011905
plateaued	0.000056
plateaued and	1.000000
that funding	0.003546
funding has	0.125000
has dried	0.011905
dried	0.000028
dried up	1.000000
up or	0.045455
or shifted	0.004505
shifted	0.000028
shifted priorities	1.000000
priorities	0.000028
priorities .	1.000000
Such perceptions	0.125000
perceptions	0.000028
perceptions are	1.000000
not new	0.008929
new .	0.041667
1969 ,	0.500000
John Pierce	0.125000
Pierce	0.000028
Pierce wrote	1.000000
open letter	0.250000
letter that	0.166667
did cause	0.200000
cause much	0.500000
much funding	0.045455
funding to	0.125000
to dry	0.001328
dry	0.000028
dry up	1.000000
In 1993	0.009524
1993 there	0.333333
strong feeling	0.250000
feeling	0.000028
feeling that	1.000000
performance had	0.055556
had plateaued	0.071429
were workshops	0.024390
workshops dedicated	0.500000
1990s ,	0.333333
funding continued	0.125000
continued more	0.111111
less uninterrupted	0.083333
uninterrupted	0.000028
uninterrupted and	1.000000
and performance	0.001445
performance continued	0.055556
continued ,	0.111111
, slowly	0.000561
slowly but	0.500000
but steadily	0.014706
steadily	0.000028
steadily ,	1.000000
improve .	0.076923
For the	0.016393
past thirty	0.333333
thirty	0.000028
thirty years	1.000000
recognition research	0.008264
been characterized	0.014706
steady accumulation	0.500000
accumulation	0.000028
accumulation of	1.000000
of small	0.000891
small incremental	0.111111
incremental	0.000028
incremental improvements	1.000000
improvements .	0.500000
a trend	0.001227
to change	0.001328
change	0.000028
change focus	1.000000
focus to	0.142857
to more	0.002656
difficult tasks	0.071429
tasks due	0.031250
to progress	0.001328
the availability	0.000692
availability	0.000028
availability of	1.000000
of faster	0.000891
faster computers	0.333333
this shifting	0.010989
shifting	0.000028
shifting to	1.000000
tasks has	0.031250
has characterized	0.011905
characterized DARPA	0.250000
DARPA funding	0.250000
funding of	0.125000
recognition since	0.008264
since the	0.100000
decade ,	0.333333
has continued	0.011905
continued with	0.111111
the EARS	0.000692
EARS	0.000028
EARS project	1.000000
which undertook	0.007246
undertook	0.000028
undertook recognition	1.000000
of Mandarin	0.000891
Mandarin	0.000056
Mandarin and	1.000000
and Arabic	0.002890
Arabic in	0.250000
to English	0.001328
which focused	0.007246
focused solely	0.090909
solely	0.000028
solely on	1.000000
on Mandarin	0.004717
Arabic and	0.250000
and required	0.001445
required translation	0.142857
translation simultaneously	0.013514
simultaneously with	0.500000
Commercial research	0.500000
other academic	0.014286
academic	0.000028
academic research	1.000000
research also	0.023810
also continue	0.014493
continue	0.000028
continue to	1.000000
to focus	0.001328
on increasingly	0.004717
increasingly difficult	0.333333
One key	0.076923
key area	0.333333
improve robustness	0.076923
robustness of	0.250000
not just	0.008929
just robustness	0.111111
robustness against	0.500000
against noise	0.200000
noise but	0.125000
but robustness	0.014706
against any	0.200000
any condition	0.032258
condition	0.000028
condition that	1.000000
that causes	0.003546
causes	0.000028
causes a	1.000000
major degradation	0.083333
degradation	0.000028
degradation in	1.000000
in performance	0.001873
Another key	0.076923
research is	0.047619
is focused	0.002033
an opportunity	0.007576
opportunity rather	0.500000
This research	0.015873
applications there	0.040000
large quantity	0.043478
quantity of	0.333333
speech data	0.006579
, up	0.000561
to millions	0.001328
of hours	0.000891
hours .	0.500000
is too	0.002033
expensive to	0.142857
have humans	0.009615
humans transcribe	0.083333
transcribe	0.000028
transcribe such	1.000000
such large	0.008130
large quantities	0.086957
quantities of	0.666667
research focus	0.023810
focus is	0.142857
on developing	0.004717
developing new	0.250000
new methods	0.041667
learning that	0.023256
can effectively	0.005525
effectively utilize	0.333333
utilize large	0.500000
of unlabeled	0.000891
unlabeled	0.000028
unlabeled data	1.000000
Another area	0.076923
is better	0.002033
better understanding	0.111111
human capabilities	0.021739
capabilities and	0.200000
this understanding	0.010989
improve machine	0.076923
machine recognition	0.012658
of identifying	0.000891
boundaries between	0.181818
between words	0.051282
, syllables	0.000561
syllables	0.000056
syllables ,	0.500000
phonemes in	0.166667
spoken natural	0.071429
term applies	0.111111
applies both	0.285714
the mental	0.000692
mental processes	0.666667
processes used	0.400000
by humans	0.011429
to artificial	0.002656
artificial processes	0.181818
processes of	0.200000
important subproblem	0.062500
subproblem	0.000028
subproblem of	1.000000
be adequately	0.004219
adequately	0.000028
adequately solved	1.000000
solved in	0.200000
in isolation	0.001873
isolation .	0.500000
processing problems	0.018519
one must	0.015385
must take	0.071429
account context	0.333333
, grammar	0.000561
even so	0.037037
often a	0.022727
a probabilistic	0.001227
probabilistic division	0.142857
division rather	0.500000
a categorical	0.001227
categorical	0.000028
categorical .	1.000000
A comprehensive	0.020000
comprehensive survey	0.200000
survey	0.000028
survey of	1.000000
segmentation problems	0.060606
and techniques	0.001445
techniques can	0.043478
seen in	0.100000
in .	0.001873
Some writing	0.047619
writing systems	0.222222
systems indicate	0.008929
indicate speech	0.333333
segmentation between	0.030303
words by	0.009174
word divider	0.016667
divider	0.000028
divider ,	1.000000
the space	0.002076
space .	0.200000
is compounded	0.002033
compounded	0.000028
compounded by	1.000000
of co-articulation	0.000891
co-articulation	0.000028
co-articulation of	1.000000
speech sounds	0.006579
where one	0.028571
one may	0.015385
be modified	0.004219
modified	0.000028
modified in	1.000000
ways by	0.125000
the adjacent	0.000692
adjacent sounds	0.166667
sounds :	0.066667
: it	0.009804
may blend	0.019231
blend smoothly	0.666667
smoothly	0.000056
smoothly with	0.500000
, fuse	0.000561
fuse	0.000056
fuse with	1.000000
, split	0.000561
split ,	0.250000
even disappear	0.037037
disappear	0.000028
disappear .	1.000000
phenomenon may	0.200000
may happen	0.019231
happen	0.000028
happen between	1.000000
between adjacent	0.051282
adjacent words	0.333333
words just	0.009174
easily as	0.111111
as within	0.003484
single word	0.071429
notion that	0.250000
that speech	0.003546
speech is	0.006579
produced like	0.111111
like writing	0.035714
of distinct	0.000891
distinct vowels	0.142857
vowels	0.000084
vowels and	0.333333
and consonants	0.001445
consonants	0.000084
consonants ,	0.333333
a relic	0.001227
relic	0.000028
relic of	1.000000
of our	0.000891
our alphabetic	0.200000
alphabetic	0.000028
alphabetic heritage	1.000000
heritage	0.000028
heritage -LRB-	1.000000
way we	0.083333
we produce	0.044444
produce vowels	0.045455
vowels depends	0.333333
the surrounding	0.001384
surrounding consonants	0.200000
consonants and	0.333333
produce consonants	0.045455
consonants depends	0.333333
surrounding vowels	0.200000
vowels .	0.333333
when we	0.057143
we say	0.044444
say `	0.285714
` kit	0.125000
kit	0.000056
kit '	1.000000
the -LRB-	0.000692
-LRB- k	0.002710
k	0.000028
k -RRB-	1.000000
is farther	0.002033
farther	0.000028
farther forward	1.000000
forward	0.000028
forward than	1.000000
than when	0.022222
` caught	0.062500
caught	0.000028
caught '	1.000000
' .	0.105263
But also	0.166667
the vowel	0.001384
vowel	0.000056
vowel in	1.000000
in `	0.003745
` kick	0.062500
kick	0.000028
kick '	1.000000
' is	0.052632
is phonetically	0.002033
phonetically	0.000028
phonetically different	1.000000
though we	0.100000
we normally	0.022222
normally	0.000056
normally do	0.500000
not hear	0.008929
hear this	0.500000
this .	0.010989
are language-specific	0.004149
language-specific	0.000028
language-specific changes	1.000000
changes	0.000028
changes which	1.000000
which occur	0.007246
occur on	0.200000
on casual	0.004717
casual	0.000028
casual speech	1.000000
speech which	0.006579
it quite	0.008547
from spelling	0.009615
spelling	0.000028
spelling .	1.000000
phrase `	0.100000
` hit	0.062500
hit	0.000028
hit you	1.000000
you '	0.076923
' could	0.052632
could often	0.062500
often be	0.022727
more appropriately	0.010526
appropriately spelled	0.500000
spelled	0.000028
spelled `	1.000000
` hitcha	0.062500
hitcha	0.000028
hitcha '	1.000000
even with	0.037037
best algorithms	0.055556
of phonetic	0.000891
phonetic	0.000056
phonetic segmentation	0.500000
segmentation will	0.030303
will usually	0.028571
usually be	0.031250
very distant	0.024390
distant	0.000028
distant from	1.000000
standard written	0.071429
written language	0.115385
the lexical	0.000692
syntactic parsing	0.076923
spoken text	0.071429
text normally	0.006289
normally requires	0.500000
requires specialized	0.062500
specialized algorithms	0.500000
, distinct	0.000561
for parsing	0.003610
parsing written	0.035714
Statistical models	0.111111
models can	0.038462
to segment	0.003984
segment and	0.111111
and align	0.001445
align	0.000028
align recorded	1.000000
recorded speech	0.500000
to words	0.001328
or phones	0.004505
phones .	0.500000
Applications include	0.500000
include automatic	0.037037
automatic lip-synch	0.043478
lip-synch	0.000028
lip-synch timing	1.000000
timing	0.000028
timing for	1.000000
for cartoon	0.003610
cartoon	0.000028
cartoon animation	1.000000
animation	0.000028
animation ,	1.000000
, follow-the-bouncing-ball	0.000561
follow-the-bouncing-ball	0.000028
follow-the-bouncing-ball video	1.000000
video sub-titling	0.200000
sub-titling	0.000028
sub-titling ,	1.000000
and linguistic	0.001445
linguistic research	0.062500
Automatic segmentation	0.333333
and alignment	0.001445
alignment software	0.500000
is commercially	0.002033
Lexical segmentation	0.500000
segmentation In	0.030303
In all	0.009524
all natural	0.023256
complex spoken	0.041667
spoken sentence	0.071429
sentence -LRB-	0.020833
which often	0.007246
been heard	0.014706
heard	0.000028
heard or	1.000000
or uttered	0.004505
uttered before	0.333333
be understood	0.004219
understood	0.000028
understood only	1.000000
by decomposing	0.005714
decomposing	0.000028
decomposing it	1.000000
into smaller	0.012821
smaller lexical	0.142857
lexical segments	0.076923
segments -LRB-	0.200000
-LRB- roughly	0.005420
roughly ,	0.666667
language -RRB-	0.013514
, associating	0.000561
associating	0.000028
associating a	1.000000
each segment	0.044444
then combining	0.028571
combining those	0.250000
those meanings	0.045455
meanings according	0.250000
The recognition	0.005208
each lexical	0.022222
lexical segment	0.076923
segment in	0.111111
turn requires	0.166667
requires its	0.062500
its decomposition	0.028571
decomposition	0.000028
decomposition into	1.000000
of discrete	0.000891
discrete phonetic	0.333333
phonetic segments	0.500000
segments and	0.200000
and mapping	0.001445
mapping each	0.500000
segment to	0.111111
one element	0.015385
element	0.000028
element of	1.000000
finite set	0.200000
of elementary	0.000891
elementary	0.000028
elementary sounds	1.000000
phonemes of	0.166667
meaning then	0.043478
then can	0.028571
found by	0.071429
by standard	0.005714
standard table	0.071429
table lookup	0.142857
lookup	0.000028
lookup algorithms	1.000000
For most	0.016393
lexical units	0.076923
are surprisingly	0.004149
surprisingly difficult	0.333333
identify .	0.083333
One might	0.076923
might expect	0.038462
expect that	0.333333
the inter-word	0.000692
inter-word	0.000056
inter-word spaces	1.000000
spaces used	0.200000
by many	0.005714
many written	0.019231
English or	0.027027
or Spanish	0.004505
Spanish ,	0.500000
would correspond	0.018868
to pauses	0.001328
pauses in	0.250000
their spoken	0.029412
spoken version	0.071429
version ;	0.333333
is true	0.002033
true only	0.500000
very slow	0.024390
slow speech	0.500000
speaker deliberately	0.055556
deliberately	0.000028
deliberately inserts	1.000000
inserts	0.000028
inserts those	1.000000
those pauses	0.045455
pauses .	0.250000
In normal	0.009524
normal speech	0.500000
one typically	0.015385
typically finds	0.055556
finds	0.000028
finds many	1.000000
many consecutive	0.019231
consecutive words	0.500000
words being	0.009174
being said	0.055556
said	0.000028
said with	1.000000
no pauses	0.076923
final sounds	0.111111
sounds of	0.133333
word blend	0.016667
smoothly or	0.500000
or fuse	0.004505
the initial	0.000692
initial sounds	0.333333
utterance can	0.333333
different meanings	0.020408
meanings depending	0.250000
A popular	0.020000
popular example	0.111111
often quoted	0.022727
quoted	0.000028
quoted in	1.000000
field ,	0.037037
phrase How	0.100000
How to	0.285714
to wreck	0.001328
wreck	0.000028
wreck a	1.000000
nice beach	0.250000
beach	0.000028
beach ,	1.000000
which sounds	0.007246
sounds very	0.066667
to How	0.001328
recognize speech	0.111111
As this	0.055556
this example	0.010989
example shows	0.012346
shows	0.000028
shows ,	1.000000
, proper	0.000561
proper lexical	0.142857
lexical segmentation	0.076923
segmentation depends	0.030303
on context	0.004717
which draws	0.007246
draws	0.000028
draws on	1.000000
the whole	0.000692
whole of	0.111111
human knowledge	0.021739
and experience	0.001445
experience ,	0.500000
and would	0.001445
would thus	0.018868
thus require	0.100000
require advanced	0.045455
advanced pattern	0.200000
and artificial	0.001445
intelligence technologies	0.125000
technologies to	0.250000
implemented on	0.200000
problem overlaps	0.022727
overlaps to	0.500000
some extent	0.012048
extent with	0.250000
segmentation that	0.030303
that occurs	0.003546
languages which	0.020000
traditionally written	0.500000
written without	0.038462
without inter-word	0.076923
spaces ,	0.200000
Chinese and	0.142857
and Japanese	0.001445
Japanese .	0.125000
even for	0.037037
often much	0.022727
much easier	0.045455
than speech	0.022222
segmentation ,	0.090909
the written	0.000692
language usually	0.006757
usually has	0.031250
has little	0.011905
little interference	0.333333
interference	0.000028
interference between	1.000000
often contains	0.022727
contains additional	0.100000
additional clues	0.166667
clues not	0.333333
of Chinese	0.000891
Chinese characters	0.142857
characters for	0.062500
for word	0.003610
word stems	0.016667
stems in	0.500000
in Japanese	0.001873
Japanese -RRB-	0.125000
Text segmentation	0.166667
of dividing	0.002674
dividing	0.000084
dividing written	0.333333
meaningful units	0.125000
units ,	0.142857
as words	0.003484
or topics	0.004505
topics .	0.142857
to mental	0.001328
humans when	0.083333
when reading	0.028571
reading text	0.125000
processes implemented	0.200000
implemented in	0.200000
in computers	0.001873
is non-trivial	0.002033
non-trivial	0.000056
non-trivial ,	0.500000
because while	0.033333
while some	0.050000
have explicit	0.009615
explicit word	0.200000
word boundary	0.016667
boundary markers	0.166667
markers ,	0.333333
word spaces	0.016667
spaces of	0.200000
written English	0.038462
English and	0.081081
the distinctive	0.000692
distinctive initial	0.500000
initial ,	0.333333
, medial	0.000561
medial	0.000028
medial and	1.000000
and final	0.001445
final letter	0.111111
of Arabic	0.000891
Arabic ,	0.250000
such signals	0.008130
are sometimes	0.004149
sometimes ambiguous	0.076923
all written	0.069767
<s> Compare	0.000769
Compare	0.000028
Compare speech	1.000000
dividing speech	0.333333
into linguistically	0.012821
linguistically	0.000028
linguistically meaningful	1.000000
meaningful portions	0.125000
portions	0.000028
portions .	1.000000
In English	0.019048
and many	0.001445
languages using	0.020000
the Latin	0.000692
Latin alphabet	0.250000
space is	0.200000
good approximation	0.076923
approximation of	0.166667
word delimiter	0.016667
delimiter	0.000028
delimiter .	1.000000
-LRB- Some	0.002710
Some examples	0.047619
examples where	0.041667
space character	0.200000
character alone	0.045455
alone may	0.250000
be sufficient	0.004219
sufficient include	0.200000
include contractions	0.037037
contractions like	0.500000
like ca	0.035714
ca	0.000028
ca n't	1.000000
n't for	0.250000
for can	0.003610
However the	0.027027
the equivalent	0.000692
equivalent to	0.200000
this character	0.010989
character is	0.090909
not found	0.008929
written scripts	0.038462
scripts ,	0.333333
without it	0.076923
it word	0.008547
word segmentation	0.050000
a difficult	0.001227
difficult problem	0.035714
Languages which	0.333333
which do	0.007246
a trivial	0.001227
trivial word	0.250000
segmentation process	0.030303
process include	0.027778
include Chinese	0.037037
Japanese ,	0.125000
sentences but	0.026316
not words	0.026786
are delimited	0.012448
delimited	0.000112
delimited ,	0.500000
, Thai	0.000561
Thai and	0.500000
and Lao	0.001445
Lao	0.000028
Lao ,	1.000000
where phrases	0.028571
and Vietnamese	0.001445
Vietnamese	0.000028
Vietnamese ,	1.000000
where syllables	0.028571
syllables but	0.500000
delimited .	0.250000
some writing	0.012048
systems however	0.008929
the Ge'ez	0.000692
Ge'ez	0.000028
Ge'ez script	1.000000
script used	0.250000
for Amharic	0.003610
Amharic	0.000028
Amharic and	1.000000
and Tigrinya	0.001445
Tigrinya	0.000028
Tigrinya among	1.000000
are explicitly	0.004149
explicitly delimited	0.250000
delimited -LRB-	0.250000
-LRB- at	0.002710
least historically	0.200000
historically -RRB-	0.500000
a non-whitespace	0.001227
non-whitespace	0.000028
non-whitespace character	1.000000
character .	0.045455
The Unicode	0.005208
Unicode	0.000028
Unicode Consortium	1.000000
Consortium	0.000028
Consortium has	1.000000
has published	0.011905
a Standard	0.001227
Standard Annex	0.500000
Annex	0.000028
Annex on	1.000000
on Text	0.004717
Text Segmentation	0.166667
Segmentation	0.000028
Segmentation ,	1.000000
, exploring	0.000561
exploring	0.000028
exploring the	1.000000
the issues	0.000692
issues of	0.200000
of segmentation	0.000891
segmentation in	0.030303
in multiscript	0.001873
multiscript	0.000028
multiscript texts	1.000000
Word splitting	0.285714
splitting	0.000056
splitting is	0.500000
parsing concatenated	0.035714
concatenated	0.000028
concatenated text	1.000000
i.e. text	0.052632
contains no	0.100000
no spaces	0.076923
spaces or	0.200000
other word	0.014286
word separators	0.016667
separators	0.000028
separators -RRB-	1.000000
to infer	0.001328
infer	0.000028
infer where	1.000000
where word	0.028571
word breaks	0.016667
breaks exist	0.500000
exist	0.000028
exist .	1.000000
splitting may	0.500000
may also	0.019231
also refer	0.014493
of hyphenation	0.000891
hyphenation	0.000028
hyphenation .	1.000000
Sentence segmentation	0.400000
segmentation Sentence	0.030303
dividing a	0.333333
language into	0.006757
into its	0.012821
its component	0.028571
component sentences	0.200000
using punctuation	0.016949
particularly the	0.200000
full stop	0.400000
stop	0.000056
stop character	1.000000
reasonable approximation	0.500000
approximation .	0.166667
However even	0.027027
English this	0.027027
not trivial	0.008929
trivial due	0.250000
character for	0.045455
for abbreviations	0.003610
which may	0.007246
may or	0.019231
not also	0.008929
also terminate	0.014493
terminate	0.000028
terminate a	1.000000
example Mr.	0.012346
Mr.	0.000056
Mr. is	0.500000
not its	0.008929
own sentence	0.166667
`` Mr.	0.005291
Mr. Smith	0.500000
Smith	0.000028
Smith went	1.000000
the shops	0.000692
shops	0.000028
shops in	1.000000
in Jones	0.001873
Jones	0.000028
Jones Street	1.000000
Street .	0.333333
When processing	0.142857
processing plain	0.018519
plain	0.000028
plain text	1.000000
, tables	0.000561
tables of	0.333333
abbreviations that	0.200000
contain periods	0.083333
periods can	0.333333
can help	0.005525
help prevent	0.111111
prevent	0.000028
prevent incorrect	1.000000
incorrect assignment	0.333333
assignment of	0.500000
of sentence	0.000891
As with	0.055556
languages contain	0.020000
contain punctuation	0.083333
punctuation characters	0.142857
are useful	0.004149
for approximating	0.003610
approximating	0.000028
approximating sentence	1.000000
Other segmentation	0.142857
problems Processes	0.058824
Processes	0.000028
Processes may	1.000000
be required	0.004219
required to	0.142857
segment text	0.222222
segments besides	0.200000
besides	0.000028
besides words	1.000000
including morphemes	0.071429
morphemes -LRB-	0.333333
task usually	0.023810
usually called	0.031250
called morphological	0.055556
morphological analysis	0.333333
, paragraphs	0.000561
paragraphs ,	0.250000
, topics	0.000561
topics or	0.142857
discourse turns	0.027778
turns .	0.333333
A document	0.020000
contain multiple	0.083333
multiple topics	0.076923
topics ,	0.142857
computerized text	0.500000
segmentation may	0.030303
to discover	0.001328
discover	0.000028
discover these	1.000000
these topics	0.023810
topics automatically	0.142857
and segment	0.001445
segment the	0.111111
text accordingly	0.006289
accordingly	0.000028
accordingly .	1.000000
The topic	0.005208
topic boundaries	0.125000
boundaries may	0.090909
be apparent	0.004219
apparent	0.000028
apparent from	1.000000
from section	0.009615
section titles	0.166667
titles and	0.500000
and paragraphs	0.001445
paragraphs .	0.250000
cases one	0.055556
one needs	0.015385
use techniques	0.013889
techniques similar	0.043478
different approaches	0.020408
tried .	0.333333
segmentation approaches	0.030303
approaches Automatic	0.035714
of implementing	0.000891
implementing	0.000028
implementing a	1.000000
computer process	0.022727
When punctuation	0.142857
and similar	0.001445
similar clues	0.037037
clues are	0.333333
consistently available	0.333333
the segmentation	0.000692
segmentation task	0.030303
task often	0.023810
often requires	0.022727
requires fairly	0.062500
fairly non-trivial	0.250000
non-trivial techniques	0.500000
as statistical	0.003484
statistical decision-making	0.030303
decision-making	0.000028
decision-making ,	1.000000
, large	0.000561
large dictionaries	0.043478
dictionaries	0.000028
dictionaries ,	1.000000
as consideration	0.003484
consideration of	0.333333
semantic constraints	0.047619
constraints .	0.250000
<s> Effective	0.000769
Effective	0.000028
Effective natural	1.000000
systems and	0.008929
segmentation tools	0.060606
tools usually	0.166667
usually operate	0.031250
operate	0.000028
operate on	1.000000
on text	0.004717
specific domains	0.047619
and sources	0.001445
, processing	0.000561
processing text	0.018519
text used	0.006289
in medical	0.001873
records is	0.250000
different problem	0.020408
problem than	0.022727
than processing	0.022222
processing news	0.018519
or real	0.004505
real estate	0.111111
estate	0.000028
estate advertisements	1.000000
of developing	0.000891
developing text	0.250000
tools starts	0.166667
starts with	0.500000
with collecting	0.005464
collecting	0.000028
collecting a	1.000000
large corpus	0.043478
application domain	0.071429
two general	0.034483
general approaches	0.045455
: Manual	0.009804
Manual analysis	0.333333
and writing	0.001445
writing custom	0.111111
custom software	0.500000
software Annotate	0.037037
Annotate	0.000028
Annotate the	1.000000
the sample	0.000692
sample corpus	0.333333
corpus with	0.032258
with boundary	0.005464
boundary information	0.166667
information and	0.021739
use Machine	0.013889
Machine Learning	0.111111
Learning	0.000028
Learning Some	1.000000
Some text	0.047619
segmentation systems	0.030303
systems take	0.008929
any markup	0.032258
markup	0.000028
markup like	1.000000
like HTML	0.035714
HTML	0.000028
HTML and	1.000000
and know	0.001445
know document	0.500000
document formats	0.027778
formats	0.000028
formats like	1.000000
like PDF	0.035714
PDF	0.000028
PDF to	1.000000
provide additional	0.166667
additional evidence	0.166667
evidence for	0.500000
for sentence	0.003610
sentence and	0.020833
and paragraph	0.001445
paragraph boundaries	0.333333
